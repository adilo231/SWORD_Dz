{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install aiogoogletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from unidecode import unidecode\n",
    "import emoji\n",
    "from aiogoogletrans import Translator\n",
    "import asyncio\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "french_stopwords = set(nltk.corpus.stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- get hashtags\n",
    "2- get mentions\n",
    "3- get urls\n",
    "4- remove hashtags\n",
    "5- treat mentions \"remove only @\"\n",
    "6- replace emojis \"only arabic case is treated\"\n",
    "7- text = re.sub('http\\S+\\s*', ' ', text)  # remove URLs\n",
    "7- remove punctuations\n",
    "10- remove stop words \n",
    "8- remove non used characters \n",
    "9- remove repeated characters\n",
    "11- for arabic remove tashkeel\n",
    "12- Strip tatweel from a text.\n",
    "13- stem words\n",
    "14- normalizeArabic\n",
    "15- tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emails(text):\n",
    "\temails = re.findall(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+\\b)', text)\n",
    "\treturn emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_hashtag_to_words(tag):\n",
    "    tag = tag.replace('#', '')\n",
    "    tags = tag.split('_')\n",
    "    if len(tags) > 1:\n",
    "        return tags\n",
    "    pattern = re.compile(r\"[A-Za-z]+|\\d+\")\n",
    "    return pattern.findall(tag)\n",
    "\n",
    "def extract_hashtag(text):\n",
    "    hash_list = ([re.sub(r\"(\\W+)$\", \"\", i) for i in text.split() if i.startswith(\"#\")])\n",
    "    word_list = []\n",
    "    for word in hash_list:\n",
    "        word_list.extend(split_hashtag_to_words(word))\n",
    "    return word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['én', 'example', 'text']\n"
     ]
    }
   ],
   "source": [
    "text = \"this is #én_example #text \"\n",
    "print(extract_hashtag(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user1', 'user2']\n"
     ]
    }
   ],
   "source": [
    "def extract_mentions(text):\n",
    "    mentions = re.findall(r'@(\\w+)', text)\n",
    "    return mentions\n",
    "\n",
    "text_arabic = \"@user1 say hi to @user2\"\n",
    "mentions_arabic = extract_mentions(text_arabic)\n",
    "print(mentions_arabic) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- get urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https', 'example.com', '/index.php'), ('https', 'example.org', '')]\n"
     ]
    }
   ],
   "source": [
    "def extract_urls(text):\n",
    "\turls = re.findall(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?',text)\n",
    "\treturn urls\n",
    "\n",
    "arabic_text_with_urls = \"enter those sites to find more infos : https://example.com/index.php، https://example.org\"\n",
    "urls = extract_urls(arabic_text_with_urls)\n",
    "print(urls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- treat hashtags : remove the whole hashtag or just the character "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hashtag(text):\n",
    "    words = text.split()\n",
    "    cleaned_text = []\n",
    "    for word in words:\n",
    "        if not is_hashtag(word):\n",
    "            cleaned_text.append(word)\n",
    "    return \" \".join(cleaned_text)\n",
    "\n",
    "def is_hashtag(word):\n",
    "    return word.startswith(\"#\")\n",
    "\n",
    "\n",
    "def remove_hashtag_char(text):\n",
    "    hashtag_pattern = r'#+'\n",
    "    cleaned_text = re.sub(hashtag_pattern, '', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this text be cleaned\n"
     ]
    }
   ],
   "source": [
    "text_with_hashtag = \"this text #must be cleaned #from_hashtags \"\n",
    "cleaned_text = clean_hashtag(text_with_hashtag)\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this text must be cleaned from_hashtags \n"
     ]
    }
   ],
   "source": [
    "text_with_hashtag = \"this text #must be cleaned #from_hashtags \"\n",
    "cleaned_text = remove_hashtag_char(text_with_hashtag)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emails(text):\n",
    "\treturn re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+\\b)',\"\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is example '"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"this is example user@gmail.com\"\n",
    "remove_emails(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5- remove mentions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi user1\n"
     ]
    }
   ],
   "source": [
    "def remove_mentions(text):\n",
    "    cleaned_text = re.sub(r'@+', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "text_with_mentions = \"hi @user1\"\n",
    "cleaned_text = remove_mentions(text_with_mentions)\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6- replace emojis \"only arabic case is treated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Selected_Fr_emojis.csv','r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    emojis_ar = {}\n",
    "    for line in lines:\n",
    "        line = line.strip('\\n').split(';')\n",
    "        emojis_ar.update({line[0].strip():line[1].strip()})\n",
    "\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def emoji_native_translation(text):\n",
    "    text = text.lower()\n",
    "    loves = [\"<3\", \"♥\",'❤']\n",
    "    smilefaces = []\n",
    "    sadfaces = []\n",
    "    neutralfaces = []\n",
    "\n",
    "    eyes = [\"8\",\":\",\"=\",\";\"]\n",
    "    nose = [\"'\",\"`\",\"-\",r\"\\\\\"]\n",
    "    for e in eyes:\n",
    "        for n in nose:\n",
    "            for s in [\"\\)\", \"d\", \"]\", \"}\",\"p\"]:\n",
    "                smilefaces.append(e+n+s)\n",
    "                smilefaces.append(e+s)\n",
    "            for s in [\"\\(\", \"\\[\", \"{\"]:\n",
    "                sadfaces.append(e+n+s)\n",
    "                sadfaces.append(e+s)\n",
    "            for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n",
    "                neutralfaces.append(e+n+s)\n",
    "                neutralfaces.append(e+s)\n",
    "            #reversed\n",
    "            for s in [\"\\(\", \"\\[\", \"{\"]:\n",
    "                smilefaces.append(s+n+e)\n",
    "                smilefaces.append(s+e)\n",
    "            for s in [\"\\)\", \"\\]\", \"}\"]:\n",
    "                sadfaces.append(s+n+e)\n",
    "                sadfaces.append(s+e)\n",
    "            for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n",
    "                neutralfaces.append(s+n+e)\n",
    "                neutralfaces.append(s+e)\n",
    "\n",
    "    smilefaces = list(set(smilefaces))\n",
    "    sadfaces = list(set(sadfaces))\n",
    "    neutralfaces = list(set(neutralfaces))\n",
    "    t = []\n",
    "    for w in text.split():\n",
    "        if w in loves:\n",
    "            t.append(\"Amour\")\n",
    "        elif w in smilefaces:\n",
    "            t.append(\"Sourire\")\n",
    "        elif w in neutralfaces:\n",
    "            t.append(\"Neutre\")\n",
    "        elif w in sadfaces:\n",
    "            t.append(\"Triste\")\n",
    "        else:\n",
    "            t.append(w)\n",
    "\n",
    "    newText = \" \".join(t)\n",
    "    return newText\n",
    "\n",
    "def is_emoji(word):\n",
    "    if word in emojis_ar:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def add_space(text):\n",
    "    return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()\n",
    "\n",
    "translator = Translator()\n",
    "loop = asyncio.get_event_loop()\n",
    "def translate_emojis(words):\n",
    "    word_list = list()\n",
    "    words_to_translate = list()\n",
    "    for word in words :\n",
    "        t = emojis_ar.get(word.get('emoji'),None)\n",
    "        if t is None:\n",
    "            word.update({'translation':'normale','translated':True})\n",
    "            #words_to_translate.append('normal')\n",
    "        else:\n",
    "            word.update({'translated':False,'translation':t})\n",
    "            words_to_translate.append(t.replace(':','').replace('_',' '))\n",
    "        word_list.append(word)\n",
    "    return word_list\n",
    "\n",
    "def emoji_unicode_translation(text):\n",
    "    text = add_space(text)\n",
    "    words = text.split()\n",
    "    text_list = list()\n",
    "    emojis_list = list()\n",
    "    c = 0\n",
    "    for word in words:\n",
    "        if is_emoji(word):\n",
    "            emojis_list.append({'emoji':word,'emplacement':c})\n",
    "        else:\n",
    "            text_list.append(word)\n",
    "        c+=1\n",
    "    emojis_translated = translate_emojis(emojis_list)\n",
    "    for em in emojis_translated:\n",
    "        text_list.insert(em.get('emplacement'),em.get('translation'))\n",
    "    text = \" \".join(text_list)\n",
    "    return text\n",
    "    \n",
    "def clean_emoji(text):\n",
    "    text = emoji_native_translation(text)\n",
    "    text = emoji_unicode_translation(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je t'aime Coeur rouge blush\n",
      "je suis très triste cri\n",
      "je suis de très bonne humeur sourire sueur\n"
     ]
    }
   ],
   "source": [
    "texte_avec_emojis = \"Je t'aime ❤️😊\"\n",
    "texte_propre = clean_emoji(texte_avec_emojis)\n",
    "print(texte_propre)\n",
    "\n",
    "\n",
    "texte_avec_emojis = \"Je suis très triste 😢\"\n",
    "texte_propre = clean_emoji(texte_avec_emojis)\n",
    "print(texte_propre)\n",
    "\n",
    "\n",
    "texte_avec_emojis = \"Je suis de très bonne humeur 😄\"\n",
    "texte_propre = clean_emoji(texte_avec_emojis)\n",
    "print(texte_propre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    cleaned_text = re.sub(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?',\"\",text)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " text with URLs removed: checké this \n"
     ]
    }
   ],
   "source": [
    "text = \"checké this http://www.google.com\"\n",
    "cleaned_text = remove_urls(text)\n",
    "print(\" text with URLs removed:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte sans stop words français: Ceci phrase d'exemple contenant stop words français.\n"
     ]
    }
   ],
   "source": [
    "def remove_french_stopwords(text):\n",
    "  return \" \".join([t for t in text.split() if t.lower() not in french_stopwords])\n",
    "\n",
    "\n",
    "sample_text = \"Ceci est une phrase d'exemple contenant des stop words français.\"\n",
    "filtered_text = remove_french_stopwords(sample_text)\n",
    "print(\"Texte sans stop words français:\", filtered_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7- remove punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_punctuation = string.punctuation\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    return re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~\"\"\"), ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text with punctuations removed: remove those punctuations      \n"
     ]
    }
   ],
   "source": [
    "text = \"remove those punctuations !;!'-\"\n",
    "cleaned_text = remove_punctuations(text)\n",
    "print(\"text with punctuations removed:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8- remove non used characters (u can keep numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_french(text):\n",
    "    french_pattern = re.compile(r'[^\\s\\dA-Za-zàâäéèêëîïôöùûüçÀÂÄÉÈÊËÎÏÔÖÙÛÜÇ]+')\n",
    "    cleaned_text = french_pattern.sub('', text)\n",
    "    \n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour Comment ça va  123 \n"
     ]
    }
   ],
   "source": [
    "text = \"Bonjour! Comment ça va ? 123 Привет\"\n",
    "cleaned_text = remove_non_french(text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9- remove repeated characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeated_characters(text):\n",
    "    repeated_pattern = re.compile(r'(\\S)(\\1)+', re.UNICODE)\n",
    "    cleaned_text = repeated_pattern.sub(r'\\1', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with repeated characters removed: thisé        is the \n"
     ]
    }
   ],
   "source": [
    "text = \"thissssé        isss theee \"\n",
    "cleaned_text = remove_repeated_characters(text)\n",
    "print(\"Text with repeated characters removed:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove multiple spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multiple_spaces(text):\n",
    "\treturn ' '.join(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stemming words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['le', 'renard', 'brun', 'rapide', 'sauter', 'par', '-', 'dessus', 'le', 'chien', 'paresseux']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    tokens_lemmas = [token.lemma_ if token.lemma_ != \"-PRON-\" and token.lemma_ != \"be\" else token.text for token in doc]\n",
    "    return tokens_lemmas\n",
    "\n",
    "text = \"Les renards bruns rapides sautent par-dessus les chiens paresseux\"\n",
    "tokens_lemmas = tokenize_and_lemmatize(text)\n",
    "print(tokens_lemmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the proposed algorithm to treat the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "text=\"ce sont des exemples de la langue français\"\n",
    "emails = get_emails(text)\n",
    "print(emails)\n",
    "hashtags = extract_hashtag(text)\n",
    "print(hashtags)\n",
    "mentions = extract_mentions(text)\n",
    "print(mentions)\n",
    "urls = extract_urls(text)\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['exemple', 'langue', 'français']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text=remove_emails(text)\n",
    "    text=clean_hashtag(text)\n",
    "    text=remove_mentions(text)\n",
    "    text=remove_urls(text)\n",
    "    text=clean_emoji(text)\n",
    "    text=remove_punctuations(text)\n",
    "    text=remove_repeated_characters(text)\n",
    "    text=remove_multiple_spaces(text)\n",
    "    text=remove_french_stopwords(text)\n",
    "    return tokenize_and_lemmatize(text)\n",
    "\n",
    "\n",
    "clean_text(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
