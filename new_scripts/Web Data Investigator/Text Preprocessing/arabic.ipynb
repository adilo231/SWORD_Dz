{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip  install pyarabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# Arabic stopwords\n",
    "arabic_stopwords = stopwords.words('arabic')\n",
    "\n",
    "# Print some of the stopwords\n",
    "print(arabic_stopwords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install aiogoogletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- get hashtags\n",
    "2- get mentions\n",
    "3- get urls\n",
    "4- remove hashtags\n",
    "5- treat mentions \"remove only @\"\n",
    "6- replace emojis \"only arabic case is treated\"\n",
    "7- text = re.sub('http\\S+\\s*', ' ', text)  # remove URLs\n",
    "7- remove punctuations\n",
    "10- remove stop words \n",
    "8- remove non used characters \n",
    "9- remove repeated characters\n",
    "11- for arabic remove tashkeel\n",
    "12- Strip tatweel from a text.\n",
    "13- stem words\n",
    "14- normalizeArabic\n",
    "15- tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def split_hashtag_to_words(tag):\n",
    "    tag = tag.replace('#','')\n",
    "    tags = tag.split('_')\n",
    "    if len(tags) > 1 :\n",
    "        \n",
    "        return tags\n",
    "    pattern = re.compile(r\"[A-Z][a-z]+|\\d+|[A-Z]+(?![a-z])\")\n",
    "    return pattern.findall(tag)\n",
    "\n",
    "def extract_hashtag(text):\n",
    "    hash_list = ([re.sub(r\"(\\W+)$\", \"\", i) for i in text.split() if i.startswith(\"#\")])\n",
    "    word_list = []\n",
    "    for word in hash_list :\n",
    "        word_list.extend(split_hashtag_to_words(word))\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ù…Ø³ØªØ®Ø¯Ù…1', 'Ù…Ø³ØªØ®Ø¯Ù…2']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_mentions_arabic(text):\n",
    "    # Find all mentions starting with '@' and extract the username\n",
    "    mentions = re.findall(r'@(\\w+)', text)\n",
    "    return mentions\n",
    "\n",
    "# Example usage:\n",
    "text_arabic = \"Ù…Ø±Ø­Ø¨Ø§ @Ù…Ø³ØªØ®Ø¯Ù…1 Ùˆ @Ù…Ø³ØªØ®Ø¯Ù…2ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒÙ…ØŸ\"\n",
    "mentions_arabic = extract_mentions_arabic(text_arabic)\n",
    "print(mentions_arabic)  # Output: ['Ù…Ø³ØªØ®Ø¯Ù…1', 'Ù…Ø³ØªØ®Ø¯Ù…2']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- get urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://example.com', 'https://example.org']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_urls(text):\n",
    "    # Regular expression pattern to match URLs\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    # Find all URLs in the text\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    return urls\n",
    "\n",
    "# Example usage:\n",
    "arabic_text_with_urls = \"Ù‚Ù… Ø¨Ø²ÙŠØ§Ø±Ø© Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆÙ‚Ø¹: https://example.comØŒ ÙˆØ£ÙŠØ¶Ù‹Ø§ https://example.org\"\n",
    "urls = extract_urls(arabic_text_with_urls)\n",
    "print(urls)  # Output: ['https://example.com', 'https://example.org']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- treat hashtags : remove the whole hashtag or just the character "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hashtag(text):\n",
    "    words = text.split()\n",
    "    text = list()\n",
    "    for word in words:\n",
    "        if is_hashtag(word):\n",
    "            text.extend(extract_hashtag(word))\n",
    "        else:\n",
    "            text.append(word)\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "def is_hashtag(word):\n",
    "    if word.startswith(\"#\"):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def remove_hashtag_char(text):\n",
    "    # Regular expression pattern to match mentions\n",
    "    mention_pattern = r'#+'\n",
    "    # Replace mentions with an empty string\n",
    "    cleaned_text = re.sub(mention_pattern, '', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ù…Ø±Ø­Ø¨Ø§ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ Ù…Ù†Ø´Ù† Ù„Ù€ @Ù…Ø³ØªØ®Ø¯Ù…2 ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©.\n"
     ]
    }
   ],
   "source": [
    "# Example usage with Arabic text:\n",
    "text_with_mentions_arabic = \"Ù…Ø±Ø­Ø¨Ø§  ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ Ù…Ù†Ø´Ù† #dfhg#  Ù„Ù€ @Ù…Ø³ØªØ®Ø¯Ù…2 ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©.\"\n",
    "cleaned_text_arabic = clean_hashtag(text_with_mentions_arabic)\n",
    "print(cleaned_text_arabic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5- remove mentions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ù…Ø±Ø­Ø¨Ø§ Ù…Ø³ØªØ®Ø¯Ù…1ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ Ù…Ù†Ø´Ù† Ù„Ù€ Ù…Ø³ØªØ®Ø¯Ù…2 ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_mentions(text):\n",
    "    # Regular expression pattern to match mentions\n",
    "    mention_pattern = r'@+'\n",
    "    # Replace mentions with an empty string\n",
    "    cleaned_text = re.sub(mention_pattern, '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "# Example usage with Arabic text:\n",
    "text_with_mentions_arabic = \"Ù…Ø±Ø­Ø¨Ø§ @Ù…Ø³ØªØ®Ø¯Ù…1ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ Ù…Ù†Ø´Ù† Ù„Ù€ @Ù…Ø³ØªØ®Ø¯Ù…2 ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©.\"\n",
    "cleaned_text_arabic = remove_mentions(text_with_mentions_arabic)\n",
    "print(cleaned_text_arabic)  # Output: \"Ù…Ø±Ø­Ø¨Ø§ ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ Ù…Ù†Ø´Ù† Ù„Ù€ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6- replace emojis \"only arabic case is treated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('emojis.csv','r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    emojis_ar = {}\n",
    "    for line in lines:\n",
    "        line = line.strip('\\n').split(';')\n",
    "        emojis_ar.update({line[0].strip():line[1].strip()})\n",
    "\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "import unicodedata\n",
    "from unidecode import unidecode\n",
    "\n",
    "def emoji_native_translation(text):\n",
    "    text = text.lower()\n",
    "    loves = [\"<3\", \"â™¥\",'â¤']\n",
    "    smilefaces = []\n",
    "    sadfaces = []\n",
    "    neutralfaces = []\n",
    "\n",
    "    eyes = [\"8\",\":\",\"=\",\";\"]\n",
    "    nose = [\"'\",\"`\",\"-\",r\"\\\\\"]\n",
    "    for e in eyes:\n",
    "        for n in nose:\n",
    "            for s in [\"\\)\", \"d\", \"]\", \"}\",\"p\"]:\n",
    "                smilefaces.append(e+n+s)\n",
    "                smilefaces.append(e+s)\n",
    "            for s in [\"\\(\", \"\\[\", \"{\"]:\n",
    "                sadfaces.append(e+n+s)\n",
    "                sadfaces.append(e+s)\n",
    "            for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n",
    "                neutralfaces.append(e+n+s)\n",
    "                neutralfaces.append(e+s)\n",
    "            #reversed\n",
    "            for s in [\"\\(\", \"\\[\", \"{\"]:\n",
    "                smilefaces.append(s+n+e)\n",
    "                smilefaces.append(s+e)\n",
    "            for s in [\"\\)\", \"\\]\", \"}\"]:\n",
    "                sadfaces.append(s+n+e)\n",
    "                sadfaces.append(s+e)\n",
    "            for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n",
    "                neutralfaces.append(s+n+e)\n",
    "                neutralfaces.append(s+e)\n",
    "\n",
    "    smilefaces = list(set(smilefaces))\n",
    "    sadfaces = list(set(sadfaces))\n",
    "    neutralfaces = list(set(neutralfaces))\n",
    "    t = []\n",
    "    for w in text.split():\n",
    "        if w in loves:\n",
    "            t.append(\"Ø­Ø¨\")\n",
    "        elif w in smilefaces:\n",
    "            t.append(\"Ù…Ø¶Ø­Ùƒ\")\n",
    "        elif w in neutralfaces:\n",
    "            t.append(\"Ø¹Ø§Ø¯ÙŠ\")\n",
    "        elif w in sadfaces:\n",
    "            t.append(\"Ù…Ø­Ø²Ù†\")\n",
    "        else:\n",
    "            t.append(w)\n",
    "    newText = \" \".join(t)\n",
    "    return newText\n",
    "\n",
    "import emoji\n",
    "def is_emoji(word):\n",
    "    if word in emojis_ar:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def add_space(text):\n",
    "    return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()\n",
    "\n",
    "from aiogoogletrans import Translator\n",
    "translator = Translator()\n",
    "import asyncio\n",
    "loop = asyncio.get_event_loop()\n",
    "def translate_emojis(words):\n",
    "    word_list = list()\n",
    "    words_to_translate = list()\n",
    "    for word in words :\n",
    "        t = emojis_ar.get(word.get('emoji'),None)\n",
    "        if t is None:\n",
    "            word.update({'translation':'Ø¹Ø§Ø¯ÙŠ','translated':True})\n",
    "            #words_to_translate.append('normal')\n",
    "        else:\n",
    "            word.update({'translated':False,'translation':t})\n",
    "            words_to_translate.append(t.replace(':','').replace('_',' '))\n",
    "        word_list.append(word)\n",
    "    return word_list\n",
    "\n",
    "def emoji_unicode_translation(text):\n",
    "    text = add_space(text)\n",
    "    words = text.split()\n",
    "    text_list = list()\n",
    "    emojis_list = list()\n",
    "    c = 0\n",
    "    for word in words:\n",
    "        if is_emoji(word):\n",
    "            emojis_list.append({'emoji':word,'emplacement':c})\n",
    "        else:\n",
    "            text_list.append(word)\n",
    "        c+=1\n",
    "    emojis_translated = translate_emojis(emojis_list)\n",
    "    for em in emojis_translated:\n",
    "        text_list.insert(em.get('emplacement'),em.get('translation'))\n",
    "    text = \" \".join(text_list)\n",
    "    return text\n",
    "    \n",
    "def clean_emoji(text):\n",
    "    text = emoji_native_translation(text)\n",
    "    text = emoji_unicode_translation(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_with_emojis = \"Ø£Ø­Ø¨Ùƒ â¤ï¸ğŸ˜Š\"\n",
    "cleaned_text = clean_emoji(text_with_emojis)\n",
    "print(cleaned_text)\n",
    "\n",
    "\n",
    "text_with_emojis = \"Ø£Ù†Ø§ Ø­Ø²ÙŠÙ† Ø¬Ø¯Ù‹Ø§ ğŸ˜¢\"\n",
    "cleaned_text = clean_emoji(text_with_emojis)\n",
    "print(cleaned_text)\n",
    "\n",
    "\n",
    "text_with_emojis = \"Ø£Ù†Ø§ ÙÙŠ Ø­Ø§Ù„Ø© Ù…Ø²Ø§Ø¬ÙŠØ© Ø¬ÙŠØ¯Ø© ğŸ˜„\"\n",
    "cleaned_text = clean_emoji(text_with_emojis)\n",
    "print(cleaned_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_urls(text):\n",
    "    # Remove URLs from the text\n",
    "    cleaned_text = re.sub(r'http\\S+\\s*', '', text)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Arabic text: ØªØ­Ù‚Ù‚ Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ø±Ø§Ø¨Ø·: https://example.comØŒ Ø¥Ù†Ù‡ Ø±Ø§Ø¦Ø¹!\n",
      "Arabic text with URLs removed: ØªØ­Ù‚Ù‚ Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ø±Ø§Ø¨Ø·: Ø¥Ù†Ù‡ Ø±Ø§Ø¦Ø¹!\n"
     ]
    }
   ],
   "source": [
    "# Arabic example\n",
    "arabic_text = \"ØªØ­Ù‚Ù‚ Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ø±Ø§Ø¨Ø·: https://example.comØŒ Ø¥Ù†Ù‡ Ø±Ø§Ø¦Ø¹!\"\n",
    "cleaned_arabic_text = remove_urls(arabic_text)\n",
    "print(\"Original Arabic text:\", arabic_text)\n",
    "print(\"Arabic text with URLs removed:\", cleaned_arabic_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7- remove punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\]'\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5796\\1715161554.py:7: SyntaxWarning: invalid escape sequence '\\]'\n",
      "  text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,ØŒ-./:;<=>ØŸ?@[\\]^_`{|}~\"\"\"), ' ', text)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "all_punctuation = string.punctuation\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    ## Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,ØŒ-./:;<=>ØŸ?@[\\]^_`{|}~\"\"\"), ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arabic example\n",
    "arabic_text = \"ØªØ­Ù‚Ù‚ Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ø±Ø§Ø¨Ø·: https://example.comØŒ Ø¥Ù†Ù‡ Ø±Ø§Ø¦Ø¹!\"\n",
    "cleaned_arabic_text = remove_punctuations(arabic_text)\n",
    "print(\"Original Arabic text:\", arabic_text)\n",
    "print(\"Arabic text with punctuations removed:\", cleaned_arabic_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10- remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "# Arabic stopwords\n",
    "arabic_stopwords = stopwords.words('arabic')\n",
    "\n",
    "# Print some of the stopwords\n",
    "print(arabic_stopwords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_arabic_stopwords(text):\n",
    "    # Load Arabic stopwords\n",
    "    arabic_stopwords = set(stopwords.words('arabic'))\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in words if word.lower() not in arabic_stopwords]\n",
    "    \n",
    "    # Recreate the text from filtered words\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "text = \"Ù‡Ø°Ø§ Ù‡Ùˆ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶  ÙˆØ£ÙŠØ¶Ù‹Ø§ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©\"\n",
    "cleaned_text = remove_arabic_stopwords(text)\n",
    "print(\"Original text:\", text)\n",
    "print(\"Text with Arabic stopwords removed:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8- remove non used characters (u can keep numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_non_arabic(text):\n",
    "    # Define regex pattern to match non-Arabic characters and non-numeric characters\n",
    "    arabic_pattern = re.compile(r'[^\\u0600-\\u06FF\\u0750-\\u077F\\d\\s]+')\n",
    "    \n",
    "    # Remove non-Arabic and non-numeric characters from the text\n",
    "    cleaned_text = arabic_pattern.sub('', text)\n",
    "    \n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Ù‡Ø°Ø§ Ù‡Ùˆ Ù†Øµ (ØªØ¬Ø±ÙŠØ¨ÙŠ) gghhgj ÙŠØ­ØªÙˆÙŠ 123 Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ø§Ù„ÙƒÙ„Ù…Ø§Øª.,; () Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©  1315Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©!\n",
      "Text with non-Arabic characters removed: Ù‡Ø°Ø§ Ù‡Ùˆ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ  ÙŠØ­ØªÙˆÙŠ 123 Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ø§Ù„ÙƒÙ„Ù…Ø§Øª  Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©  1315Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"Ù‡Ø°Ø§ Ù‡Ùˆ Ù†Øµ (ØªØ¬Ø±ÙŠØ¨ÙŠ) gghhgj ÙŠØ­ØªÙˆÙŠ 123 Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ø§Ù„ÙƒÙ„Ù…Ø§Øª.,; () Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©  1315Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©!\"\n",
    "cleaned_text = remove_non_arabic(text)\n",
    "print(\"Original text:\", text)\n",
    "print(\"Text with non-Arabic characters removed:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9- remove repeated characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_repeated_characters(text):\n",
    "    # Define regex pattern to match repeated Arabic characters\n",
    "    repeated_pattern = re.compile(r'(\\S)(\\1)+', re.UNICODE)\n",
    "    \n",
    "    # Remove repeated characters from the text\n",
    "    cleaned_text = repeated_pattern.sub(r'\\1', text)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "text = \"Ù‡Ø°Ø§Ø§Ø§Ø§ Ù‡ÙˆÙˆÙˆÙˆ Ù†Øµ Ù…ÙƒÙƒÙƒÙƒÙƒÙƒØ±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±\"\n",
    "cleaned_text = remove_repeated_characters(text)\n",
    "print(\"Original text:\", text)\n",
    "print(\"Text with repeated characters removed:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11- for arabic remove tashkeel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarabic.araby as araby\n",
    "text= araby.strip_tashkeel(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strip tatweel from a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\n"
     ]
    }
   ],
   "source": [
    "from pyarabic.araby import strip_tatweel\n",
    "text = u\"Ø§Ù„Ø¹Ù€Ù€Ù€Ù€Ù€Ø±Ø¨ÙŠØ©\"\n",
    "print(strip_tatweel(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalizeArabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeArabic(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(\"[Ø¥Ø£Ù±Ø¢Ø§]\", \"Ø§\", text)\n",
    "    text = re.sub(\"Ù‰\", \"ÙŠ\", text)\n",
    "    text = re.sub(\"Ø¤\", \"Ø¡\", text)\n",
    "    text = re.sub(\"Ø¦\", \"Ø¡\", text)\n",
    "    text = re.sub(\"Ø©\", \"Ù‡\", text)\n",
    "    noise = re.compile(\"\"\" Ù‘    | # Tashdid\n",
    "                             Ù    | # Fatha\n",
    "                             Ù‹    | # Tanwin Fath\n",
    "                             Ù    | # Damma\n",
    "                             ÙŒ    | # Tanwin Damm\n",
    "                             Ù    | # Kasra\n",
    "                             Ù    | # Tanwin Kasr\n",
    "                             Ù’    | # Sukun\n",
    "                             Ù€     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    text = re.sub(noise, '', text)\n",
    "    text = re.sub(r'(.)\\1+', r\"\\1\\1\", text) # Remove longation\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarabic import araby\n",
    "text = u\"Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„ØºØ© Ø¬Ù…ÙŠÙ„Ø©.\"\n",
    "tokens = araby.tokenize(text)\n",
    "print(u\"\\n\".join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stemming words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: snowballstemmer in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install snowballstemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÙŠØ§ÙƒÙ„\n"
     ]
    }
   ],
   "source": [
    "from snowballstemmer import stemmer\n",
    "\n",
    "def stem_arabic_word(word):\n",
    "    # Create an Arabic stemmer using the Khoja algorithm\n",
    "    arabic_stemmer = stemmer(\"arabic\")\n",
    "    # Stem the word\n",
    "    stemmed_word = arabic_stemmer.stemWord(word)\n",
    "    return stemmed_word\n",
    "\n",
    "# Example usage:\n",
    "word = \"ÙŠØ£ÙƒÙ„ÙˆÙ†\"\n",
    "stemmed_word = stem_arabic_word(word)\n",
    "print(stemmed_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the proposed algorithm to treat the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_hashtag_to_words(tag):\n",
    "    tag = tag.replace('#','')\n",
    "    tags = tag.split('_')\n",
    "    if len(tags) > 1 :\n",
    "        \n",
    "        return tags\n",
    "    pattern = re.compile(r\"[A-Z][a-z]+|\\d+|[A-Z]+(?![a-z])\")\n",
    "    return pattern.findall(tag)\n",
    "\n",
    "def extract_hashtag(text):\n",
    "    hash_list = ([re.sub(r\"(\\W+)$\", \"\", i) for i in text.split() if i.startswith(\"#\")])\n",
    "    word_list = []\n",
    "    for word in hash_list :\n",
    "        word_list.extend(split_hashtag_to_words(word))\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ù…Ø³ØªØ®Ø¯Ù…1', 'Ù…Ø³ØªØ®Ø¯Ù…2']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_mentions_arabic(text):\n",
    "    # Find all mentions starting with '@' and extract the username\n",
    "    mentions = re.findall(r'@(\\w+)', text)\n",
    "    return mentions\n",
    "\n",
    "# Example usage:\n",
    "text_arabic = \"Ù…Ø±Ø­Ø¨Ø§ @Ù…Ø³ØªØ®Ø¯Ù…1 Ùˆ @Ù…Ø³ØªØ®Ø¯Ù…2ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒÙ…ØŸ\"\n",
    "mentions_arabic = extract_mentions_arabic(text_arabic)\n",
    "print(mentions_arabic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://example.com', 'https://example.org']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_urls(text):\n",
    "    # Regular expression pattern to match URLs\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    # Find all URLs in the text\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    return urls\n",
    "\n",
    "# Example usage:\n",
    "arabic_text_with_urls = \"Ù‚Ù… Ø¨Ø²ÙŠØ§Ø±Ø© Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆÙ‚Ø¹: https://example.comØŒ ÙˆØ£ÙŠØ¶Ù‹Ø§ https://example.org\"\n",
    "urls = extract_urls(arabic_text_with_urls)\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = clean_hashtag(text)\n",
    "    text = clean_emoji(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_punctuations(text)\n",
    "    text = remove_arabic_stopwords(text)\n",
    "    text = remove_non_arabic(text)\n",
    "    text = remove_repeated_characters(text)\n",
    "    tokens = araby.tokenize(text)\n",
    "    # Assuming tokens is a list of Arabic words\n",
    "    for i, word in enumerate(tokens):\n",
    "\n",
    "        # Remove diacritics (Tashkeel)\n",
    "        diacritic_removed_word = araby.strip_tashkeel(word)\n",
    "        \n",
    "        # Remove non-Arabic characters\n",
    "        arabic_only_word = remove_non_arabic(diacritic_removed_word)\n",
    "        \n",
    "        # Remove tatweel\n",
    "        final_word = strip_tatweel(arabic_only_word)\n",
    "\n",
    "        # Apply stemming (we need a good stemmer)\n",
    "        stemmed_word = stem_arabic_word(final_word)\n",
    "    \n",
    "        # Apply normalization\n",
    "        normalized_word = normalizeArabic(stemmed_word)\n",
    "        \n",
    "        # Replace the original token with the processed one\n",
    "        tokens[i] = final_word\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "arabic_text_with_urls = \"Ù‚Ù… Ø¨Ø²ÙŠØ§Ø±Ø© 12 Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆÙ‚Ø¹: https://example.comØŒ Ùˆ Ø£ÙŠØ¶Ù‹Ø§ https://example.org\"\n",
    "clean_text(arabic_text_with_urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
