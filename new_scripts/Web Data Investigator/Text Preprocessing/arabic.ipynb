{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyarabic in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.6.15)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pyarabic) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip  install pyarabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي', 'الذي', 'الذين', 'اللاتي', 'اللائي', 'اللتان', 'اللتيا', 'اللتين', 'اللذان', 'اللذين', 'اللواتي', 'إلى', 'إليك', 'إليكم', 'إليكما', 'إليكن', 'أم', 'أما', 'أما', 'إما', 'أن', 'إن', 'إنا', 'أنا', 'أنت', 'أنتم', 'أنتما', 'أنتن', 'إنما', 'إنه', 'أنى', 'أنى', 'آه', 'آها', 'أو', 'أولاء', 'أولئك', 'أوه', 'آي', 'أي', 'أيها', 'إي', 'أين', 'أين', 'أينما', 'إيه', 'بخ', 'بس', 'بعد', 'بعض', 'بك', 'بكم', 'بكم', 'بكما', 'بكن', 'بل', 'بلى', 'بما', 'بماذا', 'بمن', 'بنا', 'به', 'بها', 'بهم', 'بهما', 'بهن', 'بي', 'بين', 'بيد', 'تلك', 'تلكم', 'تلكما', 'ته', 'تي', 'تين', 'تينك', 'ثم', 'ثمة', 'حاشا', 'حبذا', 'حتى', 'حيث', 'حيثما', 'حين', 'خلا', 'دون', 'ذا', 'ذات', 'ذاك', 'ذان', 'ذانك', 'ذلك', 'ذلكم', 'ذلكما', 'ذلكن', 'ذه', 'ذو', 'ذوا', 'ذواتا', 'ذواتي', 'ذي', 'ذين', 'ذينك', 'ريث', 'سوف', 'سوى', 'شتان', 'عدا', 'عسى', 'عل', 'على', 'عليك', 'عليه', 'عما', 'عن', 'عند', 'غير', 'فإذا', 'فإن', 'فلا', 'فمن', 'في', 'فيم', 'فيما', 'فيه', 'فيها', 'قد', 'كأن', 'كأنما', 'كأي', 'كأين', 'كذا', 'كذلك', 'كل', 'كلا', 'كلاهما', 'كلتا', 'كلما', 'كليكما', 'كليهما', 'كم', 'كم', 'كما', 'كي', 'كيت', 'كيف', 'كيفما', 'لا', 'لاسيما', 'لدى', 'لست', 'لستم', 'لستما', 'لستن', 'لسن', 'لسنا', 'لعل', 'لك', 'لكم', 'لكما', 'لكن', 'لكنما', 'لكي', 'لكيلا', 'لم', 'لما', 'لن', 'لنا', 'له', 'لها', 'لهم', 'لهما', 'لهن', 'لو', 'لولا', 'لوما', 'لي', 'لئن', 'ليت', 'ليس', 'ليسا', 'ليست', 'ليستا', 'ليسوا', 'ما', 'ماذا', 'متى', 'مذ', 'مع', 'مما', 'ممن', 'من', 'منه', 'منها', 'منذ', 'مه', 'مهما', 'نحن', 'نحو', 'نعم', 'ها', 'هاتان', 'هاته', 'هاتي', 'هاتين', 'هاك', 'هاهنا', 'هذا', 'هذان', 'هذه', 'هذي', 'هذين', 'هكذا', 'هل', 'هلا', 'هم', 'هما', 'هن', 'هنا', 'هناك', 'هنالك', 'هو', 'هؤلاء', 'هي', 'هيا', 'هيت', 'هيهات', 'والذي', 'والذين', 'وإذ', 'وإذا', 'وإن', 'ولا', 'ولكن', 'ولو', 'وما', 'ومن', 'وهو', 'يا', 'أبٌ', 'أخٌ', 'حمٌ', 'فو', 'أنتِ', 'يناير', 'فبراير', 'مارس', 'أبريل', 'مايو', 'يونيو', 'يوليو', 'أغسطس', 'سبتمبر', 'أكتوبر', 'نوفمبر', 'ديسمبر', 'جانفي', 'فيفري', 'مارس', 'أفريل', 'ماي', 'جوان', 'جويلية', 'أوت', 'كانون', 'شباط', 'آذار', 'نيسان', 'أيار', 'حزيران', 'تموز', 'آب', 'أيلول', 'تشرين', 'دولار', 'دينار', 'ريال', 'درهم', 'ليرة', 'جنيه', 'قرش', 'مليم', 'فلس', 'هللة', 'سنتيم', 'يورو', 'ين', 'يوان', 'شيكل', 'واحد', 'اثنان', 'ثلاثة', 'أربعة', 'خمسة', 'ستة', 'سبعة', 'ثمانية', 'تسعة', 'عشرة', 'أحد', 'اثنا', 'اثني', 'إحدى', 'ثلاث', 'أربع', 'خمس', 'ست', 'سبع', 'ثماني', 'تسع', 'عشر', 'ثمان', 'سبت', 'أحد', 'اثنين', 'ثلاثاء', 'أربعاء', 'خميس', 'جمعة', 'أول', 'ثان', 'ثاني', 'ثالث', 'رابع', 'خامس', 'سادس', 'سابع', 'ثامن', 'تاسع', 'عاشر', 'حادي', 'أ', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'ء', 'ى', 'آ', 'ؤ', 'ئ', 'أ', 'ة', 'ألف', 'باء', 'تاء', 'ثاء', 'جيم', 'حاء', 'خاء', 'دال', 'ذال', 'راء', 'زاي', 'سين', 'شين', 'صاد', 'ضاد', 'طاء', 'ظاء', 'عين', 'غين', 'فاء', 'قاف', 'كاف', 'لام', 'ميم', 'نون', 'هاء', 'واو', 'ياء', 'همزة', 'ي', 'نا', 'ك', 'كن', 'ه', 'إياه', 'إياها', 'إياهما', 'إياهم', 'إياهن', 'إياك', 'إياكما', 'إياكم', 'إياك', 'إياكن', 'إياي', 'إيانا', 'أولالك', 'تانِ', 'تانِك', 'تِه', 'تِي', 'تَيْنِ', 'ثمّ', 'ثمّة', 'ذانِ', 'ذِه', 'ذِي', 'ذَيْنِ', 'هَؤلاء', 'هَاتانِ', 'هَاتِه', 'هَاتِي', 'هَاتَيْنِ', 'هَذا', 'هَذانِ', 'هَذِه', 'هَذِي', 'هَذَيْنِ', 'الألى', 'الألاء', 'أل', 'أنّى', 'أيّ', 'ّأيّان', 'أنّى', 'أيّ', 'ّأيّان', 'ذيت', 'كأيّ', 'كأيّن', 'بضع', 'فلان', 'وا', 'آمينَ', 'آهِ', 'آهٍ', 'آهاً', 'أُفٍّ', 'أُفٍّ', 'أفٍّ', 'أمامك', 'أمامكَ', 'أوّهْ', 'إلَيْكَ', 'إلَيْكَ', 'إليكَ', 'إليكنّ', 'إيهٍ', 'بخٍ', 'بسّ', 'بَسْ', 'بطآن', 'بَلْهَ', 'حاي', 'حَذارِ', 'حيَّ', 'حيَّ', 'دونك', 'رويدك', 'سرعان', 'شتانَ', 'شَتَّانَ', 'صهْ', 'صهٍ', 'طاق', 'طَق', 'عَدَسْ', 'كِخ', 'مكانَك', 'مكانَك', 'مكانَك', 'مكانكم', 'مكانكما', 'مكانكنّ', 'نَخْ', 'هاكَ', 'هَجْ', 'هلم', 'هيّا', 'هَيْهات', 'وا', 'واهاً', 'وراءَك', 'وُشْكَانَ', 'وَيْ', 'يفعلان', 'تفعلان', 'يفعلون', 'تفعلون', 'تفعلين', 'اتخذ', 'ألفى', 'تخذ', 'ترك', 'تعلَّم', 'جعل', 'حجا', 'حبيب', 'خال', 'حسب', 'خال', 'درى', 'رأى', 'زعم', 'صبر', 'ظنَّ', 'عدَّ', 'علم', 'غادر', 'ذهب', 'وجد', 'ورد', 'وهب', 'أسكن', 'أطعم', 'أعطى', 'رزق', 'زود', 'سقى', 'كسا', 'أخبر', 'أرى', 'أعلم', 'أنبأ', 'حدَث', 'خبَّر', 'نبَّا', 'أفعل به', 'ما أفعله', 'بئس', 'ساء', 'طالما', 'قلما', 'لات', 'لكنَّ', 'ءَ', 'أجل', 'إذاً', 'أمّا', 'إمّا', 'إنَّ', 'أنًّ', 'أى', 'إى', 'أيا', 'ب', 'ثمَّ', 'جلل', 'جير', 'رُبَّ', 'س', 'علًّ', 'ف', 'كأنّ', 'كلَّا', 'كى', 'ل', 'لات', 'لعلَّ', 'لكنَّ', 'لكنَّ', 'م', 'نَّ', 'هلّا', 'وا', 'أل', 'إلّا', 'ت', 'ك', 'لمّا', 'ن', 'ه', 'و', 'ا', 'ي', 'تجاه', 'تلقاء', 'جميع', 'حسب', 'سبحان', 'شبه', 'لعمر', 'مثل', 'معاذ', 'أبو', 'أخو', 'حمو', 'فو', 'مئة', 'مئتان', 'ثلاثمئة', 'أربعمئة', 'خمسمئة', 'ستمئة', 'سبعمئة', 'ثمنمئة', 'تسعمئة', 'مائة', 'ثلاثمائة', 'أربعمائة', 'خمسمائة', 'ستمائة', 'سبعمائة', 'ثمانمئة', 'تسعمائة', 'عشرون', 'ثلاثون', 'اربعون', 'خمسون', 'ستون', 'سبعون', 'ثمانون', 'تسعون', 'عشرين', 'ثلاثين', 'اربعين', 'خمسين', 'ستين', 'سبعين', 'ثمانين', 'تسعين', 'بضع', 'نيف', 'أجمع', 'جميع', 'عامة', 'عين', 'نفس', 'لا سيما', 'أصلا', 'أهلا', 'أيضا', 'بؤسا', 'بعدا', 'بغتة', 'تعسا', 'حقا', 'حمدا', 'خلافا', 'خاصة', 'دواليك', 'سحقا', 'سرا', 'سمعا', 'صبرا', 'صدقا', 'صراحة', 'طرا', 'عجبا', 'عيانا', 'غالبا', 'فرادى', 'فضلا', 'قاطبة', 'كثيرا', 'لبيك', 'معاذ', 'أبدا', 'إزاء', 'أصلا', 'الآن', 'أمد', 'أمس', 'آنفا', 'آناء', 'أنّى', 'أول', 'أيّان', 'تارة', 'ثمّ', 'ثمّة', 'حقا', 'صباح', 'مساء', 'ضحوة', 'عوض', 'غدا', 'غداة', 'قطّ', 'كلّما', 'لدن', 'لمّا', 'مرّة', 'قبل', 'خلف', 'أمام', 'فوق', 'تحت', 'يمين', 'شمال', 'ارتدّ', 'استحال', 'أصبح', 'أضحى', 'آض', 'أمسى', 'انقلب', 'بات', 'تبدّل', 'تحوّل', 'حار', 'رجع', 'راح', 'صار', 'ظلّ', 'عاد', 'غدا', 'كان', 'ما انفك', 'ما برح', 'مادام', 'مازال', 'مافتئ', 'ابتدأ', 'أخذ', 'اخلولق', 'أقبل', 'انبرى', 'أنشأ', 'أوشك', 'جعل', 'حرى', 'شرع', 'طفق', 'علق', 'قام', 'كرب', 'كاد', 'هبّ']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# Arabic stopwords\n",
    "arabic_stopwords = stopwords.words('arabic')\n",
    "\n",
    "# Print some of the stopwords\n",
    "print(arabic_stopwords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: aiogoogletrans in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (3.3.3)\n",
      "Requirement already satisfied: httpx==0.23.0 in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx[http2]==0.23.0->aiogoogletrans) (0.23.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx==0.23.0->httpx[http2]==0.23.0->aiogoogletrans) (2024.2.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx==0.23.0->httpx[http2]==0.23.0->aiogoogletrans) (1.3.0)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rfc3986[idna2008]<2,>=1.3->httpx==0.23.0->httpx[http2]==0.23.0->aiogoogletrans) (1.5.0)\n",
      "Requirement already satisfied: httpcore<0.16.0,>=0.15.0 in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx==0.23.0->httpx[http2]==0.23.0->aiogoogletrans) (0.15.0)\n",
      "Requirement already satisfied: h2<5,>=3 in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx[http2]==0.23.0->aiogoogletrans) (4.1.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from h2<5,>=3->httpx[http2]==0.23.0->aiogoogletrans) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from h2<5,>=3->httpx[http2]==0.23.0->aiogoogletrans) (4.0.0)\n",
      "Requirement already satisfied: h11<0.13,>=0.11 in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpcore<0.16.0,>=0.15.0->httpx==0.23.0->httpx[http2]==0.23.0->aiogoogletrans) (0.12.0)\n",
      "Requirement already satisfied: anyio==3.* in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpcore<0.16.0,>=0.15.0->httpx==0.23.0->httpx[http2]==0.23.0->aiogoogletrans) (3.7.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from anyio==3.*->httpcore<0.16.0,>=0.15.0->httpx==0.23.0->httpx[http2]==0.23.0->aiogoogletrans) (3.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install aiogoogletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: unidecode in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.3.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: emoji in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.10.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- get hashtags\n",
    "2- get mentions\n",
    "3- get urls\n",
    "4- remove hashtags\n",
    "5- treat mentions \"remove only @\"\n",
    "6- replace emojis \"only arabic case is treated\"\n",
    "7- text = re.sub('http\\S+\\s*', ' ', text)  # remove URLs\n",
    "7- remove punctuations\n",
    "10- remove stop words \n",
    "8- remove non used characters \n",
    "9- remove repeated characters\n",
    "11- for arabic remove tashkeel\n",
    "12- Strip tatweel from a text.\n",
    "13- stem words\n",
    "14- normalizeArabic\n",
    "15- tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_hashtag_to_words(tag):\n",
    "    tag = tag.replace('#','')\n",
    "    tags = tag.split('_')\n",
    "    if len(tags) > 1 :\n",
    "        \n",
    "        return tags\n",
    "    pattern = re.compile(r\"[A-Z][a-z]+|\\d+|[A-Z]+(?![a-z])\")\n",
    "    return pattern.findall(tag)\n",
    "\n",
    "def extract_hashtag(text):\n",
    "    \n",
    "    hash_list = ([re.sub(r\"(\\W+)$\", \"\", i) for i in text.split() if i.startswith(\"#\")])\n",
    "    word_list = []\n",
    "    for word in hash_list :\n",
    "        word_list.extend(split_hashtag_to_words(word))\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['مستخدم1', 'مستخدم2']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_mentions_arabic(text):\n",
    "    # Find all mentions starting with '@' and extract the username\n",
    "    mentions = re.findall(r'@(\\w+)', text)\n",
    "    return mentions\n",
    "\n",
    "# Example usage:\n",
    "text_arabic = \"مرحبا @مستخدم1 و @مستخدم2، كيف حالكم؟\"\n",
    "mentions_arabic = extract_mentions_arabic(text_arabic)\n",
    "print(mentions_arabic)  # Output: ['مستخدم1', 'مستخدم2']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- get urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://example.com', 'https://example.org']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_urls(text):\n",
    "    # Regular expression pattern to match URLs\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    # Find all URLs in the text\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    return urls\n",
    "\n",
    "# Example usage:\n",
    "arabic_text_with_urls = \"قم بزيارة هذا الموقع: https://example.com، وأيضًا https://example.org\"\n",
    "urls = extract_urls(arabic_text_with_urls)\n",
    "print(urls)  # Output: ['https://example.com', 'https://example.org']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- treat hashtags : remove hashtags #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hashtag(text):\n",
    "    words = text.split()\n",
    "    text = list()\n",
    "    for word in words:\n",
    "        if is_hashtag(word):\n",
    "            text.extend(extract_hashtag(word))\n",
    "        else:\n",
    "            text.append(word)\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "def is_hashtag(word):\n",
    "    if word.startswith(\"#\"):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def remove_hashtag_char(text):\n",
    "    # Regular expression pattern to match mentions\n",
    "    mention_pattern = r'#+'\n",
    "    # Replace mentions with an empty string\n",
    "    cleaned_text = re.sub(mention_pattern, '', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مرحبا @مستخدم1، كيف حالك؟ منشن dfhg  لـ @مستخدم2 في هذا التغريدة.\n"
     ]
    }
   ],
   "source": [
    "# Example usage with Arabic text:\n",
    "text_with_mentions_arabic = \"مرحبا @مستخدم1، كيف حالك؟ منشن #dfhg#  لـ @مستخدم2 في هذا التغريدة.\"\n",
    "cleaned_text_arabic = remove_hashtag_char(text_with_mentions_arabic)\n",
    "print(cleaned_text_arabic)  # Output: \"مرحبا ، كيف حالك؟ منشن لـ في هذا التغريدة.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5- remove mentions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مرحبا مستخدم1، كيف حالك؟ منشن لـ مستخدم2 في هذا التغريدة.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_mentions(text):\n",
    "    # Regular expression pattern to match mentions\n",
    "    mention_pattern = r'@+'\n",
    "    # Replace mentions with an empty string\n",
    "    cleaned_text = re.sub(mention_pattern, '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "# Example usage with Arabic text:\n",
    "text_with_mentions_arabic = \"مرحبا @مستخدم1، كيف حالك؟ منشن لـ @مستخدم2 في هذا التغريدة.\"\n",
    "cleaned_text_arabic = remove_mentions(text_with_mentions_arabic)\n",
    "print(cleaned_text_arabic)  # Output: \"مرحبا ، كيف حالك؟ منشن لـ في هذا التغريدة.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6- replace emojis \"only arabic case is treated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:37: SyntaxWarning: invalid escape sequence '\\)'\n",
      "<>:40: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:40: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:43: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<>:43: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<>:47: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:47: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:50: SyntaxWarning: invalid escape sequence '\\)'\n",
      "<>:50: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:53: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<>:53: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<>:37: SyntaxWarning: invalid escape sequence '\\)'\n",
      "<>:40: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:40: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:43: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<>:43: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<>:47: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:47: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:50: SyntaxWarning: invalid escape sequence '\\)'\n",
      "<>:50: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:53: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<>:53: SyntaxWarning: invalid escape sequence '\\/'\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5796\\3581009673.py:37: SyntaxWarning: invalid escape sequence '\\)'\n",
      "  for s in [\"\\)\", \"d\", \"]\", \"}\",\"p\"]:\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5796\\3581009673.py:40: SyntaxWarning: invalid escape sequence '\\('\n",
      "  for s in [\"\\(\", \"\\[\", \"{\"]:\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5796\\3581009673.py:40: SyntaxWarning: invalid escape sequence '\\['\n",
      "  for s in [\"\\(\", \"\\[\", \"{\"]:\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5796\\3581009673.py:43: SyntaxWarning: invalid escape sequence '\\|'\n",
      "  for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5796\\3581009673.py:43: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5796\\3581009673.py:47: SyntaxWarning: invalid escape sequence '\\('\n",
      "  for s in [\"\\(\", \"\\[\", \"{\"]:\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5796\\3581009673.py:47: SyntaxWarning: invalid escape sequence '\\['\n",
      "  for s in [\"\\(\", \"\\[\", \"{\"]:\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5796\\3581009673.py:50: SyntaxWarning: invalid escape sequence '\\)'\n",
      "  for s in [\"\\)\", \"\\]\", \"}\"]:\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5796\\3581009673.py:50: SyntaxWarning: invalid escape sequence '\\]'\n",
      "  for s in [\"\\)\", \"\\]\", \"}\"]:\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5796\\3581009673.py:53: SyntaxWarning: invalid escape sequence '\\|'\n",
      "  for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5796\\3581009673.py:53: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n"
     ]
    }
   ],
   "source": [
    "with open('emojis.csv','r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    emojis_ar = {}\n",
    "    for line in lines:\n",
    "        line = line.strip('\\n').split(';')\n",
    "        emojis_ar.update({line[0].strip():line[1].strip()})\n",
    "\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "import unicodedata\n",
    "from unidecode import unidecode\n",
    "\n",
    "def emoji_native_translation(text):\n",
    "    text = text.lower()\n",
    "    loves = [\"<3\", \"♥\",'❤']\n",
    "    smilefaces = []\n",
    "    sadfaces = []\n",
    "    neutralfaces = []\n",
    "\n",
    "    eyes = [\"8\",\":\",\"=\",\";\"]\n",
    "    nose = [\"'\",\"`\",\"-\",r\"\\\\\"]\n",
    "    for e in eyes:\n",
    "        for n in nose:\n",
    "            for s in [\"\\)\", \"d\", \"]\", \"}\",\"p\"]:\n",
    "                smilefaces.append(e+n+s)\n",
    "                smilefaces.append(e+s)\n",
    "            for s in [\"\\(\", \"\\[\", \"{\"]:\n",
    "                sadfaces.append(e+n+s)\n",
    "                sadfaces.append(e+s)\n",
    "            for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n",
    "                neutralfaces.append(e+n+s)\n",
    "                neutralfaces.append(e+s)\n",
    "            #reversed\n",
    "            for s in [\"\\(\", \"\\[\", \"{\"]:\n",
    "                smilefaces.append(s+n+e)\n",
    "                smilefaces.append(s+e)\n",
    "            for s in [\"\\)\", \"\\]\", \"}\"]:\n",
    "                sadfaces.append(s+n+e)\n",
    "                sadfaces.append(s+e)\n",
    "            for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n",
    "                neutralfaces.append(s+n+e)\n",
    "                neutralfaces.append(s+e)\n",
    "\n",
    "    smilefaces = list(set(smilefaces))\n",
    "    sadfaces = list(set(sadfaces))\n",
    "    neutralfaces = list(set(neutralfaces))\n",
    "    t = []\n",
    "    for w in text.split():\n",
    "        if w in loves:\n",
    "            t.append(\"حب\")\n",
    "        elif w in smilefaces:\n",
    "            t.append(\"مضحك\")\n",
    "        elif w in neutralfaces:\n",
    "            t.append(\"عادي\")\n",
    "        elif w in sadfaces:\n",
    "            t.append(\"محزن\")\n",
    "        else:\n",
    "            t.append(w)\n",
    "    newText = \" \".join(t)\n",
    "    return newText\n",
    "\n",
    "import emoji\n",
    "def is_emoji(word):\n",
    "    if word in emojis_ar:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def add_space(text):\n",
    "    return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()\n",
    "\n",
    "from aiogoogletrans import Translator\n",
    "translator = Translator()\n",
    "import asyncio\n",
    "loop = asyncio.get_event_loop()\n",
    "def translate_emojis(words):\n",
    "    word_list = list()\n",
    "    words_to_translate = list()\n",
    "    for word in words :\n",
    "        t = emojis_ar.get(word.get('emoji'),None)\n",
    "        if t is None:\n",
    "            word.update({'translation':'عادي','translated':True})\n",
    "            #words_to_translate.append('normal')\n",
    "        else:\n",
    "            word.update({'translated':False,'translation':t})\n",
    "            words_to_translate.append(t.replace(':','').replace('_',' '))\n",
    "        word_list.append(word)\n",
    "    return word_list\n",
    "\n",
    "def emoji_unicode_translation(text):\n",
    "    text = add_space(text)\n",
    "    words = text.split()\n",
    "    text_list = list()\n",
    "    emojis_list = list()\n",
    "    c = 0\n",
    "    for word in words:\n",
    "        if is_emoji(word):\n",
    "            emojis_list.append({'emoji':word,'emplacement':c})\n",
    "        else:\n",
    "            text_list.append(word)\n",
    "        c+=1\n",
    "    emojis_translated = translate_emojis(emojis_list)\n",
    "    for em in emojis_translated:\n",
    "        text_list.insert(em.get('emplacement'),em.get('translation'))\n",
    "    text = \" \".join(text_list)\n",
    "    return text\n",
    "    \n",
    "def clean_emoji(text):\n",
    "    text = emoji_native_translation(text)\n",
    "    text = emoji_unicode_translation(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "أحبك قلب أحمر احمر خدود\n",
      "أنا حزين جدًا يبكي\n",
      "أنا في حالة مزاجية جيدة ابتسامة\n"
     ]
    }
   ],
   "source": [
    "text_with_emojis = \"أحبك ❤️😊\"\n",
    "cleaned_text = clean_emoji(text_with_emojis)\n",
    "print(cleaned_text)\n",
    "# Output: \"أحبك حب مضحك\"\n",
    "\n",
    "text_with_emojis = \"أنا حزين جدًا 😢\"\n",
    "cleaned_text = clean_emoji(text_with_emojis)\n",
    "print(cleaned_text)\n",
    "# Output: \"أنا حزين جدًا محزن\"\n",
    "\n",
    "text_with_emojis = \"أنا في حالة مزاجية جيدة 😄\"\n",
    "cleaned_text = clean_emoji(text_with_emojis)\n",
    "print(cleaned_text)\n",
    "# Output: \"أنا في حالة مزاجية جيدة مضحك\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7- text = re.sub('http\\S+\\s*', ' ', text)  # remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_urls(text):\n",
    "    # Remove URLs from the text\n",
    "    cleaned_text = re.sub(r'http\\S+\\s*', '', text)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Arabic text: تحقق من هذا الرابط: https://example.com، إنه رائع!\n",
      "Arabic text with URLs removed: تحقق من هذا الرابط: إنه رائع!\n"
     ]
    }
   ],
   "source": [
    "# Arabic example\n",
    "arabic_text = \"تحقق من هذا الرابط: https://example.com، إنه رائع!\"\n",
    "cleaned_arabic_text = remove_urls(arabic_text)\n",
    "print(\"Original Arabic text:\", arabic_text)\n",
    "print(\"Arabic text with URLs removed:\", cleaned_arabic_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7- remove punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\]'\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5796\\1715161554.py:7: SyntaxWarning: invalid escape sequence '\\]'\n",
      "  text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~\"\"\"), ' ', text)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "all_punctuation = string.punctuation\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    ## Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~\"\"\"), ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Arabic text: تحقق من هذا الرابط: https://example.com، إنه رائع!\n",
      "Arabic text with punctuations removed: تحقق من هذا الرابط  https   example com  إنه رائع \n"
     ]
    }
   ],
   "source": [
    "# Arabic example\n",
    "arabic_text = \"تحقق من هذا الرابط: https://example.com، إنه رائع!\"\n",
    "cleaned_arabic_text = remove_punctuations(arabic_text)\n",
    "print(\"Original Arabic text:\", arabic_text)\n",
    "print(\"Arabic text with punctuations removed:\", cleaned_arabic_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10- remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي', 'الذي', 'الذين', 'اللاتي', 'اللائي', 'اللتان', 'اللتيا', 'اللتين', 'اللذان', 'اللذين', 'اللواتي', 'إلى', 'إليك', 'إليكم', 'إليكما', 'إليكن', 'أم', 'أما', 'أما', 'إما', 'أن', 'إن', 'إنا', 'أنا', 'أنت', 'أنتم', 'أنتما', 'أنتن', 'إنما', 'إنه', 'أنى', 'أنى', 'آه', 'آها', 'أو', 'أولاء', 'أولئك', 'أوه', 'آي', 'أي', 'أيها', 'إي', 'أين', 'أين', 'أينما', 'إيه', 'بخ', 'بس', 'بعد', 'بعض', 'بك', 'بكم', 'بكم', 'بكما', 'بكن', 'بل', 'بلى', 'بما', 'بماذا', 'بمن', 'بنا', 'به', 'بها', 'بهم', 'بهما', 'بهن', 'بي', 'بين', 'بيد', 'تلك', 'تلكم', 'تلكما', 'ته', 'تي', 'تين', 'تينك', 'ثم', 'ثمة', 'حاشا', 'حبذا', 'حتى', 'حيث', 'حيثما', 'حين', 'خلا', 'دون', 'ذا', 'ذات', 'ذاك', 'ذان', 'ذانك', 'ذلك', 'ذلكم', 'ذلكما', 'ذلكن', 'ذه', 'ذو', 'ذوا', 'ذواتا', 'ذواتي', 'ذي', 'ذين', 'ذينك', 'ريث', 'سوف', 'سوى', 'شتان', 'عدا', 'عسى', 'عل', 'على', 'عليك', 'عليه', 'عما', 'عن', 'عند', 'غير', 'فإذا', 'فإن', 'فلا', 'فمن', 'في', 'فيم', 'فيما', 'فيه', 'فيها', 'قد', 'كأن', 'كأنما', 'كأي', 'كأين', 'كذا', 'كذلك', 'كل', 'كلا', 'كلاهما', 'كلتا', 'كلما', 'كليكما', 'كليهما', 'كم', 'كم', 'كما', 'كي', 'كيت', 'كيف', 'كيفما', 'لا', 'لاسيما', 'لدى', 'لست', 'لستم', 'لستما', 'لستن', 'لسن', 'لسنا', 'لعل', 'لك', 'لكم', 'لكما', 'لكن', 'لكنما', 'لكي', 'لكيلا', 'لم', 'لما', 'لن', 'لنا', 'له', 'لها', 'لهم', 'لهما', 'لهن', 'لو', 'لولا', 'لوما', 'لي', 'لئن', 'ليت', 'ليس', 'ليسا', 'ليست', 'ليستا', 'ليسوا', 'ما', 'ماذا', 'متى', 'مذ', 'مع', 'مما', 'ممن', 'من', 'منه', 'منها', 'منذ', 'مه', 'مهما', 'نحن', 'نحو', 'نعم', 'ها', 'هاتان', 'هاته', 'هاتي', 'هاتين', 'هاك', 'هاهنا', 'هذا', 'هذان', 'هذه', 'هذي', 'هذين', 'هكذا', 'هل', 'هلا', 'هم', 'هما', 'هن', 'هنا', 'هناك', 'هنالك', 'هو', 'هؤلاء', 'هي', 'هيا', 'هيت', 'هيهات', 'والذي', 'والذين', 'وإذ', 'وإذا', 'وإن', 'ولا', 'ولكن', 'ولو', 'وما', 'ومن', 'وهو', 'يا', 'أبٌ', 'أخٌ', 'حمٌ', 'فو', 'أنتِ', 'يناير', 'فبراير', 'مارس', 'أبريل', 'مايو', 'يونيو', 'يوليو', 'أغسطس', 'سبتمبر', 'أكتوبر', 'نوفمبر', 'ديسمبر', 'جانفي', 'فيفري', 'مارس', 'أفريل', 'ماي', 'جوان', 'جويلية', 'أوت', 'كانون', 'شباط', 'آذار', 'نيسان', 'أيار', 'حزيران', 'تموز', 'آب', 'أيلول', 'تشرين', 'دولار', 'دينار', 'ريال', 'درهم', 'ليرة', 'جنيه', 'قرش', 'مليم', 'فلس', 'هللة', 'سنتيم', 'يورو', 'ين', 'يوان', 'شيكل', 'واحد', 'اثنان', 'ثلاثة', 'أربعة', 'خمسة', 'ستة', 'سبعة', 'ثمانية', 'تسعة', 'عشرة', 'أحد', 'اثنا', 'اثني', 'إحدى', 'ثلاث', 'أربع', 'خمس', 'ست', 'سبع', 'ثماني', 'تسع', 'عشر', 'ثمان', 'سبت', 'أحد', 'اثنين', 'ثلاثاء', 'أربعاء', 'خميس', 'جمعة', 'أول', 'ثان', 'ثاني', 'ثالث', 'رابع', 'خامس', 'سادس', 'سابع', 'ثامن', 'تاسع', 'عاشر', 'حادي', 'أ', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'ء', 'ى', 'آ', 'ؤ', 'ئ', 'أ', 'ة', 'ألف', 'باء', 'تاء', 'ثاء', 'جيم', 'حاء', 'خاء', 'دال', 'ذال', 'راء', 'زاي', 'سين', 'شين', 'صاد', 'ضاد', 'طاء', 'ظاء', 'عين', 'غين', 'فاء', 'قاف', 'كاف', 'لام', 'ميم', 'نون', 'هاء', 'واو', 'ياء', 'همزة', 'ي', 'نا', 'ك', 'كن', 'ه', 'إياه', 'إياها', 'إياهما', 'إياهم', 'إياهن', 'إياك', 'إياكما', 'إياكم', 'إياك', 'إياكن', 'إياي', 'إيانا', 'أولالك', 'تانِ', 'تانِك', 'تِه', 'تِي', 'تَيْنِ', 'ثمّ', 'ثمّة', 'ذانِ', 'ذِه', 'ذِي', 'ذَيْنِ', 'هَؤلاء', 'هَاتانِ', 'هَاتِه', 'هَاتِي', 'هَاتَيْنِ', 'هَذا', 'هَذانِ', 'هَذِه', 'هَذِي', 'هَذَيْنِ', 'الألى', 'الألاء', 'أل', 'أنّى', 'أيّ', 'ّأيّان', 'أنّى', 'أيّ', 'ّأيّان', 'ذيت', 'كأيّ', 'كأيّن', 'بضع', 'فلان', 'وا', 'آمينَ', 'آهِ', 'آهٍ', 'آهاً', 'أُفٍّ', 'أُفٍّ', 'أفٍّ', 'أمامك', 'أمامكَ', 'أوّهْ', 'إلَيْكَ', 'إلَيْكَ', 'إليكَ', 'إليكنّ', 'إيهٍ', 'بخٍ', 'بسّ', 'بَسْ', 'بطآن', 'بَلْهَ', 'حاي', 'حَذارِ', 'حيَّ', 'حيَّ', 'دونك', 'رويدك', 'سرعان', 'شتانَ', 'شَتَّانَ', 'صهْ', 'صهٍ', 'طاق', 'طَق', 'عَدَسْ', 'كِخ', 'مكانَك', 'مكانَك', 'مكانَك', 'مكانكم', 'مكانكما', 'مكانكنّ', 'نَخْ', 'هاكَ', 'هَجْ', 'هلم', 'هيّا', 'هَيْهات', 'وا', 'واهاً', 'وراءَك', 'وُشْكَانَ', 'وَيْ', 'يفعلان', 'تفعلان', 'يفعلون', 'تفعلون', 'تفعلين', 'اتخذ', 'ألفى', 'تخذ', 'ترك', 'تعلَّم', 'جعل', 'حجا', 'حبيب', 'خال', 'حسب', 'خال', 'درى', 'رأى', 'زعم', 'صبر', 'ظنَّ', 'عدَّ', 'علم', 'غادر', 'ذهب', 'وجد', 'ورد', 'وهب', 'أسكن', 'أطعم', 'أعطى', 'رزق', 'زود', 'سقى', 'كسا', 'أخبر', 'أرى', 'أعلم', 'أنبأ', 'حدَث', 'خبَّر', 'نبَّا', 'أفعل به', 'ما أفعله', 'بئس', 'ساء', 'طالما', 'قلما', 'لات', 'لكنَّ', 'ءَ', 'أجل', 'إذاً', 'أمّا', 'إمّا', 'إنَّ', 'أنًّ', 'أى', 'إى', 'أيا', 'ب', 'ثمَّ', 'جلل', 'جير', 'رُبَّ', 'س', 'علًّ', 'ف', 'كأنّ', 'كلَّا', 'كى', 'ل', 'لات', 'لعلَّ', 'لكنَّ', 'لكنَّ', 'م', 'نَّ', 'هلّا', 'وا', 'أل', 'إلّا', 'ت', 'ك', 'لمّا', 'ن', 'ه', 'و', 'ا', 'ي', 'تجاه', 'تلقاء', 'جميع', 'حسب', 'سبحان', 'شبه', 'لعمر', 'مثل', 'معاذ', 'أبو', 'أخو', 'حمو', 'فو', 'مئة', 'مئتان', 'ثلاثمئة', 'أربعمئة', 'خمسمئة', 'ستمئة', 'سبعمئة', 'ثمنمئة', 'تسعمئة', 'مائة', 'ثلاثمائة', 'أربعمائة', 'خمسمائة', 'ستمائة', 'سبعمائة', 'ثمانمئة', 'تسعمائة', 'عشرون', 'ثلاثون', 'اربعون', 'خمسون', 'ستون', 'سبعون', 'ثمانون', 'تسعون', 'عشرين', 'ثلاثين', 'اربعين', 'خمسين', 'ستين', 'سبعين', 'ثمانين', 'تسعين', 'بضع', 'نيف', 'أجمع', 'جميع', 'عامة', 'عين', 'نفس', 'لا سيما', 'أصلا', 'أهلا', 'أيضا', 'بؤسا', 'بعدا', 'بغتة', 'تعسا', 'حقا', 'حمدا', 'خلافا', 'خاصة', 'دواليك', 'سحقا', 'سرا', 'سمعا', 'صبرا', 'صدقا', 'صراحة', 'طرا', 'عجبا', 'عيانا', 'غالبا', 'فرادى', 'فضلا', 'قاطبة', 'كثيرا', 'لبيك', 'معاذ', 'أبدا', 'إزاء', 'أصلا', 'الآن', 'أمد', 'أمس', 'آنفا', 'آناء', 'أنّى', 'أول', 'أيّان', 'تارة', 'ثمّ', 'ثمّة', 'حقا', 'صباح', 'مساء', 'ضحوة', 'عوض', 'غدا', 'غداة', 'قطّ', 'كلّما', 'لدن', 'لمّا', 'مرّة', 'قبل', 'خلف', 'أمام', 'فوق', 'تحت', 'يمين', 'شمال', 'ارتدّ', 'استحال', 'أصبح', 'أضحى', 'آض', 'أمسى', 'انقلب', 'بات', 'تبدّل', 'تحوّل', 'حار', 'رجع', 'راح', 'صار', 'ظلّ', 'عاد', 'غدا', 'كان', 'ما انفك', 'ما برح', 'مادام', 'مازال', 'مافتئ', 'ابتدأ', 'أخذ', 'اخلولق', 'أقبل', 'انبرى', 'أنشأ', 'أوشك', 'جعل', 'حرى', 'شرع', 'طفق', 'علق', 'قام', 'كرب', 'كاد', 'هبّ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "# Arabic stopwords\n",
    "arabic_stopwords = stopwords.words('arabic')\n",
    "\n",
    "# Print some of the stopwords\n",
    "print(arabic_stopwords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_arabic_stopwords(text):\n",
    "    # Load Arabic stopwords\n",
    "    arabic_stopwords = set(stopwords.words('arabic'))\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in words if word.lower() not in arabic_stopwords]\n",
    "    \n",
    "    # Recreate the text from filtered words\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: هذا هو نص تجريبي يحتوي على بعض  وأيضًا الكلمات العربية العادية\n",
      "Text with Arabic stopwords removed: نص تجريبي يحتوي وأيضًا الكلمات العربية العادية\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"هذا هو نص تجريبي يحتوي على بعض  وأيضًا الكلمات العربية العادية\"\n",
    "cleaned_text = remove_arabic_stopwords(text)\n",
    "print(\"Original text:\", text)\n",
    "print(\"Text with Arabic stopwords removed:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8- remove non used characters \"do it for each language\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_non_arabic(text):\n",
    "    # Define regex pattern to match non-Arabic characters and non-numeric characters\n",
    "    arabic_pattern = re.compile(r'[^\\u0600-\\u06FF\\u0750-\\u077F\\d\\s]+')\n",
    "    \n",
    "    # Remove non-Arabic and non-numeric characters from the text\n",
    "    cleaned_text = arabic_pattern.sub('', text)\n",
    "    \n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: هذا هو نص (تجريبي) gghhgj يحتوي 123 على بعض الكلمات.,; () العربية  1315العادية!\n",
      "Text with non-Arabic characters removed: هذا هو نص تجريبي  يحتوي 123 على بعض الكلمات  العربية  1315العادية\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"هذا هو نص (تجريبي) gghhgj يحتوي 123 على بعض الكلمات.,; () العربية  1315العادية!\"\n",
    "cleaned_text = remove_non_arabic(text)\n",
    "print(\"Original text:\", text)\n",
    "print(\"Text with non-Arabic characters removed:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9- remove repeated characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_repeated_characters(text):\n",
    "    # Define regex pattern to match repeated Arabic characters\n",
    "    repeated_pattern = re.compile(r'(\\S)(\\1)+', re.UNICODE)\n",
    "    \n",
    "    # Remove repeated characters from the text\n",
    "    cleaned_text = repeated_pattern.sub(r'\\1', text)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: هذاااا هوووو نص مككككككرررررررررررررررررررررررررر\n",
      "Text with repeated characters removed: هذا هو نص مكر\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "text = \"هذاااا هوووو نص مككككككرررررررررررررررررررررررررر\"\n",
    "cleaned_text = remove_repeated_characters(text)\n",
    "print(\"Original text:\", text)\n",
    "print(\"Text with repeated characters removed:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11- for arabic remove tashkeel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarabic.araby as araby\n",
    "text= araby.strip_tashkeel(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strip tatweel from a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "العربية\n"
     ]
    }
   ],
   "source": [
    "from pyarabic.araby import strip_tatweel\n",
    "text = u\"العـــــربية\"\n",
    "print(strip_tatweel(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalizeArabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeArabic(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    noise = re.compile(\"\"\" ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    text = re.sub(noise, '', text)\n",
    "    text = re.sub(r'(.)\\1+', r\"\\1\\1\", text) # Remove longation\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stemming words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: snowballstemmer in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install snowballstemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ياكل\n"
     ]
    }
   ],
   "source": [
    "from snowballstemmer import stemmer\n",
    "\n",
    "def stem_arabic_word(word):\n",
    "    # Create an Arabic stemmer using the Khoja algorithm\n",
    "    arabic_stemmer = stemmer(\"arabic\")\n",
    "    # Stem the word\n",
    "    stemmed_word = arabic_stemmer.stemWord(word)\n",
    "    return stemmed_word\n",
    "\n",
    "# Example usage:\n",
    "word = \"يأكلون\"\n",
    "stemmed_word = stem_arabic_word(word)\n",
    "print(stemmed_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word: يقرأ\n",
      "Stemmed word: قرأ\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.isri import ISRIStemmer\n",
    "\n",
    "# Initialize ISRIStemmer\n",
    "arabic_stemmer = ISRIStemmer()\n",
    "\n",
    "# Example Arabic word\n",
    "word = \"يقرأ\"\n",
    "\n",
    "# Stem the word\n",
    "stemmed_word = arabic_stemmer.stem(word)\n",
    "\n",
    "print(\"Original word:\", word)\n",
    "print(\"Stemmed word:\", stemmed_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ِاسمٌ', 'الكلبِ', 'في', 'اللغةِ', 'الإنجليزية', 'واسمُ', 'الحمارِ']"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyarabic.araby import tokenize, is_arabicrange, strip_tashkeel\n",
    "text = u\"ِاسمٌ الكلبِ في 21 اللغةِ الإنجليزية Dog واسمُ الحمارِ Donky\"\n",
    "tokenize(text, conditions=is_arabicrange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the proposed algorithm to treat the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_hashtag_to_words(tag):\n",
    "    tag = tag.replace('#','')\n",
    "    tags = tag.split('_')\n",
    "    if len(tags) > 1 :\n",
    "        \n",
    "        return tags\n",
    "    pattern = re.compile(r\"[A-Z][a-z]+|\\d+|[A-Z]+(?![a-z])\")\n",
    "    return pattern.findall(tag)\n",
    "\n",
    "def extract_hashtag(text):\n",
    "    \n",
    "    hash_list = ([re.sub(r\"(\\W+)$\", \"\", i) for i in text.split() if i.startswith(\"#\")])\n",
    "    word_list = []\n",
    "    for word in hash_list :\n",
    "        word_list.extend(split_hashtag_to_words(word))\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['مستخدم1', 'مستخدم2']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_mentions_arabic(text):\n",
    "    # Find all mentions starting with '@' and extract the username\n",
    "    mentions = re.findall(r'@(\\w+)', text)\n",
    "    return mentions\n",
    "\n",
    "# Example usage:\n",
    "text_arabic = \"مرحبا @مستخدم1 و @مستخدم2، كيف حالكم؟\"\n",
    "mentions_arabic = extract_mentions_arabic(text_arabic)\n",
    "print(mentions_arabic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://example.com', 'https://example.org']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_urls(text):\n",
    "    # Regular expression pattern to match URLs\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    # Find all URLs in the text\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    return urls\n",
    "\n",
    "# Example usage:\n",
    "arabic_text_with_urls = \"قم بزيارة هذا الموقع: https://example.com، وأيضًا https://example.org\"\n",
    "urls = extract_urls(arabic_text_with_urls)\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = clean_hashtag(text)\n",
    "    text = clean_emoji(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_punctuations(text)\n",
    "    text = remove_arabic_stopwords(text)\n",
    "    text = remove_non_arabic(text)\n",
    "    text = remove_repeated_characters(text)\n",
    "    tokens = tokenize(text)\n",
    "    print(tokens)\n",
    "    # Assuming tokens is a list of Arabic words\n",
    "    for i, word in enumerate(tokens):\n",
    "\n",
    "        # Remove diacritics (Tashkeel)\n",
    "        diacritic_removed_word = araby.strip_tashkeel(word)\n",
    "        \n",
    "        # Remove non-Arabic characters\n",
    "        arabic_only_word = remove_non_arabic(diacritic_removed_word)\n",
    "        \n",
    "        # Remove tatweel\n",
    "        final_word = strip_tatweel(arabic_only_word)\n",
    "\n",
    "        # Apply stemming (we need a good stemmer)\n",
    "        stemmed_word = ArabicLightStemmer().light_stem(final_word)\n",
    "    \n",
    "        # Apply normalization\n",
    "        normalized_word = normalizeArabic(stemmed_word)\n",
    "        \n",
    "        # Replace the original token with the processed one\n",
    "        tokens[i] = final_word\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['قم', 'بزيارة', '12', 'الموقع', 'أيضًا']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['قم', 'بزيارة', '12', 'الموقع', 'أيضا']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage:\n",
    "arabic_text_with_urls = \"قم بزيارة 12 هذا الموقع: https://example.com، و أيضًا https://example.org\"\n",
    "clean_text(arabic_text_with_urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
