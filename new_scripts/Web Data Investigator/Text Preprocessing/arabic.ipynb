{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyarabic in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.6.15)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pyarabic) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip  install pyarabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ø¥Ø°', 'Ø¥Ø°Ø§', 'Ø¥Ø°Ù…Ø§', 'Ø¥Ø°Ù†', 'Ø£Ù', 'Ø£Ù‚Ù„', 'Ø£ÙƒØ«Ø±', 'Ø£Ù„Ø§', 'Ø¥Ù„Ø§', 'Ø§Ù„ØªÙŠ', 'Ø§Ù„Ø°ÙŠ', 'Ø§Ù„Ø°ÙŠÙ†', 'Ø§Ù„Ù„Ø§ØªÙŠ', 'Ø§Ù„Ù„Ø§Ø¦ÙŠ', 'Ø§Ù„Ù„ØªØ§Ù†', 'Ø§Ù„Ù„ØªÙŠØ§', 'Ø§Ù„Ù„ØªÙŠÙ†', 'Ø§Ù„Ù„Ø°Ø§Ù†', 'Ø§Ù„Ù„Ø°ÙŠÙ†', 'Ø§Ù„Ù„ÙˆØ§ØªÙŠ', 'Ø¥Ù„Ù‰', 'Ø¥Ù„ÙŠÙƒ', 'Ø¥Ù„ÙŠÙƒÙ…', 'Ø¥Ù„ÙŠÙƒÙ…Ø§', 'Ø¥Ù„ÙŠÙƒÙ†', 'Ø£Ù…', 'Ø£Ù…Ø§', 'Ø£Ù…Ø§', 'Ø¥Ù…Ø§', 'Ø£Ù†', 'Ø¥Ù†', 'Ø¥Ù†Ø§', 'Ø£Ù†Ø§', 'Ø£Ù†Øª', 'Ø£Ù†ØªÙ…', 'Ø£Ù†ØªÙ…Ø§', 'Ø£Ù†ØªÙ†', 'Ø¥Ù†Ù…Ø§', 'Ø¥Ù†Ù‡', 'Ø£Ù†Ù‰', 'Ø£Ù†Ù‰', 'Ø¢Ù‡', 'Ø¢Ù‡Ø§', 'Ø£Ùˆ', 'Ø£ÙˆÙ„Ø§Ø¡', 'Ø£ÙˆÙ„Ø¦Ùƒ', 'Ø£ÙˆÙ‡', 'Ø¢ÙŠ', 'Ø£ÙŠ', 'Ø£ÙŠÙ‡Ø§', 'Ø¥ÙŠ', 'Ø£ÙŠÙ†', 'Ø£ÙŠÙ†', 'Ø£ÙŠÙ†Ù…Ø§', 'Ø¥ÙŠÙ‡', 'Ø¨Ø®', 'Ø¨Ø³', 'Ø¨Ø¹Ø¯', 'Ø¨Ø¹Ø¶', 'Ø¨Ùƒ', 'Ø¨ÙƒÙ…', 'Ø¨ÙƒÙ…', 'Ø¨ÙƒÙ…Ø§', 'Ø¨ÙƒÙ†', 'Ø¨Ù„', 'Ø¨Ù„Ù‰', 'Ø¨Ù…Ø§', 'Ø¨Ù…Ø§Ø°Ø§', 'Ø¨Ù…Ù†', 'Ø¨Ù†Ø§', 'Ø¨Ù‡', 'Ø¨Ù‡Ø§', 'Ø¨Ù‡Ù…', 'Ø¨Ù‡Ù…Ø§', 'Ø¨Ù‡Ù†', 'Ø¨ÙŠ', 'Ø¨ÙŠÙ†', 'Ø¨ÙŠØ¯', 'ØªÙ„Ùƒ', 'ØªÙ„ÙƒÙ…', 'ØªÙ„ÙƒÙ…Ø§', 'ØªÙ‡', 'ØªÙŠ', 'ØªÙŠÙ†', 'ØªÙŠÙ†Ùƒ', 'Ø«Ù…', 'Ø«Ù…Ø©', 'Ø­Ø§Ø´Ø§', 'Ø­Ø¨Ø°Ø§', 'Ø­ØªÙ‰', 'Ø­ÙŠØ«', 'Ø­ÙŠØ«Ù…Ø§', 'Ø­ÙŠÙ†', 'Ø®Ù„Ø§', 'Ø¯ÙˆÙ†', 'Ø°Ø§', 'Ø°Ø§Øª', 'Ø°Ø§Ùƒ', 'Ø°Ø§Ù†', 'Ø°Ø§Ù†Ùƒ', 'Ø°Ù„Ùƒ', 'Ø°Ù„ÙƒÙ…', 'Ø°Ù„ÙƒÙ…Ø§', 'Ø°Ù„ÙƒÙ†', 'Ø°Ù‡', 'Ø°Ùˆ', 'Ø°ÙˆØ§', 'Ø°ÙˆØ§ØªØ§', 'Ø°ÙˆØ§ØªÙŠ', 'Ø°ÙŠ', 'Ø°ÙŠÙ†', 'Ø°ÙŠÙ†Ùƒ', 'Ø±ÙŠØ«', 'Ø³ÙˆÙ', 'Ø³ÙˆÙ‰', 'Ø´ØªØ§Ù†', 'Ø¹Ø¯Ø§', 'Ø¹Ø³Ù‰', 'Ø¹Ù„', 'Ø¹Ù„Ù‰', 'Ø¹Ù„ÙŠÙƒ', 'Ø¹Ù„ÙŠÙ‡', 'Ø¹Ù…Ø§', 'Ø¹Ù†', 'Ø¹Ù†Ø¯', 'ØºÙŠØ±', 'ÙØ¥Ø°Ø§', 'ÙØ¥Ù†', 'ÙÙ„Ø§', 'ÙÙ…Ù†', 'ÙÙŠ', 'ÙÙŠÙ…', 'ÙÙŠÙ…Ø§', 'ÙÙŠÙ‡', 'ÙÙŠÙ‡Ø§', 'Ù‚Ø¯', 'ÙƒØ£Ù†', 'ÙƒØ£Ù†Ù…Ø§', 'ÙƒØ£ÙŠ', 'ÙƒØ£ÙŠÙ†', 'ÙƒØ°Ø§', 'ÙƒØ°Ù„Ùƒ', 'ÙƒÙ„', 'ÙƒÙ„Ø§', 'ÙƒÙ„Ø§Ù‡Ù…Ø§', 'ÙƒÙ„ØªØ§', 'ÙƒÙ„Ù…Ø§', 'ÙƒÙ„ÙŠÙƒÙ…Ø§', 'ÙƒÙ„ÙŠÙ‡Ù…Ø§', 'ÙƒÙ…', 'ÙƒÙ…', 'ÙƒÙ…Ø§', 'ÙƒÙŠ', 'ÙƒÙŠØª', 'ÙƒÙŠÙ', 'ÙƒÙŠÙÙ…Ø§', 'Ù„Ø§', 'Ù„Ø§Ø³ÙŠÙ…Ø§', 'Ù„Ø¯Ù‰', 'Ù„Ø³Øª', 'Ù„Ø³ØªÙ…', 'Ù„Ø³ØªÙ…Ø§', 'Ù„Ø³ØªÙ†', 'Ù„Ø³Ù†', 'Ù„Ø³Ù†Ø§', 'Ù„Ø¹Ù„', 'Ù„Ùƒ', 'Ù„ÙƒÙ…', 'Ù„ÙƒÙ…Ø§', 'Ù„ÙƒÙ†', 'Ù„ÙƒÙ†Ù…Ø§', 'Ù„ÙƒÙŠ', 'Ù„ÙƒÙŠÙ„Ø§', 'Ù„Ù…', 'Ù„Ù…Ø§', 'Ù„Ù†', 'Ù„Ù†Ø§', 'Ù„Ù‡', 'Ù„Ù‡Ø§', 'Ù„Ù‡Ù…', 'Ù„Ù‡Ù…Ø§', 'Ù„Ù‡Ù†', 'Ù„Ùˆ', 'Ù„ÙˆÙ„Ø§', 'Ù„ÙˆÙ…Ø§', 'Ù„ÙŠ', 'Ù„Ø¦Ù†', 'Ù„ÙŠØª', 'Ù„ÙŠØ³', 'Ù„ÙŠØ³Ø§', 'Ù„ÙŠØ³Øª', 'Ù„ÙŠØ³ØªØ§', 'Ù„ÙŠØ³ÙˆØ§', 'Ù…Ø§', 'Ù…Ø§Ø°Ø§', 'Ù…ØªÙ‰', 'Ù…Ø°', 'Ù…Ø¹', 'Ù…Ù…Ø§', 'Ù…Ù…Ù†', 'Ù…Ù†', 'Ù…Ù†Ù‡', 'Ù…Ù†Ù‡Ø§', 'Ù…Ù†Ø°', 'Ù…Ù‡', 'Ù…Ù‡Ù…Ø§', 'Ù†Ø­Ù†', 'Ù†Ø­Ùˆ', 'Ù†Ø¹Ù…', 'Ù‡Ø§', 'Ù‡Ø§ØªØ§Ù†', 'Ù‡Ø§ØªÙ‡', 'Ù‡Ø§ØªÙŠ', 'Ù‡Ø§ØªÙŠÙ†', 'Ù‡Ø§Ùƒ', 'Ù‡Ø§Ù‡Ù†Ø§', 'Ù‡Ø°Ø§', 'Ù‡Ø°Ø§Ù†', 'Ù‡Ø°Ù‡', 'Ù‡Ø°ÙŠ', 'Ù‡Ø°ÙŠÙ†', 'Ù‡ÙƒØ°Ø§', 'Ù‡Ù„', 'Ù‡Ù„Ø§', 'Ù‡Ù…', 'Ù‡Ù…Ø§', 'Ù‡Ù†', 'Ù‡Ù†Ø§', 'Ù‡Ù†Ø§Ùƒ', 'Ù‡Ù†Ø§Ù„Ùƒ', 'Ù‡Ùˆ', 'Ù‡Ø¤Ù„Ø§Ø¡', 'Ù‡ÙŠ', 'Ù‡ÙŠØ§', 'Ù‡ÙŠØª', 'Ù‡ÙŠÙ‡Ø§Øª', 'ÙˆØ§Ù„Ø°ÙŠ', 'ÙˆØ§Ù„Ø°ÙŠÙ†', 'ÙˆØ¥Ø°', 'ÙˆØ¥Ø°Ø§', 'ÙˆØ¥Ù†', 'ÙˆÙ„Ø§', 'ÙˆÙ„ÙƒÙ†', 'ÙˆÙ„Ùˆ', 'ÙˆÙ…Ø§', 'ÙˆÙ…Ù†', 'ÙˆÙ‡Ùˆ', 'ÙŠØ§', 'Ø£Ø¨ÙŒ', 'Ø£Ø®ÙŒ', 'Ø­Ù…ÙŒ', 'ÙÙˆ', 'Ø£Ù†ØªÙ', 'ÙŠÙ†Ø§ÙŠØ±', 'ÙØ¨Ø±Ø§ÙŠØ±', 'Ù…Ø§Ø±Ø³', 'Ø£Ø¨Ø±ÙŠÙ„', 'Ù…Ø§ÙŠÙˆ', 'ÙŠÙˆÙ†ÙŠÙˆ', 'ÙŠÙˆÙ„ÙŠÙˆ', 'Ø£ØºØ³Ø·Ø³', 'Ø³Ø¨ØªÙ…Ø¨Ø±', 'Ø£ÙƒØªÙˆØ¨Ø±', 'Ù†ÙˆÙÙ…Ø¨Ø±', 'Ø¯ÙŠØ³Ù…Ø¨Ø±', 'Ø¬Ø§Ù†ÙÙŠ', 'ÙÙŠÙØ±ÙŠ', 'Ù…Ø§Ø±Ø³', 'Ø£ÙØ±ÙŠÙ„', 'Ù…Ø§ÙŠ', 'Ø¬ÙˆØ§Ù†', 'Ø¬ÙˆÙŠÙ„ÙŠØ©', 'Ø£ÙˆØª', 'ÙƒØ§Ù†ÙˆÙ†', 'Ø´Ø¨Ø§Ø·', 'Ø¢Ø°Ø§Ø±', 'Ù†ÙŠØ³Ø§Ù†', 'Ø£ÙŠØ§Ø±', 'Ø­Ø²ÙŠØ±Ø§Ù†', 'ØªÙ…ÙˆØ²', 'Ø¢Ø¨', 'Ø£ÙŠÙ„ÙˆÙ„', 'ØªØ´Ø±ÙŠÙ†', 'Ø¯ÙˆÙ„Ø§Ø±', 'Ø¯ÙŠÙ†Ø§Ø±', 'Ø±ÙŠØ§Ù„', 'Ø¯Ø±Ù‡Ù…', 'Ù„ÙŠØ±Ø©', 'Ø¬Ù†ÙŠÙ‡', 'Ù‚Ø±Ø´', 'Ù…Ù„ÙŠÙ…', 'ÙÙ„Ø³', 'Ù‡Ù„Ù„Ø©', 'Ø³Ù†ØªÙŠÙ…', 'ÙŠÙˆØ±Ùˆ', 'ÙŠÙ†', 'ÙŠÙˆØ§Ù†', 'Ø´ÙŠÙƒÙ„', 'ÙˆØ§Ø­Ø¯', 'Ø§Ø«Ù†Ø§Ù†', 'Ø«Ù„Ø§Ø«Ø©', 'Ø£Ø±Ø¨Ø¹Ø©', 'Ø®Ù…Ø³Ø©', 'Ø³ØªØ©', 'Ø³Ø¨Ø¹Ø©', 'Ø«Ù…Ø§Ù†ÙŠØ©', 'ØªØ³Ø¹Ø©', 'Ø¹Ø´Ø±Ø©', 'Ø£Ø­Ø¯', 'Ø§Ø«Ù†Ø§', 'Ø§Ø«Ù†ÙŠ', 'Ø¥Ø­Ø¯Ù‰', 'Ø«Ù„Ø§Ø«', 'Ø£Ø±Ø¨Ø¹', 'Ø®Ù…Ø³', 'Ø³Øª', 'Ø³Ø¨Ø¹', 'Ø«Ù…Ø§Ù†ÙŠ', 'ØªØ³Ø¹', 'Ø¹Ø´Ø±', 'Ø«Ù…Ø§Ù†', 'Ø³Ø¨Øª', 'Ø£Ø­Ø¯', 'Ø§Ø«Ù†ÙŠÙ†', 'Ø«Ù„Ø§Ø«Ø§Ø¡', 'Ø£Ø±Ø¨Ø¹Ø§Ø¡', 'Ø®Ù…ÙŠØ³', 'Ø¬Ù…Ø¹Ø©', 'Ø£ÙˆÙ„', 'Ø«Ø§Ù†', 'Ø«Ø§Ù†ÙŠ', 'Ø«Ø§Ù„Ø«', 'Ø±Ø§Ø¨Ø¹', 'Ø®Ø§Ù…Ø³', 'Ø³Ø§Ø¯Ø³', 'Ø³Ø§Ø¨Ø¹', 'Ø«Ø§Ù…Ù†', 'ØªØ§Ø³Ø¹', 'Ø¹Ø§Ø´Ø±', 'Ø­Ø§Ø¯ÙŠ', 'Ø£', 'Ø¨', 'Øª', 'Ø«', 'Ø¬', 'Ø­', 'Ø®', 'Ø¯', 'Ø°', 'Ø±', 'Ø²', 'Ø³', 'Ø´', 'Øµ', 'Ø¶', 'Ø·', 'Ø¸', 'Ø¹', 'Øº', 'Ù', 'Ù‚', 'Ùƒ', 'Ù„', 'Ù…', 'Ù†', 'Ù‡', 'Ùˆ', 'ÙŠ', 'Ø¡', 'Ù‰', 'Ø¢', 'Ø¤', 'Ø¦', 'Ø£', 'Ø©', 'Ø£Ù„Ù', 'Ø¨Ø§Ø¡', 'ØªØ§Ø¡', 'Ø«Ø§Ø¡', 'Ø¬ÙŠÙ…', 'Ø­Ø§Ø¡', 'Ø®Ø§Ø¡', 'Ø¯Ø§Ù„', 'Ø°Ø§Ù„', 'Ø±Ø§Ø¡', 'Ø²Ø§ÙŠ', 'Ø³ÙŠÙ†', 'Ø´ÙŠÙ†', 'ØµØ§Ø¯', 'Ø¶Ø§Ø¯', 'Ø·Ø§Ø¡', 'Ø¸Ø§Ø¡', 'Ø¹ÙŠÙ†', 'ØºÙŠÙ†', 'ÙØ§Ø¡', 'Ù‚Ø§Ù', 'ÙƒØ§Ù', 'Ù„Ø§Ù…', 'Ù…ÙŠÙ…', 'Ù†ÙˆÙ†', 'Ù‡Ø§Ø¡', 'ÙˆØ§Ùˆ', 'ÙŠØ§Ø¡', 'Ù‡Ù…Ø²Ø©', 'ÙŠ', 'Ù†Ø§', 'Ùƒ', 'ÙƒÙ†', 'Ù‡', 'Ø¥ÙŠØ§Ù‡', 'Ø¥ÙŠØ§Ù‡Ø§', 'Ø¥ÙŠØ§Ù‡Ù…Ø§', 'Ø¥ÙŠØ§Ù‡Ù…', 'Ø¥ÙŠØ§Ù‡Ù†', 'Ø¥ÙŠØ§Ùƒ', 'Ø¥ÙŠØ§ÙƒÙ…Ø§', 'Ø¥ÙŠØ§ÙƒÙ…', 'Ø¥ÙŠØ§Ùƒ', 'Ø¥ÙŠØ§ÙƒÙ†', 'Ø¥ÙŠØ§ÙŠ', 'Ø¥ÙŠØ§Ù†Ø§', 'Ø£ÙˆÙ„Ø§Ù„Ùƒ', 'ØªØ§Ù†Ù', 'ØªØ§Ù†ÙÙƒ', 'ØªÙÙ‡', 'ØªÙÙŠ', 'ØªÙÙŠÙ’Ù†Ù', 'Ø«Ù…Ù‘', 'Ø«Ù…Ù‘Ø©', 'Ø°Ø§Ù†Ù', 'Ø°ÙÙ‡', 'Ø°ÙÙŠ', 'Ø°ÙÙŠÙ’Ù†Ù', 'Ù‡ÙØ¤Ù„Ø§Ø¡', 'Ù‡ÙØ§ØªØ§Ù†Ù', 'Ù‡ÙØ§ØªÙÙ‡', 'Ù‡ÙØ§ØªÙÙŠ', 'Ù‡ÙØ§ØªÙÙŠÙ’Ù†Ù', 'Ù‡ÙØ°Ø§', 'Ù‡ÙØ°Ø§Ù†Ù', 'Ù‡ÙØ°ÙÙ‡', 'Ù‡ÙØ°ÙÙŠ', 'Ù‡ÙØ°ÙÙŠÙ’Ù†Ù', 'Ø§Ù„Ø£Ù„Ù‰', 'Ø§Ù„Ø£Ù„Ø§Ø¡', 'Ø£Ù„', 'Ø£Ù†Ù‘Ù‰', 'Ø£ÙŠÙ‘', 'Ù‘Ø£ÙŠÙ‘Ø§Ù†', 'Ø£Ù†Ù‘Ù‰', 'Ø£ÙŠÙ‘', 'Ù‘Ø£ÙŠÙ‘Ø§Ù†', 'Ø°ÙŠØª', 'ÙƒØ£ÙŠÙ‘', 'ÙƒØ£ÙŠÙ‘Ù†', 'Ø¨Ø¶Ø¹', 'ÙÙ„Ø§Ù†', 'ÙˆØ§', 'Ø¢Ù…ÙŠÙ†Ù', 'Ø¢Ù‡Ù', 'Ø¢Ù‡Ù', 'Ø¢Ù‡Ø§Ù‹', 'Ø£ÙÙÙÙ‘', 'Ø£ÙÙÙÙ‘', 'Ø£ÙÙÙ‘', 'Ø£Ù…Ø§Ù…Ùƒ', 'Ø£Ù…Ø§Ù…ÙƒÙ', 'Ø£ÙˆÙ‘Ù‡Ù’', 'Ø¥Ù„ÙÙŠÙ’ÙƒÙ', 'Ø¥Ù„ÙÙŠÙ’ÙƒÙ', 'Ø¥Ù„ÙŠÙƒÙ', 'Ø¥Ù„ÙŠÙƒÙ†Ù‘', 'Ø¥ÙŠÙ‡Ù', 'Ø¨Ø®Ù', 'Ø¨Ø³Ù‘', 'Ø¨ÙØ³Ù’', 'Ø¨Ø·Ø¢Ù†', 'Ø¨ÙÙ„Ù’Ù‡Ù', 'Ø­Ø§ÙŠ', 'Ø­ÙØ°Ø§Ø±Ù', 'Ø­ÙŠÙÙ‘', 'Ø­ÙŠÙÙ‘', 'Ø¯ÙˆÙ†Ùƒ', 'Ø±ÙˆÙŠØ¯Ùƒ', 'Ø³Ø±Ø¹Ø§Ù†', 'Ø´ØªØ§Ù†Ù', 'Ø´ÙØªÙÙ‘Ø§Ù†Ù', 'ØµÙ‡Ù’', 'ØµÙ‡Ù', 'Ø·Ø§Ù‚', 'Ø·ÙÙ‚', 'Ø¹ÙØ¯ÙØ³Ù’', 'ÙƒÙØ®', 'Ù…ÙƒØ§Ù†ÙÙƒ', 'Ù…ÙƒØ§Ù†ÙÙƒ', 'Ù…ÙƒØ§Ù†ÙÙƒ', 'Ù…ÙƒØ§Ù†ÙƒÙ…', 'Ù…ÙƒØ§Ù†ÙƒÙ…Ø§', 'Ù…ÙƒØ§Ù†ÙƒÙ†Ù‘', 'Ù†ÙØ®Ù’', 'Ù‡Ø§ÙƒÙ', 'Ù‡ÙØ¬Ù’', 'Ù‡Ù„Ù…', 'Ù‡ÙŠÙ‘Ø§', 'Ù‡ÙÙŠÙ’Ù‡Ø§Øª', 'ÙˆØ§', 'ÙˆØ§Ù‡Ø§Ù‹', 'ÙˆØ±Ø§Ø¡ÙÙƒ', 'ÙˆÙØ´Ù’ÙƒÙØ§Ù†Ù', 'ÙˆÙÙŠÙ’', 'ÙŠÙØ¹Ù„Ø§Ù†', 'ØªÙØ¹Ù„Ø§Ù†', 'ÙŠÙØ¹Ù„ÙˆÙ†', 'ØªÙØ¹Ù„ÙˆÙ†', 'ØªÙØ¹Ù„ÙŠÙ†', 'Ø§ØªØ®Ø°', 'Ø£Ù„ÙÙ‰', 'ØªØ®Ø°', 'ØªØ±Ùƒ', 'ØªØ¹Ù„ÙÙ‘Ù…', 'Ø¬Ø¹Ù„', 'Ø­Ø¬Ø§', 'Ø­Ø¨ÙŠØ¨', 'Ø®Ø§Ù„', 'Ø­Ø³Ø¨', 'Ø®Ø§Ù„', 'Ø¯Ø±Ù‰', 'Ø±Ø£Ù‰', 'Ø²Ø¹Ù…', 'ØµØ¨Ø±', 'Ø¸Ù†ÙÙ‘', 'Ø¹Ø¯ÙÙ‘', 'Ø¹Ù„Ù…', 'ØºØ§Ø¯Ø±', 'Ø°Ù‡Ø¨', 'ÙˆØ¬Ø¯', 'ÙˆØ±Ø¯', 'ÙˆÙ‡Ø¨', 'Ø£Ø³ÙƒÙ†', 'Ø£Ø·Ø¹Ù…', 'Ø£Ø¹Ø·Ù‰', 'Ø±Ø²Ù‚', 'Ø²ÙˆØ¯', 'Ø³Ù‚Ù‰', 'ÙƒØ³Ø§', 'Ø£Ø®Ø¨Ø±', 'Ø£Ø±Ù‰', 'Ø£Ø¹Ù„Ù…', 'Ø£Ù†Ø¨Ø£', 'Ø­Ø¯ÙØ«', 'Ø®Ø¨ÙÙ‘Ø±', 'Ù†Ø¨ÙÙ‘Ø§', 'Ø£ÙØ¹Ù„ Ø¨Ù‡', 'Ù…Ø§ Ø£ÙØ¹Ù„Ù‡', 'Ø¨Ø¦Ø³', 'Ø³Ø§Ø¡', 'Ø·Ø§Ù„Ù…Ø§', 'Ù‚Ù„Ù…Ø§', 'Ù„Ø§Øª', 'Ù„ÙƒÙ†ÙÙ‘', 'Ø¡Ù', 'Ø£Ø¬Ù„', 'Ø¥Ø°Ø§Ù‹', 'Ø£Ù…Ù‘Ø§', 'Ø¥Ù…Ù‘Ø§', 'Ø¥Ù†ÙÙ‘', 'Ø£Ù†Ù‹Ù‘', 'Ø£Ù‰', 'Ø¥Ù‰', 'Ø£ÙŠØ§', 'Ø¨', 'Ø«Ù…ÙÙ‘', 'Ø¬Ù„Ù„', 'Ø¬ÙŠØ±', 'Ø±ÙØ¨ÙÙ‘', 'Ø³', 'Ø¹Ù„Ù‹Ù‘', 'Ù', 'ÙƒØ£Ù†Ù‘', 'ÙƒÙ„ÙÙ‘Ø§', 'ÙƒÙ‰', 'Ù„', 'Ù„Ø§Øª', 'Ù„Ø¹Ù„ÙÙ‘', 'Ù„ÙƒÙ†ÙÙ‘', 'Ù„ÙƒÙ†ÙÙ‘', 'Ù…', 'Ù†ÙÙ‘', 'Ù‡Ù„Ù‘Ø§', 'ÙˆØ§', 'Ø£Ù„', 'Ø¥Ù„Ù‘Ø§', 'Øª', 'Ùƒ', 'Ù„Ù…Ù‘Ø§', 'Ù†', 'Ù‡', 'Ùˆ', 'Ø§', 'ÙŠ', 'ØªØ¬Ø§Ù‡', 'ØªÙ„Ù‚Ø§Ø¡', 'Ø¬Ù…ÙŠØ¹', 'Ø­Ø³Ø¨', 'Ø³Ø¨Ø­Ø§Ù†', 'Ø´Ø¨Ù‡', 'Ù„Ø¹Ù…Ø±', 'Ù…Ø«Ù„', 'Ù…Ø¹Ø§Ø°', 'Ø£Ø¨Ùˆ', 'Ø£Ø®Ùˆ', 'Ø­Ù…Ùˆ', 'ÙÙˆ', 'Ù…Ø¦Ø©', 'Ù…Ø¦ØªØ§Ù†', 'Ø«Ù„Ø§Ø«Ù…Ø¦Ø©', 'Ø£Ø±Ø¨Ø¹Ù…Ø¦Ø©', 'Ø®Ù…Ø³Ù…Ø¦Ø©', 'Ø³ØªÙ…Ø¦Ø©', 'Ø³Ø¨Ø¹Ù…Ø¦Ø©', 'Ø«Ù…Ù†Ù…Ø¦Ø©', 'ØªØ³Ø¹Ù…Ø¦Ø©', 'Ù…Ø§Ø¦Ø©', 'Ø«Ù„Ø§Ø«Ù…Ø§Ø¦Ø©', 'Ø£Ø±Ø¨Ø¹Ù…Ø§Ø¦Ø©', 'Ø®Ù…Ø³Ù…Ø§Ø¦Ø©', 'Ø³ØªÙ…Ø§Ø¦Ø©', 'Ø³Ø¨Ø¹Ù…Ø§Ø¦Ø©', 'Ø«Ù…Ø§Ù†Ù…Ø¦Ø©', 'ØªØ³Ø¹Ù…Ø§Ø¦Ø©', 'Ø¹Ø´Ø±ÙˆÙ†', 'Ø«Ù„Ø§Ø«ÙˆÙ†', 'Ø§Ø±Ø¨Ø¹ÙˆÙ†', 'Ø®Ù…Ø³ÙˆÙ†', 'Ø³ØªÙˆÙ†', 'Ø³Ø¨Ø¹ÙˆÙ†', 'Ø«Ù…Ø§Ù†ÙˆÙ†', 'ØªØ³Ø¹ÙˆÙ†', 'Ø¹Ø´Ø±ÙŠÙ†', 'Ø«Ù„Ø§Ø«ÙŠÙ†', 'Ø§Ø±Ø¨Ø¹ÙŠÙ†', 'Ø®Ù…Ø³ÙŠÙ†', 'Ø³ØªÙŠÙ†', 'Ø³Ø¨Ø¹ÙŠÙ†', 'Ø«Ù…Ø§Ù†ÙŠÙ†', 'ØªØ³Ø¹ÙŠÙ†', 'Ø¨Ø¶Ø¹', 'Ù†ÙŠÙ', 'Ø£Ø¬Ù…Ø¹', 'Ø¬Ù…ÙŠØ¹', 'Ø¹Ø§Ù…Ø©', 'Ø¹ÙŠÙ†', 'Ù†ÙØ³', 'Ù„Ø§ Ø³ÙŠÙ…Ø§', 'Ø£ØµÙ„Ø§', 'Ø£Ù‡Ù„Ø§', 'Ø£ÙŠØ¶Ø§', 'Ø¨Ø¤Ø³Ø§', 'Ø¨Ø¹Ø¯Ø§', 'Ø¨ØºØªØ©', 'ØªØ¹Ø³Ø§', 'Ø­Ù‚Ø§', 'Ø­Ù…Ø¯Ø§', 'Ø®Ù„Ø§ÙØ§', 'Ø®Ø§ØµØ©', 'Ø¯ÙˆØ§Ù„ÙŠÙƒ', 'Ø³Ø­Ù‚Ø§', 'Ø³Ø±Ø§', 'Ø³Ù…Ø¹Ø§', 'ØµØ¨Ø±Ø§', 'ØµØ¯Ù‚Ø§', 'ØµØ±Ø§Ø­Ø©', 'Ø·Ø±Ø§', 'Ø¹Ø¬Ø¨Ø§', 'Ø¹ÙŠØ§Ù†Ø§', 'ØºØ§Ù„Ø¨Ø§', 'ÙØ±Ø§Ø¯Ù‰', 'ÙØ¶Ù„Ø§', 'Ù‚Ø§Ø·Ø¨Ø©', 'ÙƒØ«ÙŠØ±Ø§', 'Ù„Ø¨ÙŠÙƒ', 'Ù…Ø¹Ø§Ø°', 'Ø£Ø¨Ø¯Ø§', 'Ø¥Ø²Ø§Ø¡', 'Ø£ØµÙ„Ø§', 'Ø§Ù„Ø¢Ù†', 'Ø£Ù…Ø¯', 'Ø£Ù…Ø³', 'Ø¢Ù†ÙØ§', 'Ø¢Ù†Ø§Ø¡', 'Ø£Ù†Ù‘Ù‰', 'Ø£ÙˆÙ„', 'Ø£ÙŠÙ‘Ø§Ù†', 'ØªØ§Ø±Ø©', 'Ø«Ù…Ù‘', 'Ø«Ù…Ù‘Ø©', 'Ø­Ù‚Ø§', 'ØµØ¨Ø§Ø­', 'Ù…Ø³Ø§Ø¡', 'Ø¶Ø­ÙˆØ©', 'Ø¹ÙˆØ¶', 'ØºØ¯Ø§', 'ØºØ¯Ø§Ø©', 'Ù‚Ø·Ù‘', 'ÙƒÙ„Ù‘Ù…Ø§', 'Ù„Ø¯Ù†', 'Ù„Ù…Ù‘Ø§', 'Ù…Ø±Ù‘Ø©', 'Ù‚Ø¨Ù„', 'Ø®Ù„Ù', 'Ø£Ù…Ø§Ù…', 'ÙÙˆÙ‚', 'ØªØ­Øª', 'ÙŠÙ…ÙŠÙ†', 'Ø´Ù…Ø§Ù„', 'Ø§Ø±ØªØ¯Ù‘', 'Ø§Ø³ØªØ­Ø§Ù„', 'Ø£ØµØ¨Ø­', 'Ø£Ø¶Ø­Ù‰', 'Ø¢Ø¶', 'Ø£Ù…Ø³Ù‰', 'Ø§Ù†Ù‚Ù„Ø¨', 'Ø¨Ø§Øª', 'ØªØ¨Ø¯Ù‘Ù„', 'ØªØ­ÙˆÙ‘Ù„', 'Ø­Ø§Ø±', 'Ø±Ø¬Ø¹', 'Ø±Ø§Ø­', 'ØµØ§Ø±', 'Ø¸Ù„Ù‘', 'Ø¹Ø§Ø¯', 'ØºØ¯Ø§', 'ÙƒØ§Ù†', 'Ù…Ø§ Ø§Ù†ÙÙƒ', 'Ù…Ø§ Ø¨Ø±Ø­', 'Ù…Ø§Ø¯Ø§Ù…', 'Ù…Ø§Ø²Ø§Ù„', 'Ù…Ø§ÙØªØ¦', 'Ø§Ø¨ØªØ¯Ø£', 'Ø£Ø®Ø°', 'Ø§Ø®Ù„ÙˆÙ„Ù‚', 'Ø£Ù‚Ø¨Ù„', 'Ø§Ù†Ø¨Ø±Ù‰', 'Ø£Ù†Ø´Ø£', 'Ø£ÙˆØ´Ùƒ', 'Ø¬Ø¹Ù„', 'Ø­Ø±Ù‰', 'Ø´Ø±Ø¹', 'Ø·ÙÙ‚', 'Ø¹Ù„Ù‚', 'Ù‚Ø§Ù…', 'ÙƒØ±Ø¨', 'ÙƒØ§Ø¯', 'Ù‡Ø¨Ù‘']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# Arabic stopwords\n",
    "arabic_stopwords = stopwords.words('arabic')\n",
    "\n",
    "# Print some of the stopwords\n",
    "print(arabic_stopwords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: aiogoogletrans in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (3.3.3)\n",
      "Requirement already satisfied: httpx==0.23.0 in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx[http2]==0.23.0->aiogoogletrans) (0.23.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx==0.23.0->httpx[http2]==0.23.0->aiogoogletrans) (2024.2.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx==0.23.0->httpx[http2]==0.23.0->aiogoogletrans) (1.3.0)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rfc3986[idna2008]<2,>=1.3->httpx==0.23.0->httpx[http2]==0.23.0->aiogoogletrans) (1.5.0)\n",
      "Requirement already satisfied: httpcore<0.16.0,>=0.15.0 in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx==0.23.0->httpx[http2]==0.23.0->aiogoogletrans) (0.15.0)\n",
      "Requirement already satisfied: h2<5,>=3 in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx[http2]==0.23.0->aiogoogletrans) (4.1.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from h2<5,>=3->httpx[http2]==0.23.0->aiogoogletrans) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from h2<5,>=3->httpx[http2]==0.23.0->aiogoogletrans) (4.0.0)\n",
      "Requirement already satisfied: h11<0.13,>=0.11 in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpcore<0.16.0,>=0.15.0->httpx==0.23.0->httpx[http2]==0.23.0->aiogoogletrans) (0.12.0)\n",
      "Requirement already satisfied: anyio==3.* in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpcore<0.16.0,>=0.15.0->httpx==0.23.0->httpx[http2]==0.23.0->aiogoogletrans) (3.7.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from anyio==3.*->httpcore<0.16.0,>=0.15.0->httpx==0.23.0->httpx[http2]==0.23.0->aiogoogletrans) (3.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install aiogoogletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: unidecode in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.3.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: emoji in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.10.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- get hashtags\n",
    "2- get mentions\n",
    "3- get urls\n",
    "4- remove hashtags\n",
    "5- treat mentions \"remove only @\"\n",
    "6- replace emojis \"only arabic case is treated\"\n",
    "7- text = re.sub('http\\S+\\s*', ' ', text)  # remove URLs\n",
    "7- remove punctuations\n",
    "10- remove stop words \n",
    "8- remove non used characters \n",
    "9- remove repeated characters\n",
    "11- for arabic remove tashkeel\n",
    "12- Strip tatweel from a text.\n",
    "13- stem words\n",
    "14- normalizeArabic\n",
    "15- tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_hashtag_to_words(tag):\n",
    "    tag = tag.replace('#','')\n",
    "    tags = tag.split('_')\n",
    "    if len(tags) > 1 :\n",
    "        \n",
    "        return tags\n",
    "    pattern = re.compile(r\"[A-Z][a-z]+|\\d+|[A-Z]+(?![a-z])\")\n",
    "    return pattern.findall(tag)\n",
    "\n",
    "def extract_hashtag(text):\n",
    "    \n",
    "    hash_list = ([re.sub(r\"(\\W+)$\", \"\", i) for i in text.split() if i.startswith(\"#\")])\n",
    "    word_list = []\n",
    "    for word in hash_list :\n",
    "        word_list.extend(split_hashtag_to_words(word))\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ù…Ø³ØªØ®Ø¯Ù…1', 'Ù…Ø³ØªØ®Ø¯Ù…2']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_mentions_arabic(text):\n",
    "    # Find all mentions starting with '@' and extract the username\n",
    "    mentions = re.findall(r'@(\\w+)', text)\n",
    "    return mentions\n",
    "\n",
    "# Example usage:\n",
    "text_arabic = \"Ù…Ø±Ø­Ø¨Ø§ @Ù…Ø³ØªØ®Ø¯Ù…1 Ùˆ @Ù…Ø³ØªØ®Ø¯Ù…2ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒÙ…ØŸ\"\n",
    "mentions_arabic = extract_mentions_arabic(text_arabic)\n",
    "print(mentions_arabic)  # Output: ['Ù…Ø³ØªØ®Ø¯Ù…1', 'Ù…Ø³ØªØ®Ø¯Ù…2']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- get urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://example.com', 'https://example.org']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_urls(text):\n",
    "    # Regular expression pattern to match URLs\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    # Find all URLs in the text\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    return urls\n",
    "\n",
    "# Example usage:\n",
    "arabic_text_with_urls = \"Ù‚Ù… Ø¨Ø²ÙŠØ§Ø±Ø© Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆÙ‚Ø¹: https://example.comØŒ ÙˆØ£ÙŠØ¶Ù‹Ø§ https://example.org\"\n",
    "urls = extract_urls(arabic_text_with_urls)\n",
    "print(urls)  # Output: ['https://example.com', 'https://example.org']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- treat hashtags : remove hashtags #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hashtag(text):\n",
    "    words = text.split()\n",
    "    text = list()\n",
    "    for word in words:\n",
    "        if is_hashtag(word):\n",
    "            text.extend(extract_hashtag(word))\n",
    "        else:\n",
    "            text.append(word)\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "def is_hashtag(word):\n",
    "    if word.startswith(\"#\"):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def remove_hashtag_char(text):\n",
    "    # Regular expression pattern to match mentions\n",
    "    mention_pattern = r'#+'\n",
    "    # Replace mentions with an empty string\n",
    "    cleaned_text = re.sub(mention_pattern, '', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ù…Ø±Ø­Ø¨Ø§ @Ù…Ø³ØªØ®Ø¯Ù…1ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ Ù…Ù†Ø´Ù† dfhg  Ù„Ù€ @Ù…Ø³ØªØ®Ø¯Ù…2 ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©.\n"
     ]
    }
   ],
   "source": [
    "# Example usage with Arabic text:\n",
    "text_with_mentions_arabic = \"Ù…Ø±Ø­Ø¨Ø§ @Ù…Ø³ØªØ®Ø¯Ù…1ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ Ù…Ù†Ø´Ù† #dfhg#  Ù„Ù€ @Ù…Ø³ØªØ®Ø¯Ù…2 ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©.\"\n",
    "cleaned_text_arabic = remove_hashtag_char(text_with_mentions_arabic)\n",
    "print(cleaned_text_arabic)  # Output: \"Ù…Ø±Ø­Ø¨Ø§ ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ Ù…Ù†Ø´Ù† Ù„Ù€ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5- remove mentions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ù…Ø±Ø­Ø¨Ø§ Ù…Ø³ØªØ®Ø¯Ù…1ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ Ù…Ù†Ø´Ù† Ù„Ù€ Ù…Ø³ØªØ®Ø¯Ù…2 ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_mentions(text):\n",
    "    # Regular expression pattern to match mentions\n",
    "    mention_pattern = r'@+'\n",
    "    # Replace mentions with an empty string\n",
    "    cleaned_text = re.sub(mention_pattern, '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "# Example usage with Arabic text:\n",
    "text_with_mentions_arabic = \"Ù…Ø±Ø­Ø¨Ø§ @Ù…Ø³ØªØ®Ø¯Ù…1ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ Ù…Ù†Ø´Ù† Ù„Ù€ @Ù…Ø³ØªØ®Ø¯Ù…2 ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©.\"\n",
    "cleaned_text_arabic = remove_mentions(text_with_mentions_arabic)\n",
    "print(cleaned_text_arabic)  # Output: \"Ù…Ø±Ø­Ø¨Ø§ ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ Ù…Ù†Ø´Ù† Ù„Ù€ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6- replace emojis \"only arabic case is treated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:37: SyntaxWarning: invalid escape sequence '\\)'\n",
      "<>:40: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:40: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:43: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<>:43: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<>:47: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:47: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:50: SyntaxWarning: invalid escape sequence '\\)'\n",
      "<>:50: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:53: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<>:53: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<>:37: SyntaxWarning: invalid escape sequence '\\)'\n",
      "<>:40: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:40: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:43: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<>:43: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<>:47: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:47: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:50: SyntaxWarning: invalid escape sequence '\\)'\n",
      "<>:50: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:53: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<>:53: SyntaxWarning: invalid escape sequence '\\/'\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5796\\3581009673.py:37: SyntaxWarning: invalid escape sequence '\\)'\n",
      "  for s in [\"\\)\", \"d\", \"]\", \"}\",\"p\"]:\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5796\\3581009673.py:40: SyntaxWarning: invalid escape sequence '\\('\n",
      "  for s in [\"\\(\", \"\\[\", \"{\"]:\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5796\\3581009673.py:40: SyntaxWarning: invalid escape sequence '\\['\n",
      "  for s in [\"\\(\", \"\\[\", \"{\"]:\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5796\\3581009673.py:43: SyntaxWarning: invalid escape sequence '\\|'\n",
      "  for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5796\\3581009673.py:43: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5796\\3581009673.py:47: SyntaxWarning: invalid escape sequence '\\('\n",
      "  for s in [\"\\(\", \"\\[\", \"{\"]:\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5796\\3581009673.py:47: SyntaxWarning: invalid escape sequence '\\['\n",
      "  for s in [\"\\(\", \"\\[\", \"{\"]:\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5796\\3581009673.py:50: SyntaxWarning: invalid escape sequence '\\)'\n",
      "  for s in [\"\\)\", \"\\]\", \"}\"]:\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5796\\3581009673.py:50: SyntaxWarning: invalid escape sequence '\\]'\n",
      "  for s in [\"\\)\", \"\\]\", \"}\"]:\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5796\\3581009673.py:53: SyntaxWarning: invalid escape sequence '\\|'\n",
      "  for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5796\\3581009673.py:53: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n"
     ]
    }
   ],
   "source": [
    "with open('emojis.csv','r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    emojis_ar = {}\n",
    "    for line in lines:\n",
    "        line = line.strip('\\n').split(';')\n",
    "        emojis_ar.update({line[0].strip():line[1].strip()})\n",
    "\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "import unicodedata\n",
    "from unidecode import unidecode\n",
    "\n",
    "def emoji_native_translation(text):\n",
    "    text = text.lower()\n",
    "    loves = [\"<3\", \"â™¥\",'â¤']\n",
    "    smilefaces = []\n",
    "    sadfaces = []\n",
    "    neutralfaces = []\n",
    "\n",
    "    eyes = [\"8\",\":\",\"=\",\";\"]\n",
    "    nose = [\"'\",\"`\",\"-\",r\"\\\\\"]\n",
    "    for e in eyes:\n",
    "        for n in nose:\n",
    "            for s in [\"\\)\", \"d\", \"]\", \"}\",\"p\"]:\n",
    "                smilefaces.append(e+n+s)\n",
    "                smilefaces.append(e+s)\n",
    "            for s in [\"\\(\", \"\\[\", \"{\"]:\n",
    "                sadfaces.append(e+n+s)\n",
    "                sadfaces.append(e+s)\n",
    "            for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n",
    "                neutralfaces.append(e+n+s)\n",
    "                neutralfaces.append(e+s)\n",
    "            #reversed\n",
    "            for s in [\"\\(\", \"\\[\", \"{\"]:\n",
    "                smilefaces.append(s+n+e)\n",
    "                smilefaces.append(s+e)\n",
    "            for s in [\"\\)\", \"\\]\", \"}\"]:\n",
    "                sadfaces.append(s+n+e)\n",
    "                sadfaces.append(s+e)\n",
    "            for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n",
    "                neutralfaces.append(s+n+e)\n",
    "                neutralfaces.append(s+e)\n",
    "\n",
    "    smilefaces = list(set(smilefaces))\n",
    "    sadfaces = list(set(sadfaces))\n",
    "    neutralfaces = list(set(neutralfaces))\n",
    "    t = []\n",
    "    for w in text.split():\n",
    "        if w in loves:\n",
    "            t.append(\"Ø­Ø¨\")\n",
    "        elif w in smilefaces:\n",
    "            t.append(\"Ù…Ø¶Ø­Ùƒ\")\n",
    "        elif w in neutralfaces:\n",
    "            t.append(\"Ø¹Ø§Ø¯ÙŠ\")\n",
    "        elif w in sadfaces:\n",
    "            t.append(\"Ù…Ø­Ø²Ù†\")\n",
    "        else:\n",
    "            t.append(w)\n",
    "    newText = \" \".join(t)\n",
    "    return newText\n",
    "\n",
    "import emoji\n",
    "def is_emoji(word):\n",
    "    if word in emojis_ar:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def add_space(text):\n",
    "    return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()\n",
    "\n",
    "from aiogoogletrans import Translator\n",
    "translator = Translator()\n",
    "import asyncio\n",
    "loop = asyncio.get_event_loop()\n",
    "def translate_emojis(words):\n",
    "    word_list = list()\n",
    "    words_to_translate = list()\n",
    "    for word in words :\n",
    "        t = emojis_ar.get(word.get('emoji'),None)\n",
    "        if t is None:\n",
    "            word.update({'translation':'Ø¹Ø§Ø¯ÙŠ','translated':True})\n",
    "            #words_to_translate.append('normal')\n",
    "        else:\n",
    "            word.update({'translated':False,'translation':t})\n",
    "            words_to_translate.append(t.replace(':','').replace('_',' '))\n",
    "        word_list.append(word)\n",
    "    return word_list\n",
    "\n",
    "def emoji_unicode_translation(text):\n",
    "    text = add_space(text)\n",
    "    words = text.split()\n",
    "    text_list = list()\n",
    "    emojis_list = list()\n",
    "    c = 0\n",
    "    for word in words:\n",
    "        if is_emoji(word):\n",
    "            emojis_list.append({'emoji':word,'emplacement':c})\n",
    "        else:\n",
    "            text_list.append(word)\n",
    "        c+=1\n",
    "    emojis_translated = translate_emojis(emojis_list)\n",
    "    for em in emojis_translated:\n",
    "        text_list.insert(em.get('emplacement'),em.get('translation'))\n",
    "    text = \" \".join(text_list)\n",
    "    return text\n",
    "    \n",
    "def clean_emoji(text):\n",
    "    text = emoji_native_translation(text)\n",
    "    text = emoji_unicode_translation(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø£Ø­Ø¨Ùƒ Ù‚Ù„Ø¨ Ø£Ø­Ù…Ø± Ø§Ø­Ù…Ø± Ø®Ø¯ÙˆØ¯\n",
      "Ø£Ù†Ø§ Ø­Ø²ÙŠÙ† Ø¬Ø¯Ù‹Ø§ ÙŠØ¨ÙƒÙŠ\n",
      "Ø£Ù†Ø§ ÙÙŠ Ø­Ø§Ù„Ø© Ù…Ø²Ø§Ø¬ÙŠØ© Ø¬ÙŠØ¯Ø© Ø§Ø¨ØªØ³Ø§Ù…Ø©\n"
     ]
    }
   ],
   "source": [
    "text_with_emojis = \"Ø£Ø­Ø¨Ùƒ â¤ï¸ğŸ˜Š\"\n",
    "cleaned_text = clean_emoji(text_with_emojis)\n",
    "print(cleaned_text)\n",
    "# Output: \"Ø£Ø­Ø¨Ùƒ Ø­Ø¨ Ù…Ø¶Ø­Ùƒ\"\n",
    "\n",
    "text_with_emojis = \"Ø£Ù†Ø§ Ø­Ø²ÙŠÙ† Ø¬Ø¯Ù‹Ø§ ğŸ˜¢\"\n",
    "cleaned_text = clean_emoji(text_with_emojis)\n",
    "print(cleaned_text)\n",
    "# Output: \"Ø£Ù†Ø§ Ø­Ø²ÙŠÙ† Ø¬Ø¯Ù‹Ø§ Ù…Ø­Ø²Ù†\"\n",
    "\n",
    "text_with_emojis = \"Ø£Ù†Ø§ ÙÙŠ Ø­Ø§Ù„Ø© Ù…Ø²Ø§Ø¬ÙŠØ© Ø¬ÙŠØ¯Ø© ğŸ˜„\"\n",
    "cleaned_text = clean_emoji(text_with_emojis)\n",
    "print(cleaned_text)\n",
    "# Output: \"Ø£Ù†Ø§ ÙÙŠ Ø­Ø§Ù„Ø© Ù…Ø²Ø§Ø¬ÙŠØ© Ø¬ÙŠØ¯Ø© Ù…Ø¶Ø­Ùƒ\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7- text = re.sub('http\\S+\\s*', ' ', text)  # remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_urls(text):\n",
    "    # Remove URLs from the text\n",
    "    cleaned_text = re.sub(r'http\\S+\\s*', '', text)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Arabic text: ØªØ­Ù‚Ù‚ Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ø±Ø§Ø¨Ø·: https://example.comØŒ Ø¥Ù†Ù‡ Ø±Ø§Ø¦Ø¹!\n",
      "Arabic text with URLs removed: ØªØ­Ù‚Ù‚ Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ø±Ø§Ø¨Ø·: Ø¥Ù†Ù‡ Ø±Ø§Ø¦Ø¹!\n"
     ]
    }
   ],
   "source": [
    "# Arabic example\n",
    "arabic_text = \"ØªØ­Ù‚Ù‚ Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ø±Ø§Ø¨Ø·: https://example.comØŒ Ø¥Ù†Ù‡ Ø±Ø§Ø¦Ø¹!\"\n",
    "cleaned_arabic_text = remove_urls(arabic_text)\n",
    "print(\"Original Arabic text:\", arabic_text)\n",
    "print(\"Arabic text with URLs removed:\", cleaned_arabic_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7- remove punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\]'\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5796\\1715161554.py:7: SyntaxWarning: invalid escape sequence '\\]'\n",
      "  text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,ØŒ-./:;<=>ØŸ?@[\\]^_`{|}~\"\"\"), ' ', text)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "all_punctuation = string.punctuation\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    ## Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,ØŒ-./:;<=>ØŸ?@[\\]^_`{|}~\"\"\"), ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Arabic text: ØªØ­Ù‚Ù‚ Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ø±Ø§Ø¨Ø·: https://example.comØŒ Ø¥Ù†Ù‡ Ø±Ø§Ø¦Ø¹!\n",
      "Arabic text with punctuations removed: ØªØ­Ù‚Ù‚ Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ø±Ø§Ø¨Ø·  https   example com  Ø¥Ù†Ù‡ Ø±Ø§Ø¦Ø¹ \n"
     ]
    }
   ],
   "source": [
    "# Arabic example\n",
    "arabic_text = \"ØªØ­Ù‚Ù‚ Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ø±Ø§Ø¨Ø·: https://example.comØŒ Ø¥Ù†Ù‡ Ø±Ø§Ø¦Ø¹!\"\n",
    "cleaned_arabic_text = remove_punctuations(arabic_text)\n",
    "print(\"Original Arabic text:\", arabic_text)\n",
    "print(\"Arabic text with punctuations removed:\", cleaned_arabic_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10- remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ø¥Ø°', 'Ø¥Ø°Ø§', 'Ø¥Ø°Ù…Ø§', 'Ø¥Ø°Ù†', 'Ø£Ù', 'Ø£Ù‚Ù„', 'Ø£ÙƒØ«Ø±', 'Ø£Ù„Ø§', 'Ø¥Ù„Ø§', 'Ø§Ù„ØªÙŠ', 'Ø§Ù„Ø°ÙŠ', 'Ø§Ù„Ø°ÙŠÙ†', 'Ø§Ù„Ù„Ø§ØªÙŠ', 'Ø§Ù„Ù„Ø§Ø¦ÙŠ', 'Ø§Ù„Ù„ØªØ§Ù†', 'Ø§Ù„Ù„ØªÙŠØ§', 'Ø§Ù„Ù„ØªÙŠÙ†', 'Ø§Ù„Ù„Ø°Ø§Ù†', 'Ø§Ù„Ù„Ø°ÙŠÙ†', 'Ø§Ù„Ù„ÙˆØ§ØªÙŠ', 'Ø¥Ù„Ù‰', 'Ø¥Ù„ÙŠÙƒ', 'Ø¥Ù„ÙŠÙƒÙ…', 'Ø¥Ù„ÙŠÙƒÙ…Ø§', 'Ø¥Ù„ÙŠÙƒÙ†', 'Ø£Ù…', 'Ø£Ù…Ø§', 'Ø£Ù…Ø§', 'Ø¥Ù…Ø§', 'Ø£Ù†', 'Ø¥Ù†', 'Ø¥Ù†Ø§', 'Ø£Ù†Ø§', 'Ø£Ù†Øª', 'Ø£Ù†ØªÙ…', 'Ø£Ù†ØªÙ…Ø§', 'Ø£Ù†ØªÙ†', 'Ø¥Ù†Ù…Ø§', 'Ø¥Ù†Ù‡', 'Ø£Ù†Ù‰', 'Ø£Ù†Ù‰', 'Ø¢Ù‡', 'Ø¢Ù‡Ø§', 'Ø£Ùˆ', 'Ø£ÙˆÙ„Ø§Ø¡', 'Ø£ÙˆÙ„Ø¦Ùƒ', 'Ø£ÙˆÙ‡', 'Ø¢ÙŠ', 'Ø£ÙŠ', 'Ø£ÙŠÙ‡Ø§', 'Ø¥ÙŠ', 'Ø£ÙŠÙ†', 'Ø£ÙŠÙ†', 'Ø£ÙŠÙ†Ù…Ø§', 'Ø¥ÙŠÙ‡', 'Ø¨Ø®', 'Ø¨Ø³', 'Ø¨Ø¹Ø¯', 'Ø¨Ø¹Ø¶', 'Ø¨Ùƒ', 'Ø¨ÙƒÙ…', 'Ø¨ÙƒÙ…', 'Ø¨ÙƒÙ…Ø§', 'Ø¨ÙƒÙ†', 'Ø¨Ù„', 'Ø¨Ù„Ù‰', 'Ø¨Ù…Ø§', 'Ø¨Ù…Ø§Ø°Ø§', 'Ø¨Ù…Ù†', 'Ø¨Ù†Ø§', 'Ø¨Ù‡', 'Ø¨Ù‡Ø§', 'Ø¨Ù‡Ù…', 'Ø¨Ù‡Ù…Ø§', 'Ø¨Ù‡Ù†', 'Ø¨ÙŠ', 'Ø¨ÙŠÙ†', 'Ø¨ÙŠØ¯', 'ØªÙ„Ùƒ', 'ØªÙ„ÙƒÙ…', 'ØªÙ„ÙƒÙ…Ø§', 'ØªÙ‡', 'ØªÙŠ', 'ØªÙŠÙ†', 'ØªÙŠÙ†Ùƒ', 'Ø«Ù…', 'Ø«Ù…Ø©', 'Ø­Ø§Ø´Ø§', 'Ø­Ø¨Ø°Ø§', 'Ø­ØªÙ‰', 'Ø­ÙŠØ«', 'Ø­ÙŠØ«Ù…Ø§', 'Ø­ÙŠÙ†', 'Ø®Ù„Ø§', 'Ø¯ÙˆÙ†', 'Ø°Ø§', 'Ø°Ø§Øª', 'Ø°Ø§Ùƒ', 'Ø°Ø§Ù†', 'Ø°Ø§Ù†Ùƒ', 'Ø°Ù„Ùƒ', 'Ø°Ù„ÙƒÙ…', 'Ø°Ù„ÙƒÙ…Ø§', 'Ø°Ù„ÙƒÙ†', 'Ø°Ù‡', 'Ø°Ùˆ', 'Ø°ÙˆØ§', 'Ø°ÙˆØ§ØªØ§', 'Ø°ÙˆØ§ØªÙŠ', 'Ø°ÙŠ', 'Ø°ÙŠÙ†', 'Ø°ÙŠÙ†Ùƒ', 'Ø±ÙŠØ«', 'Ø³ÙˆÙ', 'Ø³ÙˆÙ‰', 'Ø´ØªØ§Ù†', 'Ø¹Ø¯Ø§', 'Ø¹Ø³Ù‰', 'Ø¹Ù„', 'Ø¹Ù„Ù‰', 'Ø¹Ù„ÙŠÙƒ', 'Ø¹Ù„ÙŠÙ‡', 'Ø¹Ù…Ø§', 'Ø¹Ù†', 'Ø¹Ù†Ø¯', 'ØºÙŠØ±', 'ÙØ¥Ø°Ø§', 'ÙØ¥Ù†', 'ÙÙ„Ø§', 'ÙÙ…Ù†', 'ÙÙŠ', 'ÙÙŠÙ…', 'ÙÙŠÙ…Ø§', 'ÙÙŠÙ‡', 'ÙÙŠÙ‡Ø§', 'Ù‚Ø¯', 'ÙƒØ£Ù†', 'ÙƒØ£Ù†Ù…Ø§', 'ÙƒØ£ÙŠ', 'ÙƒØ£ÙŠÙ†', 'ÙƒØ°Ø§', 'ÙƒØ°Ù„Ùƒ', 'ÙƒÙ„', 'ÙƒÙ„Ø§', 'ÙƒÙ„Ø§Ù‡Ù…Ø§', 'ÙƒÙ„ØªØ§', 'ÙƒÙ„Ù…Ø§', 'ÙƒÙ„ÙŠÙƒÙ…Ø§', 'ÙƒÙ„ÙŠÙ‡Ù…Ø§', 'ÙƒÙ…', 'ÙƒÙ…', 'ÙƒÙ…Ø§', 'ÙƒÙŠ', 'ÙƒÙŠØª', 'ÙƒÙŠÙ', 'ÙƒÙŠÙÙ…Ø§', 'Ù„Ø§', 'Ù„Ø§Ø³ÙŠÙ…Ø§', 'Ù„Ø¯Ù‰', 'Ù„Ø³Øª', 'Ù„Ø³ØªÙ…', 'Ù„Ø³ØªÙ…Ø§', 'Ù„Ø³ØªÙ†', 'Ù„Ø³Ù†', 'Ù„Ø³Ù†Ø§', 'Ù„Ø¹Ù„', 'Ù„Ùƒ', 'Ù„ÙƒÙ…', 'Ù„ÙƒÙ…Ø§', 'Ù„ÙƒÙ†', 'Ù„ÙƒÙ†Ù…Ø§', 'Ù„ÙƒÙŠ', 'Ù„ÙƒÙŠÙ„Ø§', 'Ù„Ù…', 'Ù„Ù…Ø§', 'Ù„Ù†', 'Ù„Ù†Ø§', 'Ù„Ù‡', 'Ù„Ù‡Ø§', 'Ù„Ù‡Ù…', 'Ù„Ù‡Ù…Ø§', 'Ù„Ù‡Ù†', 'Ù„Ùˆ', 'Ù„ÙˆÙ„Ø§', 'Ù„ÙˆÙ…Ø§', 'Ù„ÙŠ', 'Ù„Ø¦Ù†', 'Ù„ÙŠØª', 'Ù„ÙŠØ³', 'Ù„ÙŠØ³Ø§', 'Ù„ÙŠØ³Øª', 'Ù„ÙŠØ³ØªØ§', 'Ù„ÙŠØ³ÙˆØ§', 'Ù…Ø§', 'Ù…Ø§Ø°Ø§', 'Ù…ØªÙ‰', 'Ù…Ø°', 'Ù…Ø¹', 'Ù…Ù…Ø§', 'Ù…Ù…Ù†', 'Ù…Ù†', 'Ù…Ù†Ù‡', 'Ù…Ù†Ù‡Ø§', 'Ù…Ù†Ø°', 'Ù…Ù‡', 'Ù…Ù‡Ù…Ø§', 'Ù†Ø­Ù†', 'Ù†Ø­Ùˆ', 'Ù†Ø¹Ù…', 'Ù‡Ø§', 'Ù‡Ø§ØªØ§Ù†', 'Ù‡Ø§ØªÙ‡', 'Ù‡Ø§ØªÙŠ', 'Ù‡Ø§ØªÙŠÙ†', 'Ù‡Ø§Ùƒ', 'Ù‡Ø§Ù‡Ù†Ø§', 'Ù‡Ø°Ø§', 'Ù‡Ø°Ø§Ù†', 'Ù‡Ø°Ù‡', 'Ù‡Ø°ÙŠ', 'Ù‡Ø°ÙŠÙ†', 'Ù‡ÙƒØ°Ø§', 'Ù‡Ù„', 'Ù‡Ù„Ø§', 'Ù‡Ù…', 'Ù‡Ù…Ø§', 'Ù‡Ù†', 'Ù‡Ù†Ø§', 'Ù‡Ù†Ø§Ùƒ', 'Ù‡Ù†Ø§Ù„Ùƒ', 'Ù‡Ùˆ', 'Ù‡Ø¤Ù„Ø§Ø¡', 'Ù‡ÙŠ', 'Ù‡ÙŠØ§', 'Ù‡ÙŠØª', 'Ù‡ÙŠÙ‡Ø§Øª', 'ÙˆØ§Ù„Ø°ÙŠ', 'ÙˆØ§Ù„Ø°ÙŠÙ†', 'ÙˆØ¥Ø°', 'ÙˆØ¥Ø°Ø§', 'ÙˆØ¥Ù†', 'ÙˆÙ„Ø§', 'ÙˆÙ„ÙƒÙ†', 'ÙˆÙ„Ùˆ', 'ÙˆÙ…Ø§', 'ÙˆÙ…Ù†', 'ÙˆÙ‡Ùˆ', 'ÙŠØ§', 'Ø£Ø¨ÙŒ', 'Ø£Ø®ÙŒ', 'Ø­Ù…ÙŒ', 'ÙÙˆ', 'Ø£Ù†ØªÙ', 'ÙŠÙ†Ø§ÙŠØ±', 'ÙØ¨Ø±Ø§ÙŠØ±', 'Ù…Ø§Ø±Ø³', 'Ø£Ø¨Ø±ÙŠÙ„', 'Ù…Ø§ÙŠÙˆ', 'ÙŠÙˆÙ†ÙŠÙˆ', 'ÙŠÙˆÙ„ÙŠÙˆ', 'Ø£ØºØ³Ø·Ø³', 'Ø³Ø¨ØªÙ…Ø¨Ø±', 'Ø£ÙƒØªÙˆØ¨Ø±', 'Ù†ÙˆÙÙ…Ø¨Ø±', 'Ø¯ÙŠØ³Ù…Ø¨Ø±', 'Ø¬Ø§Ù†ÙÙŠ', 'ÙÙŠÙØ±ÙŠ', 'Ù…Ø§Ø±Ø³', 'Ø£ÙØ±ÙŠÙ„', 'Ù…Ø§ÙŠ', 'Ø¬ÙˆØ§Ù†', 'Ø¬ÙˆÙŠÙ„ÙŠØ©', 'Ø£ÙˆØª', 'ÙƒØ§Ù†ÙˆÙ†', 'Ø´Ø¨Ø§Ø·', 'Ø¢Ø°Ø§Ø±', 'Ù†ÙŠØ³Ø§Ù†', 'Ø£ÙŠØ§Ø±', 'Ø­Ø²ÙŠØ±Ø§Ù†', 'ØªÙ…ÙˆØ²', 'Ø¢Ø¨', 'Ø£ÙŠÙ„ÙˆÙ„', 'ØªØ´Ø±ÙŠÙ†', 'Ø¯ÙˆÙ„Ø§Ø±', 'Ø¯ÙŠÙ†Ø§Ø±', 'Ø±ÙŠØ§Ù„', 'Ø¯Ø±Ù‡Ù…', 'Ù„ÙŠØ±Ø©', 'Ø¬Ù†ÙŠÙ‡', 'Ù‚Ø±Ø´', 'Ù…Ù„ÙŠÙ…', 'ÙÙ„Ø³', 'Ù‡Ù„Ù„Ø©', 'Ø³Ù†ØªÙŠÙ…', 'ÙŠÙˆØ±Ùˆ', 'ÙŠÙ†', 'ÙŠÙˆØ§Ù†', 'Ø´ÙŠÙƒÙ„', 'ÙˆØ§Ø­Ø¯', 'Ø§Ø«Ù†Ø§Ù†', 'Ø«Ù„Ø§Ø«Ø©', 'Ø£Ø±Ø¨Ø¹Ø©', 'Ø®Ù…Ø³Ø©', 'Ø³ØªØ©', 'Ø³Ø¨Ø¹Ø©', 'Ø«Ù…Ø§Ù†ÙŠØ©', 'ØªØ³Ø¹Ø©', 'Ø¹Ø´Ø±Ø©', 'Ø£Ø­Ø¯', 'Ø§Ø«Ù†Ø§', 'Ø§Ø«Ù†ÙŠ', 'Ø¥Ø­Ø¯Ù‰', 'Ø«Ù„Ø§Ø«', 'Ø£Ø±Ø¨Ø¹', 'Ø®Ù…Ø³', 'Ø³Øª', 'Ø³Ø¨Ø¹', 'Ø«Ù…Ø§Ù†ÙŠ', 'ØªØ³Ø¹', 'Ø¹Ø´Ø±', 'Ø«Ù…Ø§Ù†', 'Ø³Ø¨Øª', 'Ø£Ø­Ø¯', 'Ø§Ø«Ù†ÙŠÙ†', 'Ø«Ù„Ø§Ø«Ø§Ø¡', 'Ø£Ø±Ø¨Ø¹Ø§Ø¡', 'Ø®Ù…ÙŠØ³', 'Ø¬Ù…Ø¹Ø©', 'Ø£ÙˆÙ„', 'Ø«Ø§Ù†', 'Ø«Ø§Ù†ÙŠ', 'Ø«Ø§Ù„Ø«', 'Ø±Ø§Ø¨Ø¹', 'Ø®Ø§Ù…Ø³', 'Ø³Ø§Ø¯Ø³', 'Ø³Ø§Ø¨Ø¹', 'Ø«Ø§Ù…Ù†', 'ØªØ§Ø³Ø¹', 'Ø¹Ø§Ø´Ø±', 'Ø­Ø§Ø¯ÙŠ', 'Ø£', 'Ø¨', 'Øª', 'Ø«', 'Ø¬', 'Ø­', 'Ø®', 'Ø¯', 'Ø°', 'Ø±', 'Ø²', 'Ø³', 'Ø´', 'Øµ', 'Ø¶', 'Ø·', 'Ø¸', 'Ø¹', 'Øº', 'Ù', 'Ù‚', 'Ùƒ', 'Ù„', 'Ù…', 'Ù†', 'Ù‡', 'Ùˆ', 'ÙŠ', 'Ø¡', 'Ù‰', 'Ø¢', 'Ø¤', 'Ø¦', 'Ø£', 'Ø©', 'Ø£Ù„Ù', 'Ø¨Ø§Ø¡', 'ØªØ§Ø¡', 'Ø«Ø§Ø¡', 'Ø¬ÙŠÙ…', 'Ø­Ø§Ø¡', 'Ø®Ø§Ø¡', 'Ø¯Ø§Ù„', 'Ø°Ø§Ù„', 'Ø±Ø§Ø¡', 'Ø²Ø§ÙŠ', 'Ø³ÙŠÙ†', 'Ø´ÙŠÙ†', 'ØµØ§Ø¯', 'Ø¶Ø§Ø¯', 'Ø·Ø§Ø¡', 'Ø¸Ø§Ø¡', 'Ø¹ÙŠÙ†', 'ØºÙŠÙ†', 'ÙØ§Ø¡', 'Ù‚Ø§Ù', 'ÙƒØ§Ù', 'Ù„Ø§Ù…', 'Ù…ÙŠÙ…', 'Ù†ÙˆÙ†', 'Ù‡Ø§Ø¡', 'ÙˆØ§Ùˆ', 'ÙŠØ§Ø¡', 'Ù‡Ù…Ø²Ø©', 'ÙŠ', 'Ù†Ø§', 'Ùƒ', 'ÙƒÙ†', 'Ù‡', 'Ø¥ÙŠØ§Ù‡', 'Ø¥ÙŠØ§Ù‡Ø§', 'Ø¥ÙŠØ§Ù‡Ù…Ø§', 'Ø¥ÙŠØ§Ù‡Ù…', 'Ø¥ÙŠØ§Ù‡Ù†', 'Ø¥ÙŠØ§Ùƒ', 'Ø¥ÙŠØ§ÙƒÙ…Ø§', 'Ø¥ÙŠØ§ÙƒÙ…', 'Ø¥ÙŠØ§Ùƒ', 'Ø¥ÙŠØ§ÙƒÙ†', 'Ø¥ÙŠØ§ÙŠ', 'Ø¥ÙŠØ§Ù†Ø§', 'Ø£ÙˆÙ„Ø§Ù„Ùƒ', 'ØªØ§Ù†Ù', 'ØªØ§Ù†ÙÙƒ', 'ØªÙÙ‡', 'ØªÙÙŠ', 'ØªÙÙŠÙ’Ù†Ù', 'Ø«Ù…Ù‘', 'Ø«Ù…Ù‘Ø©', 'Ø°Ø§Ù†Ù', 'Ø°ÙÙ‡', 'Ø°ÙÙŠ', 'Ø°ÙÙŠÙ’Ù†Ù', 'Ù‡ÙØ¤Ù„Ø§Ø¡', 'Ù‡ÙØ§ØªØ§Ù†Ù', 'Ù‡ÙØ§ØªÙÙ‡', 'Ù‡ÙØ§ØªÙÙŠ', 'Ù‡ÙØ§ØªÙÙŠÙ’Ù†Ù', 'Ù‡ÙØ°Ø§', 'Ù‡ÙØ°Ø§Ù†Ù', 'Ù‡ÙØ°ÙÙ‡', 'Ù‡ÙØ°ÙÙŠ', 'Ù‡ÙØ°ÙÙŠÙ’Ù†Ù', 'Ø§Ù„Ø£Ù„Ù‰', 'Ø§Ù„Ø£Ù„Ø§Ø¡', 'Ø£Ù„', 'Ø£Ù†Ù‘Ù‰', 'Ø£ÙŠÙ‘', 'Ù‘Ø£ÙŠÙ‘Ø§Ù†', 'Ø£Ù†Ù‘Ù‰', 'Ø£ÙŠÙ‘', 'Ù‘Ø£ÙŠÙ‘Ø§Ù†', 'Ø°ÙŠØª', 'ÙƒØ£ÙŠÙ‘', 'ÙƒØ£ÙŠÙ‘Ù†', 'Ø¨Ø¶Ø¹', 'ÙÙ„Ø§Ù†', 'ÙˆØ§', 'Ø¢Ù…ÙŠÙ†Ù', 'Ø¢Ù‡Ù', 'Ø¢Ù‡Ù', 'Ø¢Ù‡Ø§Ù‹', 'Ø£ÙÙÙÙ‘', 'Ø£ÙÙÙÙ‘', 'Ø£ÙÙÙ‘', 'Ø£Ù…Ø§Ù…Ùƒ', 'Ø£Ù…Ø§Ù…ÙƒÙ', 'Ø£ÙˆÙ‘Ù‡Ù’', 'Ø¥Ù„ÙÙŠÙ’ÙƒÙ', 'Ø¥Ù„ÙÙŠÙ’ÙƒÙ', 'Ø¥Ù„ÙŠÙƒÙ', 'Ø¥Ù„ÙŠÙƒÙ†Ù‘', 'Ø¥ÙŠÙ‡Ù', 'Ø¨Ø®Ù', 'Ø¨Ø³Ù‘', 'Ø¨ÙØ³Ù’', 'Ø¨Ø·Ø¢Ù†', 'Ø¨ÙÙ„Ù’Ù‡Ù', 'Ø­Ø§ÙŠ', 'Ø­ÙØ°Ø§Ø±Ù', 'Ø­ÙŠÙÙ‘', 'Ø­ÙŠÙÙ‘', 'Ø¯ÙˆÙ†Ùƒ', 'Ø±ÙˆÙŠØ¯Ùƒ', 'Ø³Ø±Ø¹Ø§Ù†', 'Ø´ØªØ§Ù†Ù', 'Ø´ÙØªÙÙ‘Ø§Ù†Ù', 'ØµÙ‡Ù’', 'ØµÙ‡Ù', 'Ø·Ø§Ù‚', 'Ø·ÙÙ‚', 'Ø¹ÙØ¯ÙØ³Ù’', 'ÙƒÙØ®', 'Ù…ÙƒØ§Ù†ÙÙƒ', 'Ù…ÙƒØ§Ù†ÙÙƒ', 'Ù…ÙƒØ§Ù†ÙÙƒ', 'Ù…ÙƒØ§Ù†ÙƒÙ…', 'Ù…ÙƒØ§Ù†ÙƒÙ…Ø§', 'Ù…ÙƒØ§Ù†ÙƒÙ†Ù‘', 'Ù†ÙØ®Ù’', 'Ù‡Ø§ÙƒÙ', 'Ù‡ÙØ¬Ù’', 'Ù‡Ù„Ù…', 'Ù‡ÙŠÙ‘Ø§', 'Ù‡ÙÙŠÙ’Ù‡Ø§Øª', 'ÙˆØ§', 'ÙˆØ§Ù‡Ø§Ù‹', 'ÙˆØ±Ø§Ø¡ÙÙƒ', 'ÙˆÙØ´Ù’ÙƒÙØ§Ù†Ù', 'ÙˆÙÙŠÙ’', 'ÙŠÙØ¹Ù„Ø§Ù†', 'ØªÙØ¹Ù„Ø§Ù†', 'ÙŠÙØ¹Ù„ÙˆÙ†', 'ØªÙØ¹Ù„ÙˆÙ†', 'ØªÙØ¹Ù„ÙŠÙ†', 'Ø§ØªØ®Ø°', 'Ø£Ù„ÙÙ‰', 'ØªØ®Ø°', 'ØªØ±Ùƒ', 'ØªØ¹Ù„ÙÙ‘Ù…', 'Ø¬Ø¹Ù„', 'Ø­Ø¬Ø§', 'Ø­Ø¨ÙŠØ¨', 'Ø®Ø§Ù„', 'Ø­Ø³Ø¨', 'Ø®Ø§Ù„', 'Ø¯Ø±Ù‰', 'Ø±Ø£Ù‰', 'Ø²Ø¹Ù…', 'ØµØ¨Ø±', 'Ø¸Ù†ÙÙ‘', 'Ø¹Ø¯ÙÙ‘', 'Ø¹Ù„Ù…', 'ØºØ§Ø¯Ø±', 'Ø°Ù‡Ø¨', 'ÙˆØ¬Ø¯', 'ÙˆØ±Ø¯', 'ÙˆÙ‡Ø¨', 'Ø£Ø³ÙƒÙ†', 'Ø£Ø·Ø¹Ù…', 'Ø£Ø¹Ø·Ù‰', 'Ø±Ø²Ù‚', 'Ø²ÙˆØ¯', 'Ø³Ù‚Ù‰', 'ÙƒØ³Ø§', 'Ø£Ø®Ø¨Ø±', 'Ø£Ø±Ù‰', 'Ø£Ø¹Ù„Ù…', 'Ø£Ù†Ø¨Ø£', 'Ø­Ø¯ÙØ«', 'Ø®Ø¨ÙÙ‘Ø±', 'Ù†Ø¨ÙÙ‘Ø§', 'Ø£ÙØ¹Ù„ Ø¨Ù‡', 'Ù…Ø§ Ø£ÙØ¹Ù„Ù‡', 'Ø¨Ø¦Ø³', 'Ø³Ø§Ø¡', 'Ø·Ø§Ù„Ù…Ø§', 'Ù‚Ù„Ù…Ø§', 'Ù„Ø§Øª', 'Ù„ÙƒÙ†ÙÙ‘', 'Ø¡Ù', 'Ø£Ø¬Ù„', 'Ø¥Ø°Ø§Ù‹', 'Ø£Ù…Ù‘Ø§', 'Ø¥Ù…Ù‘Ø§', 'Ø¥Ù†ÙÙ‘', 'Ø£Ù†Ù‹Ù‘', 'Ø£Ù‰', 'Ø¥Ù‰', 'Ø£ÙŠØ§', 'Ø¨', 'Ø«Ù…ÙÙ‘', 'Ø¬Ù„Ù„', 'Ø¬ÙŠØ±', 'Ø±ÙØ¨ÙÙ‘', 'Ø³', 'Ø¹Ù„Ù‹Ù‘', 'Ù', 'ÙƒØ£Ù†Ù‘', 'ÙƒÙ„ÙÙ‘Ø§', 'ÙƒÙ‰', 'Ù„', 'Ù„Ø§Øª', 'Ù„Ø¹Ù„ÙÙ‘', 'Ù„ÙƒÙ†ÙÙ‘', 'Ù„ÙƒÙ†ÙÙ‘', 'Ù…', 'Ù†ÙÙ‘', 'Ù‡Ù„Ù‘Ø§', 'ÙˆØ§', 'Ø£Ù„', 'Ø¥Ù„Ù‘Ø§', 'Øª', 'Ùƒ', 'Ù„Ù…Ù‘Ø§', 'Ù†', 'Ù‡', 'Ùˆ', 'Ø§', 'ÙŠ', 'ØªØ¬Ø§Ù‡', 'ØªÙ„Ù‚Ø§Ø¡', 'Ø¬Ù…ÙŠØ¹', 'Ø­Ø³Ø¨', 'Ø³Ø¨Ø­Ø§Ù†', 'Ø´Ø¨Ù‡', 'Ù„Ø¹Ù…Ø±', 'Ù…Ø«Ù„', 'Ù…Ø¹Ø§Ø°', 'Ø£Ø¨Ùˆ', 'Ø£Ø®Ùˆ', 'Ø­Ù…Ùˆ', 'ÙÙˆ', 'Ù…Ø¦Ø©', 'Ù…Ø¦ØªØ§Ù†', 'Ø«Ù„Ø§Ø«Ù…Ø¦Ø©', 'Ø£Ø±Ø¨Ø¹Ù…Ø¦Ø©', 'Ø®Ù…Ø³Ù…Ø¦Ø©', 'Ø³ØªÙ…Ø¦Ø©', 'Ø³Ø¨Ø¹Ù…Ø¦Ø©', 'Ø«Ù…Ù†Ù…Ø¦Ø©', 'ØªØ³Ø¹Ù…Ø¦Ø©', 'Ù…Ø§Ø¦Ø©', 'Ø«Ù„Ø§Ø«Ù…Ø§Ø¦Ø©', 'Ø£Ø±Ø¨Ø¹Ù…Ø§Ø¦Ø©', 'Ø®Ù…Ø³Ù…Ø§Ø¦Ø©', 'Ø³ØªÙ…Ø§Ø¦Ø©', 'Ø³Ø¨Ø¹Ù…Ø§Ø¦Ø©', 'Ø«Ù…Ø§Ù†Ù…Ø¦Ø©', 'ØªØ³Ø¹Ù…Ø§Ø¦Ø©', 'Ø¹Ø´Ø±ÙˆÙ†', 'Ø«Ù„Ø§Ø«ÙˆÙ†', 'Ø§Ø±Ø¨Ø¹ÙˆÙ†', 'Ø®Ù…Ø³ÙˆÙ†', 'Ø³ØªÙˆÙ†', 'Ø³Ø¨Ø¹ÙˆÙ†', 'Ø«Ù…Ø§Ù†ÙˆÙ†', 'ØªØ³Ø¹ÙˆÙ†', 'Ø¹Ø´Ø±ÙŠÙ†', 'Ø«Ù„Ø§Ø«ÙŠÙ†', 'Ø§Ø±Ø¨Ø¹ÙŠÙ†', 'Ø®Ù…Ø³ÙŠÙ†', 'Ø³ØªÙŠÙ†', 'Ø³Ø¨Ø¹ÙŠÙ†', 'Ø«Ù…Ø§Ù†ÙŠÙ†', 'ØªØ³Ø¹ÙŠÙ†', 'Ø¨Ø¶Ø¹', 'Ù†ÙŠÙ', 'Ø£Ø¬Ù…Ø¹', 'Ø¬Ù…ÙŠØ¹', 'Ø¹Ø§Ù…Ø©', 'Ø¹ÙŠÙ†', 'Ù†ÙØ³', 'Ù„Ø§ Ø³ÙŠÙ…Ø§', 'Ø£ØµÙ„Ø§', 'Ø£Ù‡Ù„Ø§', 'Ø£ÙŠØ¶Ø§', 'Ø¨Ø¤Ø³Ø§', 'Ø¨Ø¹Ø¯Ø§', 'Ø¨ØºØªØ©', 'ØªØ¹Ø³Ø§', 'Ø­Ù‚Ø§', 'Ø­Ù…Ø¯Ø§', 'Ø®Ù„Ø§ÙØ§', 'Ø®Ø§ØµØ©', 'Ø¯ÙˆØ§Ù„ÙŠÙƒ', 'Ø³Ø­Ù‚Ø§', 'Ø³Ø±Ø§', 'Ø³Ù…Ø¹Ø§', 'ØµØ¨Ø±Ø§', 'ØµØ¯Ù‚Ø§', 'ØµØ±Ø§Ø­Ø©', 'Ø·Ø±Ø§', 'Ø¹Ø¬Ø¨Ø§', 'Ø¹ÙŠØ§Ù†Ø§', 'ØºØ§Ù„Ø¨Ø§', 'ÙØ±Ø§Ø¯Ù‰', 'ÙØ¶Ù„Ø§', 'Ù‚Ø§Ø·Ø¨Ø©', 'ÙƒØ«ÙŠØ±Ø§', 'Ù„Ø¨ÙŠÙƒ', 'Ù…Ø¹Ø§Ø°', 'Ø£Ø¨Ø¯Ø§', 'Ø¥Ø²Ø§Ø¡', 'Ø£ØµÙ„Ø§', 'Ø§Ù„Ø¢Ù†', 'Ø£Ù…Ø¯', 'Ø£Ù…Ø³', 'Ø¢Ù†ÙØ§', 'Ø¢Ù†Ø§Ø¡', 'Ø£Ù†Ù‘Ù‰', 'Ø£ÙˆÙ„', 'Ø£ÙŠÙ‘Ø§Ù†', 'ØªØ§Ø±Ø©', 'Ø«Ù…Ù‘', 'Ø«Ù…Ù‘Ø©', 'Ø­Ù‚Ø§', 'ØµØ¨Ø§Ø­', 'Ù…Ø³Ø§Ø¡', 'Ø¶Ø­ÙˆØ©', 'Ø¹ÙˆØ¶', 'ØºØ¯Ø§', 'ØºØ¯Ø§Ø©', 'Ù‚Ø·Ù‘', 'ÙƒÙ„Ù‘Ù…Ø§', 'Ù„Ø¯Ù†', 'Ù„Ù…Ù‘Ø§', 'Ù…Ø±Ù‘Ø©', 'Ù‚Ø¨Ù„', 'Ø®Ù„Ù', 'Ø£Ù…Ø§Ù…', 'ÙÙˆÙ‚', 'ØªØ­Øª', 'ÙŠÙ…ÙŠÙ†', 'Ø´Ù…Ø§Ù„', 'Ø§Ø±ØªØ¯Ù‘', 'Ø§Ø³ØªØ­Ø§Ù„', 'Ø£ØµØ¨Ø­', 'Ø£Ø¶Ø­Ù‰', 'Ø¢Ø¶', 'Ø£Ù…Ø³Ù‰', 'Ø§Ù†Ù‚Ù„Ø¨', 'Ø¨Ø§Øª', 'ØªØ¨Ø¯Ù‘Ù„', 'ØªØ­ÙˆÙ‘Ù„', 'Ø­Ø§Ø±', 'Ø±Ø¬Ø¹', 'Ø±Ø§Ø­', 'ØµØ§Ø±', 'Ø¸Ù„Ù‘', 'Ø¹Ø§Ø¯', 'ØºØ¯Ø§', 'ÙƒØ§Ù†', 'Ù…Ø§ Ø§Ù†ÙÙƒ', 'Ù…Ø§ Ø¨Ø±Ø­', 'Ù…Ø§Ø¯Ø§Ù…', 'Ù…Ø§Ø²Ø§Ù„', 'Ù…Ø§ÙØªØ¦', 'Ø§Ø¨ØªØ¯Ø£', 'Ø£Ø®Ø°', 'Ø§Ø®Ù„ÙˆÙ„Ù‚', 'Ø£Ù‚Ø¨Ù„', 'Ø§Ù†Ø¨Ø±Ù‰', 'Ø£Ù†Ø´Ø£', 'Ø£ÙˆØ´Ùƒ', 'Ø¬Ø¹Ù„', 'Ø­Ø±Ù‰', 'Ø´Ø±Ø¹', 'Ø·ÙÙ‚', 'Ø¹Ù„Ù‚', 'Ù‚Ø§Ù…', 'ÙƒØ±Ø¨', 'ÙƒØ§Ø¯', 'Ù‡Ø¨Ù‘']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "# Arabic stopwords\n",
    "arabic_stopwords = stopwords.words('arabic')\n",
    "\n",
    "# Print some of the stopwords\n",
    "print(arabic_stopwords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_arabic_stopwords(text):\n",
    "    # Load Arabic stopwords\n",
    "    arabic_stopwords = set(stopwords.words('arabic'))\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in words if word.lower() not in arabic_stopwords]\n",
    "    \n",
    "    # Recreate the text from filtered words\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Ù‡Ø°Ø§ Ù‡Ùˆ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶  ÙˆØ£ÙŠØ¶Ù‹Ø§ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©\n",
      "Text with Arabic stopwords removed: Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ ÙŠØ­ØªÙˆÙŠ ÙˆØ£ÙŠØ¶Ù‹Ø§ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"Ù‡Ø°Ø§ Ù‡Ùˆ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶  ÙˆØ£ÙŠØ¶Ù‹Ø§ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©\"\n",
    "cleaned_text = remove_arabic_stopwords(text)\n",
    "print(\"Original text:\", text)\n",
    "print(\"Text with Arabic stopwords removed:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8- remove non used characters \"do it for each language\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_non_arabic(text):\n",
    "    # Define regex pattern to match non-Arabic characters and non-numeric characters\n",
    "    arabic_pattern = re.compile(r'[^\\u0600-\\u06FF\\u0750-\\u077F\\d\\s]+')\n",
    "    \n",
    "    # Remove non-Arabic and non-numeric characters from the text\n",
    "    cleaned_text = arabic_pattern.sub('', text)\n",
    "    \n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Ù‡Ø°Ø§ Ù‡Ùˆ Ù†Øµ (ØªØ¬Ø±ÙŠØ¨ÙŠ) gghhgj ÙŠØ­ØªÙˆÙŠ 123 Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ø§Ù„ÙƒÙ„Ù…Ø§Øª.,; () Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©  1315Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©!\n",
      "Text with non-Arabic characters removed: Ù‡Ø°Ø§ Ù‡Ùˆ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ  ÙŠØ­ØªÙˆÙŠ 123 Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ø§Ù„ÙƒÙ„Ù…Ø§Øª  Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©  1315Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"Ù‡Ø°Ø§ Ù‡Ùˆ Ù†Øµ (ØªØ¬Ø±ÙŠØ¨ÙŠ) gghhgj ÙŠØ­ØªÙˆÙŠ 123 Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ø§Ù„ÙƒÙ„Ù…Ø§Øª.,; () Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©  1315Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©!\"\n",
    "cleaned_text = remove_non_arabic(text)\n",
    "print(\"Original text:\", text)\n",
    "print(\"Text with non-Arabic characters removed:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9- remove repeated characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_repeated_characters(text):\n",
    "    # Define regex pattern to match repeated Arabic characters\n",
    "    repeated_pattern = re.compile(r'(\\S)(\\1)+', re.UNICODE)\n",
    "    \n",
    "    # Remove repeated characters from the text\n",
    "    cleaned_text = repeated_pattern.sub(r'\\1', text)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Ù‡Ø°Ø§Ø§Ø§Ø§ Ù‡ÙˆÙˆÙˆÙˆ Ù†Øµ Ù…ÙƒÙƒÙƒÙƒÙƒÙƒØ±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±\n",
      "Text with repeated characters removed: Ù‡Ø°Ø§ Ù‡Ùˆ Ù†Øµ Ù…ÙƒØ±\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "text = \"Ù‡Ø°Ø§Ø§Ø§Ø§ Ù‡ÙˆÙˆÙˆÙˆ Ù†Øµ Ù…ÙƒÙƒÙƒÙƒÙƒÙƒØ±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±\"\n",
    "cleaned_text = remove_repeated_characters(text)\n",
    "print(\"Original text:\", text)\n",
    "print(\"Text with repeated characters removed:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11- for arabic remove tashkeel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarabic.araby as araby\n",
    "text= araby.strip_tashkeel(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strip tatweel from a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\n"
     ]
    }
   ],
   "source": [
    "from pyarabic.araby import strip_tatweel\n",
    "text = u\"Ø§Ù„Ø¹Ù€Ù€Ù€Ù€Ù€Ø±Ø¨ÙŠØ©\"\n",
    "print(strip_tatweel(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalizeArabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeArabic(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(\"[Ø¥Ø£Ù±Ø¢Ø§]\", \"Ø§\", text)\n",
    "    text = re.sub(\"Ù‰\", \"ÙŠ\", text)\n",
    "    text = re.sub(\"Ø¤\", \"Ø¡\", text)\n",
    "    text = re.sub(\"Ø¦\", \"Ø¡\", text)\n",
    "    text = re.sub(\"Ø©\", \"Ù‡\", text)\n",
    "    noise = re.compile(\"\"\" Ù‘    | # Tashdid\n",
    "                             Ù    | # Fatha\n",
    "                             Ù‹    | # Tanwin Fath\n",
    "                             Ù    | # Damma\n",
    "                             ÙŒ    | # Tanwin Damm\n",
    "                             Ù    | # Kasra\n",
    "                             Ù    | # Tanwin Kasr\n",
    "                             Ù’    | # Sukun\n",
    "                             Ù€     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    text = re.sub(noise, '', text)\n",
    "    text = re.sub(r'(.)\\1+', r\"\\1\\1\", text) # Remove longation\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stemming words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: snowballstemmer in c:\\users\\pc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install snowballstemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÙŠØ§ÙƒÙ„\n"
     ]
    }
   ],
   "source": [
    "from snowballstemmer import stemmer\n",
    "\n",
    "def stem_arabic_word(word):\n",
    "    # Create an Arabic stemmer using the Khoja algorithm\n",
    "    arabic_stemmer = stemmer(\"arabic\")\n",
    "    # Stem the word\n",
    "    stemmed_word = arabic_stemmer.stemWord(word)\n",
    "    return stemmed_word\n",
    "\n",
    "# Example usage:\n",
    "word = \"ÙŠØ£ÙƒÙ„ÙˆÙ†\"\n",
    "stemmed_word = stem_arabic_word(word)\n",
    "print(stemmed_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word: ÙŠÙ‚Ø±Ø£\n",
      "Stemmed word: Ù‚Ø±Ø£\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.isri import ISRIStemmer\n",
    "\n",
    "# Initialize ISRIStemmer\n",
    "arabic_stemmer = ISRIStemmer()\n",
    "\n",
    "# Example Arabic word\n",
    "word = \"ÙŠÙ‚Ø±Ø£\"\n",
    "\n",
    "# Stem the word\n",
    "stemmed_word = arabic_stemmer.stem(word)\n",
    "\n",
    "print(\"Original word:\", word)\n",
    "print(\"Stemmed word:\", stemmed_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ÙØ§Ø³Ù…ÙŒ', 'Ø§Ù„ÙƒÙ„Ø¨Ù', 'ÙÙŠ', 'Ø§Ù„Ù„ØºØ©Ù', 'Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©', 'ÙˆØ§Ø³Ù…Ù', 'Ø§Ù„Ø­Ù…Ø§Ø±Ù']"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyarabic.araby import tokenize, is_arabicrange, strip_tashkeel\n",
    "text = u\"ÙØ§Ø³Ù…ÙŒ Ø§Ù„ÙƒÙ„Ø¨Ù ÙÙŠ 21 Ø§Ù„Ù„ØºØ©Ù Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Dog ÙˆØ§Ø³Ù…Ù Ø§Ù„Ø­Ù…Ø§Ø±Ù Donky\"\n",
    "tokenize(text, conditions=is_arabicrange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the proposed algorithm to treat the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_hashtag_to_words(tag):\n",
    "    tag = tag.replace('#','')\n",
    "    tags = tag.split('_')\n",
    "    if len(tags) > 1 :\n",
    "        \n",
    "        return tags\n",
    "    pattern = re.compile(r\"[A-Z][a-z]+|\\d+|[A-Z]+(?![a-z])\")\n",
    "    return pattern.findall(tag)\n",
    "\n",
    "def extract_hashtag(text):\n",
    "    \n",
    "    hash_list = ([re.sub(r\"(\\W+)$\", \"\", i) for i in text.split() if i.startswith(\"#\")])\n",
    "    word_list = []\n",
    "    for word in hash_list :\n",
    "        word_list.extend(split_hashtag_to_words(word))\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ù…Ø³ØªØ®Ø¯Ù…1', 'Ù…Ø³ØªØ®Ø¯Ù…2']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_mentions_arabic(text):\n",
    "    # Find all mentions starting with '@' and extract the username\n",
    "    mentions = re.findall(r'@(\\w+)', text)\n",
    "    return mentions\n",
    "\n",
    "# Example usage:\n",
    "text_arabic = \"Ù…Ø±Ø­Ø¨Ø§ @Ù…Ø³ØªØ®Ø¯Ù…1 Ùˆ @Ù…Ø³ØªØ®Ø¯Ù…2ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒÙ…ØŸ\"\n",
    "mentions_arabic = extract_mentions_arabic(text_arabic)\n",
    "print(mentions_arabic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://example.com', 'https://example.org']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_urls(text):\n",
    "    # Regular expression pattern to match URLs\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    # Find all URLs in the text\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    return urls\n",
    "\n",
    "# Example usage:\n",
    "arabic_text_with_urls = \"Ù‚Ù… Ø¨Ø²ÙŠØ§Ø±Ø© Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆÙ‚Ø¹: https://example.comØŒ ÙˆØ£ÙŠØ¶Ù‹Ø§ https://example.org\"\n",
    "urls = extract_urls(arabic_text_with_urls)\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = clean_hashtag(text)\n",
    "    text = clean_emoji(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_punctuations(text)\n",
    "    text = remove_arabic_stopwords(text)\n",
    "    text = remove_non_arabic(text)\n",
    "    text = remove_repeated_characters(text)\n",
    "    tokens = tokenize(text)\n",
    "    print(tokens)\n",
    "    # Assuming tokens is a list of Arabic words\n",
    "    for i, word in enumerate(tokens):\n",
    "\n",
    "        # Remove diacritics (Tashkeel)\n",
    "        diacritic_removed_word = araby.strip_tashkeel(word)\n",
    "        \n",
    "        # Remove non-Arabic characters\n",
    "        arabic_only_word = remove_non_arabic(diacritic_removed_word)\n",
    "        \n",
    "        # Remove tatweel\n",
    "        final_word = strip_tatweel(arabic_only_word)\n",
    "\n",
    "        # Apply stemming (we need a good stemmer)\n",
    "        stemmed_word = ArabicLightStemmer().light_stem(final_word)\n",
    "    \n",
    "        # Apply normalization\n",
    "        normalized_word = normalizeArabic(stemmed_word)\n",
    "        \n",
    "        # Replace the original token with the processed one\n",
    "        tokens[i] = final_word\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ù‚Ù…', 'Ø¨Ø²ÙŠØ§Ø±Ø©', '12', 'Ø§Ù„Ù…ÙˆÙ‚Ø¹', 'Ø£ÙŠØ¶Ù‹Ø§']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Ù‚Ù…', 'Ø¨Ø²ÙŠØ§Ø±Ø©', '12', 'Ø§Ù„Ù…ÙˆÙ‚Ø¹', 'Ø£ÙŠØ¶Ø§']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage:\n",
    "arabic_text_with_urls = \"Ù‚Ù… Ø¨Ø²ÙŠØ§Ø±Ø© 12 Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆÙ‚Ø¹: https://example.comØŒ Ùˆ Ø£ÙŠØ¶Ù‹Ø§ https://example.org\"\n",
    "clean_text(arabic_text_with_urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
