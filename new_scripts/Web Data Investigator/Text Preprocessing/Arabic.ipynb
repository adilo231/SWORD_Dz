{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip  install pyarabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install aiogoogletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install snowballstemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import unicodedata\n",
    "from unidecode import unidecode\n",
    "import emoji\n",
    "from aiogoogletrans import Translator\n",
    "import asyncio\n",
    "import string\n",
    "import re\n",
    "import pyarabic.araby as araby\n",
    "from pyarabic.araby import strip_tatweel\n",
    "from pyarabic import araby\n",
    "from snowballstemmer import stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "# Arabic stopwords\n",
    "arabic_stopwords = stopwords.words('arabic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emails(text):\n",
    "\temails = re.findall(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+\\b)', text)\n",
    "\treturn emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_hashtag_to_words(tag):\n",
    "    tag = tag.replace('#', '')\n",
    "    tags = tag.split('_')\n",
    "    if len(tags) > 1:\n",
    "        return tags\n",
    "    else:\n",
    "        return [tag]\n",
    "\n",
    "def extract_hashtag(text):\n",
    "    hash_list = ([re.sub(r\"(\\W+)$\", \"\", i) for i in text.split() if i.startswith(\"#\")])\n",
    "    word_list = []\n",
    "    for word in hash_list:\n",
    "        word_list.extend(split_hashtag_to_words(word))\n",
    "    return word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ø¬ÙŠØ¯', 'Ø§Ø³ØªØ®Ø±Ø§Ø¬', 'Ø§Ù„ÙˆØ³Ù…']\n"
     ]
    }
   ],
   "source": [
    "text = \"Ù‡Ø°Ø§ Ù‡Ùˆ Ù…Ø«Ø§Ù„ #Ø¬ÙŠØ¯ Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ÙˆØ¸ÙŠÙØ© #Ø§Ø³ØªØ®Ø±Ø§Ø¬_Ø§Ù„ÙˆØ³Ù…\"\n",
    "result = extract_hashtag(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ø§Ù„ØªØ­Ø¯Ø«', 'Ø§Ù„ØªØ¹Ø¨ÙŠØ±', '']\n"
     ]
    }
   ],
   "source": [
    "text = \"Ù‡Ù„ Ø¬Ø±Ø¨Øª #Ø§Ù„ØªØ­Ø¯Ø« Ø¨Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠØŸ ÙŠØ¶ÙŠÙ Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª #Ø§Ù„ØªØ¹Ø¨ÙŠØ±_ Ø¹Ù† Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø¨Ø´ÙƒÙ„ Ù…Ù…ØªØ¹! ğŸ˜ŠğŸ“±\"\n",
    "result = extract_hashtag(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ù…Ø³ØªØ®Ø¯Ù…1', 'Ù…Ø³ØªØ®Ø¯Ù…2']\n"
     ]
    }
   ],
   "source": [
    "def extract_mentions_arabic(text):\n",
    "    mentions = re.findall(r'@(\\w+)', text)\n",
    "    return mentions\n",
    "\n",
    "\n",
    "text_arabic = \"Ù…Ø±Ø­Ø¨Ø§ @Ù…Ø³ØªØ®Ø¯Ù…1 Ùˆ @Ù…Ø³ØªØ®Ø¯Ù…2ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒÙ…ØŸ\"\n",
    "mentions_arabic = extract_mentions_arabic(text_arabic)\n",
    "print(mentions_arabic)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- get urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https', 'example.com', ''), ('https', 'example.org', '')]\n"
     ]
    }
   ],
   "source": [
    "def extract_urls(text):\n",
    "\turls = re.findall(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?',text)\n",
    "\treturn urls\n",
    "\n",
    "\n",
    "text = \"Ù‚Ù… Ø¨Ø²ÙŠØ§Ø±Ø© Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆÙ‚Ø¹: https://example.comØŒ ÙˆØ£ÙŠØ¶Ù‹Ø§ https://example.org\"\n",
    "urls = extract_urls(text)\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- treat hashtags : remove the whole hashtag or just the character "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hashtag(text):\n",
    "    words = text.split()\n",
    "    text = list()\n",
    "    for word in words:\n",
    "        if is_hashtag(word):\n",
    "            text.extend(extract_hashtag(word))\n",
    "        else:\n",
    "            text.append(word)\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "def is_hashtag(word):\n",
    "    if word.startswith(\"#\"):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def remove_hashtag_char(text):\n",
    "    mention_pattern = r'#+'\n",
    "    cleaned_text = re.sub(mention_pattern, '', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ù…Ø±Ø­Ø¨Ø§ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ Ù…Ù†Ø´Ù† dfhg Ù„Ù€ @Ù…Ø³ØªØ®Ø¯Ù…2 ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©.\n"
     ]
    }
   ],
   "source": [
    "text = \"Ù…Ø±Ø­Ø¨Ø§  ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ Ù…Ù†Ø´Ù† #dfhg#  Ù„Ù€ @Ù…Ø³ØªØ®Ø¯Ù…2 ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©.\"\n",
    "cleaned_text = clean_hashtag(text)\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5- remove mentions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ù…Ø±Ø­Ø¨Ø§ Ù…Ø³ØªØ®Ø¯Ù…1ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ Ù…Ù†Ø´Ù† Ù„Ù€ Ù…Ø³ØªØ®Ø¯Ù…2 ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©.\n"
     ]
    }
   ],
   "source": [
    "def remove_mentions(text):\n",
    "    mention_pattern = r'@+'\n",
    "    cleaned_text = re.sub(mention_pattern, '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "text = \"Ù…Ø±Ø­Ø¨Ø§ @Ù…Ø³ØªØ®Ø¯Ù…1ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ Ù…Ù†Ø´Ù† Ù„Ù€ @Ù…Ø³ØªØ®Ø¯Ù…2 ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©.\"\n",
    "cleaned_text = remove_mentions(text)\n",
    "print(cleaned_text)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6- replace emojis \"only arabic case is treated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('emojis.csv','r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    emojis_ar = {}\n",
    "    for line in lines:\n",
    "        line = line.strip('\\n').split(';')\n",
    "        emojis_ar.update({line[0].strip():line[1].strip()})\n",
    "\n",
    "\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  \n",
    "                                   u\"\\U0001F300-\\U0001F5FF\" \n",
    "                                   u\"\\U0001F680-\\U0001F6FF\" \n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\" \n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def emoji_native_translation(text):\n",
    "    text = text.lower()\n",
    "    loves = [\"<3\", \"â™¥\",'â¤']\n",
    "    smilefaces = []\n",
    "    sadfaces = []\n",
    "    neutralfaces = []\n",
    "\n",
    "    eyes = [\"8\",\":\",\"=\",\";\"]\n",
    "    nose = [\"'\",\"`\",\"-\",r\"\\\\\"]\n",
    "    for e in eyes:\n",
    "        for n in nose:\n",
    "            for s in [\"\\)\", \"d\", \"]\", \"}\",\"p\"]:\n",
    "                smilefaces.append(e+n+s)\n",
    "                smilefaces.append(e+s)\n",
    "            for s in [\"\\(\", \"\\[\", \"{\"]:\n",
    "                sadfaces.append(e+n+s)\n",
    "                sadfaces.append(e+s)\n",
    "            for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n",
    "                neutralfaces.append(e+n+s)\n",
    "                neutralfaces.append(e+s)\n",
    "            #reversed\n",
    "            for s in [\"\\(\", \"\\[\", \"{\"]:\n",
    "                smilefaces.append(s+n+e)\n",
    "                smilefaces.append(s+e)\n",
    "            for s in [\"\\)\", \"\\]\", \"}\"]:\n",
    "                sadfaces.append(s+n+e)\n",
    "                sadfaces.append(s+e)\n",
    "            for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n",
    "                neutralfaces.append(s+n+e)\n",
    "                neutralfaces.append(s+e)\n",
    "\n",
    "    smilefaces = list(set(smilefaces))\n",
    "    sadfaces = list(set(sadfaces))\n",
    "    neutralfaces = list(set(neutralfaces))\n",
    "    t = []\n",
    "    for w in text.split():\n",
    "        if w in loves:\n",
    "            t.append(\"Ø­Ø¨\")\n",
    "        elif w in smilefaces:\n",
    "            t.append(\"Ù…Ø¶Ø­Ùƒ\")\n",
    "        elif w in neutralfaces:\n",
    "            t.append(\"Ø¹Ø§Ø¯ÙŠ\")\n",
    "        elif w in sadfaces:\n",
    "            t.append(\"Ù…Ø­Ø²Ù†\")\n",
    "        else:\n",
    "            t.append(w)\n",
    "    newText = \" \".join(t)\n",
    "    return newText\n",
    "\n",
    "   \n",
    "def is_emoji(word):\n",
    "    if word in emojis_ar:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def add_space(text):\n",
    "    return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()\n",
    "\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "def translate_emojis(words):\n",
    "    word_list = list()\n",
    "    words_to_translate = list()\n",
    "    for word in words :\n",
    "        t = emojis_ar.get(word.get('emoji'),None)\n",
    "        if t is None:\n",
    "            word.update({'translation':'Ø¹Ø§Ø¯ÙŠ','translated':True})\n",
    "            #words_to_translate.append('normal')\n",
    "        else:\n",
    "            word.update({'translated':False,'translation':t})\n",
    "            words_to_translate.append(t.replace(':','').replace('_',' '))\n",
    "        word_list.append(word)\n",
    "    return word_list\n",
    "\n",
    "def emoji_unicode_translation(text):\n",
    "    text = add_space(text)\n",
    "    words = text.split()\n",
    "    text_list = list()\n",
    "    emojis_list = list()\n",
    "    c = 0\n",
    "    for word in words:\n",
    "        if is_emoji(word):\n",
    "            emojis_list.append({'emoji':word,'emplacement':c})\n",
    "        else:\n",
    "            text_list.append(word)\n",
    "        c+=1\n",
    "    emojis_translated = translate_emojis(emojis_list)\n",
    "    for em in emojis_translated:\n",
    "        text_list.insert(em.get('emplacement'),em.get('translation'))\n",
    "    text = \" \".join(text_list)\n",
    "    return text\n",
    "    \n",
    "def clean_emoji(text):\n",
    "    text = emoji_native_translation(text)\n",
    "    text = emoji_unicode_translation(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø£Ø­Ø¨Ùƒ Ù‚Ù„Ø¨ Ø§Ø­Ù…Ø± Ø®Ø¯ÙˆØ¯\n",
      "Ø£Ù†Ø§ Ø­Ø²ÙŠÙ† Ø¬Ø¯Ù‹Ø§ ÙŠØ¨ÙƒÙŠ\n",
      "Ø£Ù†Ø§ ÙÙŠ Ø­Ø§Ù„Ø© Ù…Ø²Ø§Ø¬ÙŠØ© Ø¬ÙŠØ¯Ø© Ø§Ø¨ØªØ³Ø§Ù…Ø©\n"
     ]
    }
   ],
   "source": [
    "text_with_emojis = \"Ø£Ø­Ø¨Ùƒ â¤ï¸ğŸ˜Š\"\n",
    "cleaned_text = clean_emoji(text_with_emojis)\n",
    "print(cleaned_text)\n",
    "\n",
    "\n",
    "text_with_emojis = \"Ø£Ù†Ø§ Ø­Ø²ÙŠÙ† Ø¬Ø¯Ù‹Ø§ ğŸ˜¢\"\n",
    "cleaned_text = clean_emoji(text_with_emojis)\n",
    "print(cleaned_text)\n",
    "\n",
    "\n",
    "text_with_emojis = \"Ø£Ù†Ø§ ÙÙŠ Ø­Ø§Ù„Ø© Ù…Ø²Ø§Ø¬ÙŠØ© Ø¬ÙŠØ¯Ø© ğŸ˜„\"\n",
    "cleaned_text = clean_emoji(text_with_emojis)\n",
    "print(cleaned_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emails(text):\n",
    "\treturn re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+\\b)',\"\",text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    return re.sub(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?',\"\",text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic text with URLs removed: ØªØ­Ù‚Ù‚ Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ø±Ø§Ø¨Ø·: ØŒ Ø¥Ù†Ù‡ Ø±Ø§Ø¦Ø¹!\n"
     ]
    }
   ],
   "source": [
    "text = \"ØªØ­Ù‚Ù‚ Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ø±Ø§Ø¨Ø·: https://example.comØŒ Ø¥Ù†Ù‡ Ø±Ø§Ø¦Ø¹!\"\n",
    "cleaned_text = remove_urls(text)\n",
    "print(\"Arabic text with URLs removed:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7- remove punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_punctuation = string.punctuation\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    return re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,ØŒ-./:;<=>ØŸ?@[\\]^_`{|}~\"\"\"), ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text with punctuations removed: ØªØ­Ù‚Ù‚ Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ø±Ø§Ø¨Ø·  https   example com  Ø¥Ù†Ù‡ Ø±Ø§Ø¦Ø¹ \n"
     ]
    }
   ],
   "source": [
    "text = \"ØªØ­Ù‚Ù‚ Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ø±Ø§Ø¨Ø·: https://example.comØŒ Ø¥Ù†Ù‡ Ø±Ø§Ø¦Ø¹!\"\n",
    "cleaned_text = remove_punctuations(text)\n",
    "\n",
    "print(\"text with punctuations removed:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove non used arabic characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_arabic(text):\n",
    "    arabic_pattern = re.compile(r'[^\\u0600-\\u06FF\\u0750-\\u077F\\d\\s]+')\n",
    "    cleaned_text = arabic_pattern.sub('', text)\n",
    "    \n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove repeated characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeated_characters(text):\n",
    "    repeated_pattern = re.compile(r'(\\S)(\\1)+', re.UNICODE)\n",
    "    cleaned_text = repeated_pattern.sub(r'\\1', text)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with repeated characters removed: Ù‡Ø°Ø§ Ù‡Ùˆ Ù†Øµ Ù…ÙƒØ±\n"
     ]
    }
   ],
   "source": [
    "text = \"Ù‡Ø°Ø§Ø§Ø§Ø§ Ù‡ÙˆÙˆÙˆÙˆ Ù†Øµ Ù…ÙƒÙƒÙƒÙƒÙƒÙƒØ±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±Ø±\"\n",
    "cleaned_text = remove_repeated_characters(text)\n",
    "print(\"Text with repeated characters removed:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11- for arabic remove tashkeel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= araby.strip_tashkeel(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strip tatweel from a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\n"
     ]
    }
   ],
   "source": [
    "text = u\"Ø§Ù„Ø¹Ù€Ù€Ù€Ù€Ù€Ø±Ø¨ÙŠØ©\"\n",
    "print(strip_tatweel(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalizeArabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeArabic(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(\"[Ø¥Ø£Ù±Ø¢Ø§]\", \"Ø§\", text)\n",
    "    text = re.sub(\"Ù‰\", \"ÙŠ\", text)\n",
    "    text = re.sub(\"Ø¤\", \"Ø¡\", text)\n",
    "    text = re.sub(\"Ø¦\", \"Ø¡\", text)\n",
    "    text = re.sub(\"Ø©\", \"Ù‡\", text)\n",
    "    noise = re.compile(\"\"\" Ù‘    | # Tashdid\n",
    "                             Ù    | # Fatha\n",
    "                             Ù‹    | # Tanwin Fath\n",
    "                             Ù    | # Damma\n",
    "                             ÙŒ    | # Tanwin Damm\n",
    "                             Ù    | # Kasra\n",
    "                             Ù    | # Tanwin Kasr\n",
    "                             Ù’    | # Sukun\n",
    "                             Ù€     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    text = re.sub(noise, '', text)\n",
    "    text = re.sub(r'(.)\\1+', r\"\\1\\1\", text) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_arabic_stopwords_tokenize(text):    \n",
    "    tokens = araby.tokenize(text)\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stopwords.words()]\n",
    "    \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with Arabic stopwords removed: ['Ù†Øµ', 'ØªØ¬Ø±ÙŠØ¨ÙŠ', 'ÙŠØ­ØªÙˆÙŠ', 'Ø§Ù„ÙƒÙ„Ù…Ø§Øª', 'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©', 'Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©']\n"
     ]
    }
   ],
   "source": [
    "text = \"Ù‡Ø°Ø§ Ù‡Ùˆ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©\"\n",
    "cleaned_text = remove_arabic_stopwords_tokenize(text)\n",
    "print(\"Text with Arabic stopwords removed:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stemming words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÙŠØ§ÙƒÙ„\n"
     ]
    }
   ],
   "source": [
    "def stem_arabic_word(word):\n",
    "    arabic_stemmer = stemmer(\"arabic\")\n",
    "    stemmed_word = arabic_stemmer.stemWord(word)\n",
    "    return stemmed_word\n",
    "\n",
    "word = \"ÙŠØ£ÙƒÙ„ÙˆÙ†\"\n",
    "stemmed_word = stem_arabic_word(word)\n",
    "print(stemmed_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove multipe spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multiple_spaces(text):\n",
    "\treturn ' '.join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„ØºØ© Ø¬Ù…ÙŠÙ„Ø©.'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = u\"Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„ØºØ©       Ø¬Ù…ÙŠÙ„Ø©.\"\n",
    "remove_multiple_spaces(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the proposed algorithm to treat the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user@gmail.com']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"user@gmail.com\"\n",
    "get_emails(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ø§Ù„ØªØ­Ø¯Ø«', 'Ø§Ù„ØªØ¹Ø¨ÙŠØ±']\n"
     ]
    }
   ],
   "source": [
    "text = \"Ù‡Ù„ Ø¬Ø±Ø¨Øª #Ø§Ù„ØªØ­Ø¯Ø« Ø¨Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠØŸ ÙŠØ¶ÙŠÙ Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª #Ø§Ù„ØªØ¹Ø¨ÙŠØ± Ø¹Ù† Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø¨Ø´ÙƒÙ„ Ù…Ù…ØªØ¹! ğŸ˜ŠğŸ“±\"\n",
    "result = extract_hashtag(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ù…Ø³ØªØ®Ø¯Ù…1', 'Ù…Ø³ØªØ®Ø¯Ù…2']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"Ù…Ø±Ø­Ø¨Ø§ @Ù…Ø³ØªØ®Ø¯Ù…1 Ùˆ @Ù…Ø³ØªØ®Ø¯Ù…2ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒÙ…ØŸ\"\n",
    "mentions = extract_mentions_arabic(text)\n",
    "print(mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https', 'example.com', ''), ('https', 'example.org', '')]\n"
     ]
    }
   ],
   "source": [
    "text_with_urls = \"Ù‚Ù… Ø¨Ø²ÙŠØ§Ø±Ø© Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆÙ‚Ø¹: https://example.comØŒ ÙˆØ£ÙŠØ¶Ù‹Ø§ https://example.org\"\n",
    "urls = extract_urls(text_with_urls)\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = clean_hashtag(text)\n",
    "    text = clean_emoji(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_punctuations(text)\n",
    "    text = remove_non_arabic(text)\n",
    "    text = remove_repeated_characters(text)\n",
    "    text = remove_multiple_spaces(text)\n",
    "    tokens = remove_arabic_stopwords_tokenize(text)\n",
    "    for i, word in enumerate(tokens):\n",
    "        diacritic_removed_word = araby.strip_tashkeel(word)\n",
    "        final_word = strip_tatweel(diacritic_removed_word)\n",
    "        normalized_word = normalizeArabic(final_word)\n",
    "        stemmed_word = stem_arabic_word(normalized_word)\n",
    "        tokens[i] = stemmed_word\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ù‚Ù…', 'Ø²ÙŠØ§Ø±', '12', 'Ù…ÙˆÙ‚Ø¹', 'Ø§ÙŠØ¶']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arabic_text_with_urls = \"Ù‚Ù…       Ø¨Ø²ÙŠØ§Ø±Ø© 12 Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆÙ‚Ø¹: https://example.comØŒ Ùˆ Ø£ÙŠØ¶Ù‹Ø§ https://example.org\"\n",
    "clean_text(arabic_text_with_urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
