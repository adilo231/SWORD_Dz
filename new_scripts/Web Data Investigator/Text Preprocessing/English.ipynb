{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install aiogoogletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import emoji\n",
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- get hashtags\n",
    "2- get mentions\n",
    "3- get urls\n",
    "4- remove hashtags\n",
    "5- treat mentions \"remove only @\"\n",
    "6- replace emojis \"only arabic case is treated\"\n",
    "7- text = re.sub('http\\S+\\s*', ' ', text)  # remove URLs\n",
    "7- remove punctuations\n",
    "10- remove stop words \n",
    "8- remove non used characters \n",
    "9- remove repeated characters\n",
    "11- for arabic remove tashkeel\n",
    "12- Strip tatweel from a text.\n",
    "13- stem words\n",
    "14- normalizeArabic\n",
    "15- tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emails(text):\n",
    "\temails = re.findall(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+\\b)', text)\n",
    "\n",
    "\treturn emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_hashtag_to_words(tag):\n",
    "    tag = tag.replace('#', '')\n",
    "    tags = tag.split('_')\n",
    "    if len(tags) > 1:\n",
    "        return tags\n",
    "    pattern = re.compile(r\"[A-Za-z]+|\\d+\")\n",
    "    return pattern.findall(tag)\n",
    "\n",
    "def extract_hashtag(text):\n",
    "    hash_list = ([re.sub(r\"(\\W+)$\", \"\", i) for i in text.split() if i.startswith(\"#\")])\n",
    "    word_list = []\n",
    "    for word in hash_list:\n",
    "        word_list.extend(split_hashtag_to_words(word))\n",
    "    return word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nexample', 'text']\n"
     ]
    }
   ],
   "source": [
    "text = \"this is #√©nexample #text \"\n",
    "print(extract_hashtag(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user1', 'user2']\n"
     ]
    }
   ],
   "source": [
    "def extract_mentions(text):\n",
    "    mentions = re.findall(r'@(\\w+)', text)\n",
    "    return mentions\n",
    "\n",
    "text_arabic = \"@user1 watch out from @user2\"\n",
    "mentions_arabic = extract_mentions(text_arabic)\n",
    "print(mentions_arabic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- get urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https', 'example.com', ''), ('https', 'example.org', '')]\n"
     ]
    }
   ],
   "source": [
    "def extract_urls(text):\n",
    "\treturn re.findall(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?',text)\n",
    "\n",
    "\n",
    "arabic_text_with_urls = \"enter those sites to find more infos : https://example.comÿå https://example.org\"\n",
    "urls = extract_urls(arabic_text_with_urls)\n",
    "print(urls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spelling_correction(text):\n",
    "\treturn str(TextBlob(text).correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' hi how are you,you look good'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \" hi how are yu,yu look good\"\n",
    "spelling_correction(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- treat hashtags : remove the whole hashtag or just the character "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hashtag(text):\n",
    "    words = text.split()\n",
    "    cleaned_text = []\n",
    "    for word in words:\n",
    "        if not is_hashtag(word):\n",
    "            cleaned_text.append(word)\n",
    "    return \" \".join(cleaned_text)\n",
    "\n",
    "def is_hashtag(word):\n",
    "    return word.startswith(\"#\")\n",
    "\n",
    "\n",
    "def remove_hashtag_char(text):\n",
    "    cleaned_text = re.sub(r'#+', '', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this text be cleaned\n"
     ]
    }
   ],
   "source": [
    "text_with_mentions_arabic = \"this text #must be cleaned #from_hashtags \"\n",
    "cleaned_text_arabic = clean_hashtag(text_with_mentions_arabic)\n",
    "print(cleaned_text_arabic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emails(text):\n",
    "\treturn re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+\\b)',\"\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is example'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"this is example\"\n",
    "remove_emails(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5- remove mentions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi user1\n"
     ]
    }
   ],
   "source": [
    "def remove_mentions(text):\n",
    "    mention_pattern = r'@+'\n",
    "    cleaned_text = re.sub(mention_pattern, '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "text_with_mentions_arabic = \"hi @user1\"\n",
    "cleaned_text_arabic = remove_mentions(text_with_mentions_arabic)\n",
    "print(cleaned_text_arabic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6- replace emojis \"only arabic case is treated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified text: I love programming :smiling_face_with_smiling_eyes: and it makes me :red_heart:\n"
     ]
    }
   ],
   "source": [
    "def replace_emojis_with_words(text):\n",
    "    modified_text = emoji.demojize(text)\n",
    "    return modified_text\n",
    "\n",
    "example_text = \"I love programming üòä and it makes me ‚ù§Ô∏è\"\n",
    "modified_text = replace_emojis_with_words(example_text)\n",
    "print(\"Modified text:\", modified_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    cleaned_text = re.sub(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?',\"\",text)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic text with URLs removed: check this \n"
     ]
    }
   ],
   "source": [
    "text = \"check this http://www.google.com\"\n",
    "cleaned_text = remove_urls(text)\n",
    "print(\"Arabic text with URLs removed:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7- remove punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\]'\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_3360\\3878505240.py:4: SyntaxWarning: invalid escape sequence '\\]'\n",
      "  return re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,ÿå-./:;<=>ÿü?@[\\]^_`{|}~\"\"\"), ' ', text)\n"
     ]
    }
   ],
   "source": [
    "all_punctuation = string.punctuation\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    return re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,ÿå-./:;<=>ÿü?@[\\]^_`{|}~\"\"\"), ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic text with punctuations removed: remove those punctuations      \n"
     ]
    }
   ],
   "source": [
    "# Arabic example\n",
    "arabic_text = \"remove those punctuations !;!'-\"\n",
    "cleaned_arabic_text = remove_punctuations(arabic_text)\n",
    "print(\"Arabic text with punctuations removed:\", cleaned_arabic_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10- remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with English stopwords removed: sample sentence containing English stopwords.\n"
     ]
    }
   ],
   "source": [
    "def remove_english_stopwords(text):\n",
    "    return \" \".join([t for t in text.split() if t.lower() not in stopwords])\n",
    "\n",
    "sample_text = \"This is a sample sentence containing some English stopwords.\"\n",
    "filtered_text = remove_english_stopwords(sample_text)\n",
    "print(\"Text with English stopwords removed:\", filtered_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8- remove non used characters (u can keep numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_english(text):\n",
    "    english_pattern = re.compile(r'[^\\x00-\\x7F\\d\\s]+')    \n",
    "    cleaned_text = english_pattern.sub('', text)\n",
    "    \n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with non-Arabic characters removed:     () gghhgj  123   .,; ()   1315!\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"Ÿáÿ∞ÿß√©  ŸáŸà ŸÜÿµ (ÿ™ÿ¨ÿ±Ÿäÿ®Ÿä) gghhgj Ÿäÿ≠ÿ™ŸàŸä 123 ÿπŸÑŸâ ÿ®ÿπÿ∂ ÿßŸÑŸÉŸÑŸÖÿßÿ™.,; () ÿßŸÑÿπÿ±ÿ®Ÿäÿ©  1315ÿßŸÑÿπÿßÿØŸäÿ©!\"\n",
    "cleaned_text = remove_non_english(text)\n",
    "print(\"Text with non-Arabic characters removed:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9- remove repeated characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeated_characters(text):\n",
    "    repeated_pattern = re.compile(r'(\\S)(\\1)+', re.UNICODE)\n",
    "    cleaned_text = repeated_pattern.sub(r'\\1', text)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: thissss        isss theee \n",
      "Text with repeated characters removed: this        is the \n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"thissss        isss theee \"\n",
    "cleaned_text = remove_repeated_characters(text)\n",
    "print(\"Original text:\", text)\n",
    "print(\"Text with repeated characters removed:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove multiple spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multiple_spaces(text):\n",
    "\treturn ' '.join(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stemming words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'are', 'jump', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    tokens_lemmas = [token.lemma_ if token.lemma_ != \"-PRON-\" and token.lemma_ != \"be\" else token.text for token in doc]\n",
    "    return tokens_lemmas\n",
    "\n",
    "text = \"The quick brown foxes are jumping over the lazy dogs\"\n",
    "tokens_lemmas = tokenize_and_lemmatize(text)\n",
    "print(tokens_lemmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the proposed algorithm to treat the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "this is an example text with multiple phrases\n"
     ]
    }
   ],
   "source": [
    "text=\"this is an example text with multiple phrases\"\n",
    "emails = get_emails(text)\n",
    "print(emails)\n",
    "hashtags = extract_hashtag(text)\n",
    "print(hashtags)\n",
    "mentions = extract_mentions(text)\n",
    "print(mentions)\n",
    "urls = extract_urls(text)\n",
    "print(urls)\n",
    "corrected_text = spelling_correction(text)\n",
    "print(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['example', 'text', 'multiple', 'phrase']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text=remove_emails(text)\n",
    "    text=clean_hashtag(text)\n",
    "    text=remove_mentions(text)\n",
    "    text=remove_urls(text)\n",
    "    text=replace_emojis_with_words(text)\n",
    "    text=remove_punctuations(text)\n",
    "    text=remove_repeated_characters(text)\n",
    "    text=remove_multiple_spaces(text)\n",
    "    text=remove_english_stopwords(text)\n",
    "    return tokenize_and_lemmatize(text)\n",
    "\n",
    "\n",
    "clean_text(corrected_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
